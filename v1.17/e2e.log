I0925 02:36:29.933052      23 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-888863303
I0925 02:36:29.933074      23 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0925 02:36:29.933193      23 e2e.go:109] Starting e2e run "9279dc4d-74b6-48d5-b084-db66b9fd2bf4" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1601001388 - Will randomize all specs
Will run 280 of 4844 specs

Sep 25 02:36:29.954: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
E0925 02:36:29.955056      23 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp [::1]:8099: connect: connection refused
Sep 25 02:36:29.959: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 25 02:36:29.981: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 25 02:36:30.014: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 25 02:36:30.014: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Sep 25 02:36:30.014: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 25 02:36:30.021: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep 25 02:36:30.021: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 25 02:36:30.021: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'local-volume-provisioner' (0 seconds elapsed)
Sep 25 02:36:30.021: INFO: e2e test version: v1.17.11
Sep 25 02:36:30.022: INFO: kube-apiserver version: v1.17.11
Sep 25 02:36:30.022: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:30.025: INFO: Cluster IP family: ipv4
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:36:30.025: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
Sep 25 02:36:30.050: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Sep 25 02:36:30.060: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 02:36:30.181: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a93590c4-4758-415e-96b5-c539c21f2f74" in namespace "projected-2419" to be "success or failure"
Sep 25 02:36:30.185: INFO: Pod "downwardapi-volume-a93590c4-4758-415e-96b5-c539c21f2f74": Phase="Pending", Reason="", readiness=false. Elapsed: 3.149872ms
Sep 25 02:36:32.190: INFO: Pod "downwardapi-volume-a93590c4-4758-415e-96b5-c539c21f2f74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008327869s
Sep 25 02:36:34.194: INFO: Pod "downwardapi-volume-a93590c4-4758-415e-96b5-c539c21f2f74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012552336s
STEP: Saw pod success
Sep 25 02:36:34.194: INFO: Pod "downwardapi-volume-a93590c4-4758-415e-96b5-c539c21f2f74" satisfied condition "success or failure"
Sep 25 02:36:34.197: INFO: Trying to get logs from node biz-k8s-node-2 pod downwardapi-volume-a93590c4-4758-415e-96b5-c539c21f2f74 container client-container: <nil>
STEP: delete the pod
Sep 25 02:36:34.221: INFO: Waiting for pod downwardapi-volume-a93590c4-4758-415e-96b5-c539c21f2f74 to disappear
Sep 25 02:36:34.224: INFO: Pod downwardapi-volume-a93590c4-4758-415e-96b5-c539c21f2f74 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:36:34.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2419" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":1,"skipped":8,"failed":0}

------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:36:34.230: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-8908
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 25 02:36:52.404: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8908 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:36:52.405: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:52.613: INFO: Exec stderr: ""
Sep 25 02:36:52.613: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8908 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:36:52.614: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:52.715: INFO: Exec stderr: ""
Sep 25 02:36:52.715: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8908 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:36:52.715: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:52.806: INFO: Exec stderr: ""
Sep 25 02:36:52.806: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8908 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:36:52.806: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:52.905: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 25 02:36:52.905: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8908 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:36:52.905: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:53.004: INFO: Exec stderr: ""
Sep 25 02:36:53.004: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8908 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:36:53.004: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:53.099: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 25 02:36:53.099: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8908 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:36:53.099: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:53.208: INFO: Exec stderr: ""
Sep 25 02:36:53.208: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8908 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:36:53.208: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:53.303: INFO: Exec stderr: ""
Sep 25 02:36:53.303: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8908 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:36:53.303: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:53.404: INFO: Exec stderr: ""
Sep 25 02:36:53.404: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8908 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:36:53.404: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:36:53.479: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:36:53.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8908" for this suite.

â€¢ [SLOW TEST:19.257 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":2,"skipped":8,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:36:53.487: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep 25 02:37:03.704: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:37:03.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1766" for this suite.

â€¢ [SLOW TEST:10.225 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":3,"skipped":23,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:37:03.713: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-a9594c82-07f2-40ef-a699-ee89b5d9c499 in namespace container-probe-897
Sep 25 02:37:09.854: INFO: Started pod test-webserver-a9594c82-07f2-40ef-a699-ee89b5d9c499 in namespace container-probe-897
STEP: checking the pod's current state and verifying that restartCount is present
Sep 25 02:37:09.857: INFO: Initial restart count of pod test-webserver-a9594c82-07f2-40ef-a699-ee89b5d9c499 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:41:10.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-897" for this suite.

â€¢ [SLOW TEST:246.704 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":4,"skipped":34,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:41:10.417: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-725287d2-d687-462d-9033-374c7296f65a
STEP: Creating a pod to test consume secrets
Sep 25 02:41:10.561: INFO: Waiting up to 5m0s for pod "pod-secrets-e432c342-28b7-4484-a313-df3115f900f2" in namespace "secrets-1974" to be "success or failure"
Sep 25 02:41:10.567: INFO: Pod "pod-secrets-e432c342-28b7-4484-a313-df3115f900f2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037551ms
Sep 25 02:41:12.571: INFO: Pod "pod-secrets-e432c342-28b7-4484-a313-df3115f900f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010418708s
Sep 25 02:41:14.576: INFO: Pod "pod-secrets-e432c342-28b7-4484-a313-df3115f900f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014817781s
Sep 25 02:41:16.580: INFO: Pod "pod-secrets-e432c342-28b7-4484-a313-df3115f900f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018963933s
STEP: Saw pod success
Sep 25 02:41:16.580: INFO: Pod "pod-secrets-e432c342-28b7-4484-a313-df3115f900f2" satisfied condition "success or failure"
Sep 25 02:41:16.584: INFO: Trying to get logs from node biz-k8s-node-1 pod pod-secrets-e432c342-28b7-4484-a313-df3115f900f2 container secret-volume-test: <nil>
STEP: delete the pod
Sep 25 02:41:16.627: INFO: Waiting for pod pod-secrets-e432c342-28b7-4484-a313-df3115f900f2 to disappear
Sep 25 02:41:16.629: INFO: Pod pod-secrets-e432c342-28b7-4484-a313-df3115f900f2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:41:16.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1974" for this suite.

â€¢ [SLOW TEST:6.219 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":5,"skipped":40,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:41:16.636: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 02:41:16.765: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 25 02:41:16.774: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 25 02:41:21.778: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 25 02:41:27.784: INFO: Creating deployment "test-rolling-update-deployment"
Sep 25 02:41:27.789: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 25 02:41:27.796: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 25 02:41:29.804: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 25 02:41:29.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598487, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598487, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598487, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598487, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 02:41:31.813: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Sep 25 02:41:31.823: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6081 /apis/apps/v1/namespaces/deployment-6081/deployments/test-rolling-update-deployment b04667d5-a7a3-4d49-9c07-29cbfc30e3f9 403236 1 2020-09-25 02:41:27 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000788a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-09-25 02:41:27 +0000 UTC,LastTransitionTime:2020-09-25 02:41:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-09-25 02:41:29 +0000 UTC,LastTransitionTime:2020-09-25 02:41:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 25 02:41:31.827: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-6081 /apis/apps/v1/namespaces/deployment-6081/replicasets/test-rolling-update-deployment-67cf4f6444 826d64b4-1499-4c69-9d45-30359dcd9c85 403226 1 2020-09-25 02:41:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b04667d5-a7a3-4d49-9c07-29cbfc30e3f9 0xc000789597 0xc000789598}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000789668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 25 02:41:31.827: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 25 02:41:31.827: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6081 /apis/apps/v1/namespaces/deployment-6081/replicasets/test-rolling-update-controller d5722495-80a4-4f2e-9533-7283f4bff449 403234 2 2020-09-25 02:41:16 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b04667d5-a7a3-4d49-9c07-29cbfc30e3f9 0xc000789397 0xc000789398}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000789408 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 25 02:41:31.831: INFO: Pod "test-rolling-update-deployment-67cf4f6444-hj2nf" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-hj2nf test-rolling-update-deployment-67cf4f6444- deployment-6081 /api/v1/namespaces/deployment-6081/pods/test-rolling-update-deployment-67cf4f6444-hj2nf b7206367-a0ed-41b9-a278-d59b7f7878ca 403225 0 2020-09-25 02:41:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:192.168.100.172/32 cni.projectcalico.org/podIPs:192.168.100.172/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 826d64b4-1499-4c69-9d45-30359dcd9c85 0xc000789d67 0xc000789d68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h4f2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h4f2d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h4f2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 02:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 02:41:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 02:41:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 02:41:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:192.168.100.172,StartTime:2020-09-25 02:41:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 02:41:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://fc2035c70ef34c839289fd5ed161797621671c48cbfb98f5221126f641ec16b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:41:31.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6081" for this suite.

â€¢ [SLOW TEST:15.204 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":6,"skipped":47,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:41:31.841: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:41:48.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4790" for this suite.

â€¢ [SLOW TEST:16.173 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":7,"skipped":67,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:41:48.015: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-5871/secret-test-8448f52b-543d-4ef9-b5ca-058a109a521e
STEP: Creating a pod to test consume secrets
Sep 25 02:41:48.164: INFO: Waiting up to 5m0s for pod "pod-configmaps-66e8b1ab-2cbe-4e34-adfa-984068e3d34f" in namespace "secrets-5871" to be "success or failure"
Sep 25 02:41:48.167: INFO: Pod "pod-configmaps-66e8b1ab-2cbe-4e34-adfa-984068e3d34f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37535ms
Sep 25 02:41:50.171: INFO: Pod "pod-configmaps-66e8b1ab-2cbe-4e34-adfa-984068e3d34f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007031637s
Sep 25 02:41:52.176: INFO: Pod "pod-configmaps-66e8b1ab-2cbe-4e34-adfa-984068e3d34f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011405124s
Sep 25 02:41:54.180: INFO: Pod "pod-configmaps-66e8b1ab-2cbe-4e34-adfa-984068e3d34f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015868433s
Sep 25 02:41:56.185: INFO: Pod "pod-configmaps-66e8b1ab-2cbe-4e34-adfa-984068e3d34f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.020547215s
STEP: Saw pod success
Sep 25 02:41:56.185: INFO: Pod "pod-configmaps-66e8b1ab-2cbe-4e34-adfa-984068e3d34f" satisfied condition "success or failure"
Sep 25 02:41:56.188: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-configmaps-66e8b1ab-2cbe-4e34-adfa-984068e3d34f container env-test: <nil>
STEP: delete the pod
Sep 25 02:41:56.216: INFO: Waiting for pod pod-configmaps-66e8b1ab-2cbe-4e34-adfa-984068e3d34f to disappear
Sep 25 02:41:56.218: INFO: Pod pod-configmaps-66e8b1ab-2cbe-4e34-adfa-984068e3d34f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:41:56.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5871" for this suite.

â€¢ [SLOW TEST:8.209 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":8,"skipped":88,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:41:56.225: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 25 02:41:56.365: INFO: Waiting up to 5m0s for pod "pod-8751177e-c48f-4304-b648-7dd7b4a9ba0b" in namespace "emptydir-4853" to be "success or failure"
Sep 25 02:41:56.369: INFO: Pod "pod-8751177e-c48f-4304-b648-7dd7b4a9ba0b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.142072ms
Sep 25 02:41:58.373: INFO: Pod "pod-8751177e-c48f-4304-b648-7dd7b4a9ba0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007325825s
Sep 25 02:42:00.377: INFO: Pod "pod-8751177e-c48f-4304-b648-7dd7b4a9ba0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011864958s
STEP: Saw pod success
Sep 25 02:42:00.377: INFO: Pod "pod-8751177e-c48f-4304-b648-7dd7b4a9ba0b" satisfied condition "success or failure"
Sep 25 02:42:00.381: INFO: Trying to get logs from node biz-k8s-node-1 pod pod-8751177e-c48f-4304-b648-7dd7b4a9ba0b container test-container: <nil>
STEP: delete the pod
Sep 25 02:42:00.398: INFO: Waiting for pod pod-8751177e-c48f-4304-b648-7dd7b4a9ba0b to disappear
Sep 25 02:42:00.401: INFO: Pod pod-8751177e-c48f-4304-b648-7dd7b4a9ba0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:42:00.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4853" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":9,"skipped":104,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:42:00.409: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3237
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-rdqq
STEP: Creating a pod to test atomic-volume-subpath
Sep 25 02:42:00.556: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rdqq" in namespace "subpath-3237" to be "success or failure"
Sep 25 02:42:00.559: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.583838ms
Sep 25 02:42:02.563: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006893543s
Sep 25 02:42:04.569: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Running", Reason="", readiness=true. Elapsed: 4.012551515s
Sep 25 02:42:06.573: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Running", Reason="", readiness=true. Elapsed: 6.016821594s
Sep 25 02:42:08.581: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Running", Reason="", readiness=true. Elapsed: 8.024590789s
Sep 25 02:42:10.586: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Running", Reason="", readiness=true. Elapsed: 10.029541342s
Sep 25 02:42:12.590: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Running", Reason="", readiness=true. Elapsed: 12.034012433s
Sep 25 02:42:14.594: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Running", Reason="", readiness=true. Elapsed: 14.037543957s
Sep 25 02:42:16.598: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Running", Reason="", readiness=true. Elapsed: 16.04178982s
Sep 25 02:42:18.602: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Running", Reason="", readiness=true. Elapsed: 18.046056792s
Sep 25 02:42:20.607: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Running", Reason="", readiness=true. Elapsed: 20.050743813s
Sep 25 02:42:22.612: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Running", Reason="", readiness=true. Elapsed: 22.055422525s
Sep 25 02:42:24.617: INFO: Pod "pod-subpath-test-projected-rdqq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.060436561s
STEP: Saw pod success
Sep 25 02:42:24.617: INFO: Pod "pod-subpath-test-projected-rdqq" satisfied condition "success or failure"
Sep 25 02:42:24.620: INFO: Trying to get logs from node biz-k8s-node-1 pod pod-subpath-test-projected-rdqq container test-container-subpath-projected-rdqq: <nil>
STEP: delete the pod
Sep 25 02:42:24.637: INFO: Waiting for pod pod-subpath-test-projected-rdqq to disappear
Sep 25 02:42:24.644: INFO: Pod pod-subpath-test-projected-rdqq no longer exists
STEP: Deleting pod pod-subpath-test-projected-rdqq
Sep 25 02:42:24.644: INFO: Deleting pod "pod-subpath-test-projected-rdqq" in namespace "subpath-3237"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:42:24.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3237" for this suite.

â€¢ [SLOW TEST:24.253 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":10,"skipped":104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:42:24.663: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-330
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:43:04.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-330" for this suite.

â€¢ [SLOW TEST:39.447 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":11,"skipped":146,"failed":0}
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:43:04.110: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-321
STEP: creating replication controller nodeport-test in namespace services-321
I0925 02:43:04.248961      23 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-321, replica count: 2
I0925 02:43:07.300211      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 25 02:43:07.300: INFO: Creating new exec pod
Sep 25 02:43:14.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-321 execpod8rqgb -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep 25 02:43:14.624: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 25 02:43:14.624: INFO: stdout: ""
Sep 25 02:43:14.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-321 execpod8rqgb -- /bin/sh -x -c nc -zv -t -w 2 10.0.40.55 80'
Sep 25 02:43:14.833: INFO: stderr: "+ nc -zv -t -w 2 10.0.40.55 80\nConnection to 10.0.40.55 80 port [tcp/http] succeeded!\n"
Sep 25 02:43:14.833: INFO: stdout: ""
Sep 25 02:43:14.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-321 execpod8rqgb -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.31 31435'
Sep 25 02:43:15.033: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.31 31435\nConnection to 192.168.0.31 31435 port [tcp/31435] succeeded!\n"
Sep 25 02:43:15.033: INFO: stdout: ""
Sep 25 02:43:15.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-321 execpod8rqgb -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.29 31435'
Sep 25 02:43:15.228: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.29 31435\nConnection to 192.168.0.29 31435 port [tcp/31435] succeeded!\n"
Sep 25 02:43:15.228: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:43:15.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-321" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:11.130 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":12,"skipped":146,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:43:15.240: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:43:17.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8967" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":13,"skipped":170,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:43:17.406: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:43:17.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6566" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":14,"skipped":170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:43:17.558: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-41
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 25 02:43:17.690: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:43:20.156: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:43:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-41" for this suite.

â€¢ [SLOW TEST:14.860 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":15,"skipped":194,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:43:32.418: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4479
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4479.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4479.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4479.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4479.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 25 02:43:50.584: INFO: DNS probes using dns-test-830d8fe9-b193-476e-b37e-571985f9aba7 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4479.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4479.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4479.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4479.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 25 02:43:54.630: INFO: DNS probes using dns-test-d26986a7-ee26-49ae-bc43-81b1742fbb9f succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4479.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4479.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4479.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4479.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 25 02:44:10.689: INFO: DNS probes using dns-test-1a23020d-96fd-4583-860c-0c83cc72638d succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:44:10.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4479" for this suite.

â€¢ [SLOW TEST:38.301 seconds]
[sig-network] DNS
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":16,"skipped":201,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:44:10.719: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3263
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3263
STEP: creating replication controller externalsvc in namespace services-3263
I0925 02:44:10.872108      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-3263, replica count: 2
I0925 02:44:13.922552      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 25 02:44:13.935: INFO: Creating new exec pod
Sep 25 02:44:17.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-3263 execpodqdmq4 -- /bin/sh -x -c nslookup clusterip-service'
Sep 25 02:44:18.183: INFO: stderr: "+ nslookup clusterip-service\n"
Sep 25 02:44:18.183: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-3263.svc.cluster.local\tcanonical name = externalsvc.services-3263.svc.cluster.local.\nName:\texternalsvc.services-3263.svc.cluster.local\nAddress: 10.0.18.157\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3263, will wait for the garbage collector to delete the pods
Sep 25 02:44:18.243: INFO: Deleting ReplicationController externalsvc took: 6.450054ms
Sep 25 02:44:19.143: INFO: Terminating ReplicationController externalsvc pods took: 900.19799ms
Sep 25 02:44:32.956: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:44:32.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3263" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:22.255 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":17,"skipped":210,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:44:32.975: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 02:44:33.515: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 02:44:36.533: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 02:44:36.537: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:44:37.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1460" for this suite.
STEP: Destroying namespace "webhook-1460-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":18,"skipped":226,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:44:37.186: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-167694bc-662e-41ed-9357-8754ee0d0ce4
STEP: Creating a pod to test consume configMaps
Sep 25 02:44:37.330: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-90626fb9-09e9-4c39-9677-adb865ad6f5f" in namespace "projected-7861" to be "success or failure"
Sep 25 02:44:37.334: INFO: Pod "pod-projected-configmaps-90626fb9-09e9-4c39-9677-adb865ad6f5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.338467ms
Sep 25 02:44:39.338: INFO: Pod "pod-projected-configmaps-90626fb9-09e9-4c39-9677-adb865ad6f5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008716179s
Sep 25 02:44:41.343: INFO: Pod "pod-projected-configmaps-90626fb9-09e9-4c39-9677-adb865ad6f5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013091864s
STEP: Saw pod success
Sep 25 02:44:41.343: INFO: Pod "pod-projected-configmaps-90626fb9-09e9-4c39-9677-adb865ad6f5f" satisfied condition "success or failure"
Sep 25 02:44:41.346: INFO: Trying to get logs from node biz-k8s-node-1 pod pod-projected-configmaps-90626fb9-09e9-4c39-9677-adb865ad6f5f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 02:44:41.364: INFO: Waiting for pod pod-projected-configmaps-90626fb9-09e9-4c39-9677-adb865ad6f5f to disappear
Sep 25 02:44:41.366: INFO: Pod pod-projected-configmaps-90626fb9-09e9-4c39-9677-adb865ad6f5f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:44:41.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7861" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":19,"skipped":232,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:44:41.374: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1919
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 02:44:41.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 version'
Sep 25 02:44:41.592: INFO: stderr: ""
Sep 25 02:44:41.592: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.11\", GitCommit:\"ea5f00d93211b7c80247bf607cfa422ad6fb5347\", GitTreeState:\"clean\", BuildDate:\"2020-08-13T15:20:25Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.11\", GitCommit:\"ea5f00d93211b7c80247bf607cfa422ad6fb5347\", GitTreeState:\"clean\", BuildDate:\"2020-08-13T15:11:47Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:44:41.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1919" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":20,"skipped":235,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:44:41.601: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3528
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3528
I0925 02:44:41.752036      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-3528, replica count: 2
Sep 25 02:44:44.802: INFO: Creating new exec pod
I0925 02:44:44.802527      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 25 02:44:47.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-3528 execpodmwv2h -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 25 02:44:48.044: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 25 02:44:48.044: INFO: stdout: ""
Sep 25 02:44:48.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-3528 execpodmwv2h -- /bin/sh -x -c nc -zv -t -w 2 10.0.21.45 80'
Sep 25 02:44:48.250: INFO: stderr: "+ nc -zv -t -w 2 10.0.21.45 80\nConnection to 10.0.21.45 80 port [tcp/http] succeeded!\n"
Sep 25 02:44:48.250: INFO: stdout: ""
Sep 25 02:44:48.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-3528 execpodmwv2h -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.27 32011'
Sep 25 02:44:48.442: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.27 32011\nConnection to 192.168.0.27 32011 port [tcp/32011] succeeded!\n"
Sep 25 02:44:48.442: INFO: stdout: ""
Sep 25 02:44:48.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-3528 execpodmwv2h -- /bin/sh -x -c nc -zv -t -w 2 192.168.0.31 32011'
Sep 25 02:44:48.648: INFO: stderr: "+ nc -zv -t -w 2 192.168.0.31 32011\nConnection to 192.168.0.31 32011 port [tcp/32011] succeeded!\n"
Sep 25 02:44:48.648: INFO: stdout: ""
Sep 25 02:44:48.648: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:44:48.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3528" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:7.072 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":21,"skipped":256,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:44:48.673: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1821
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 25 02:44:57.328: INFO: Successfully updated pod "adopt-release-hfxph"
STEP: Checking that the Job readopts the Pod
Sep 25 02:44:57.328: INFO: Waiting up to 15m0s for pod "adopt-release-hfxph" in namespace "job-1821" to be "adopted"
Sep 25 02:44:57.333: INFO: Pod "adopt-release-hfxph": Phase="Running", Reason="", readiness=true. Elapsed: 4.444158ms
Sep 25 02:44:59.337: INFO: Pod "adopt-release-hfxph": Phase="Running", Reason="", readiness=true. Elapsed: 2.008699937s
Sep 25 02:44:59.337: INFO: Pod "adopt-release-hfxph" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 25 02:44:59.853: INFO: Successfully updated pod "adopt-release-hfxph"
STEP: Checking that the Job releases the Pod
Sep 25 02:44:59.853: INFO: Waiting up to 15m0s for pod "adopt-release-hfxph" in namespace "job-1821" to be "released"
Sep 25 02:44:59.858: INFO: Pod "adopt-release-hfxph": Phase="Running", Reason="", readiness=true. Elapsed: 5.303232ms
Sep 25 02:45:01.863: INFO: Pod "adopt-release-hfxph": Phase="Running", Reason="", readiness=true. Elapsed: 2.010176017s
Sep 25 02:45:01.863: INFO: Pod "adopt-release-hfxph" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:45:01.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1821" for this suite.

â€¢ [SLOW TEST:13.201 seconds]
[sig-apps] Job
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":22,"skipped":259,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:45:01.874: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1877
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-f8eea244-09ac-4685-bab1-65e090b50570
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f8eea244-09ac-4685-bab1-65e090b50570
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:45:08.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1877" for this suite.

â€¢ [SLOW TEST:6.218 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":23,"skipped":273,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:45:08.093: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:46:08.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7135" for this suite.

â€¢ [SLOW TEST:60.157 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":24,"skipped":279,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:46:08.251: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 02:46:08.397: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-ea1b8e04-dcd5-4d6c-b2ed-c0e2da8a7329" in namespace "security-context-test-620" to be "success or failure"
Sep 25 02:46:08.400: INFO: Pod "busybox-privileged-false-ea1b8e04-dcd5-4d6c-b2ed-c0e2da8a7329": Phase="Pending", Reason="", readiness=false. Elapsed: 2.832247ms
Sep 25 02:46:10.404: INFO: Pod "busybox-privileged-false-ea1b8e04-dcd5-4d6c-b2ed-c0e2da8a7329": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006807437s
Sep 25 02:46:12.414: INFO: Pod "busybox-privileged-false-ea1b8e04-dcd5-4d6c-b2ed-c0e2da8a7329": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016663032s
Sep 25 02:46:12.414: INFO: Pod "busybox-privileged-false-ea1b8e04-dcd5-4d6c-b2ed-c0e2da8a7329" satisfied condition "success or failure"
Sep 25 02:46:12.420: INFO: Got logs for pod "busybox-privileged-false-ea1b8e04-dcd5-4d6c-b2ed-c0e2da8a7329": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:46:12.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-620" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":25,"skipped":304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:46:12.429: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 02:46:12.570: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34133026-e4ce-4cbf-b4bd-ecceda3309a2" in namespace "projected-5464" to be "success or failure"
Sep 25 02:46:12.574: INFO: Pod "downwardapi-volume-34133026-e4ce-4cbf-b4bd-ecceda3309a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155795ms
Sep 25 02:46:14.578: INFO: Pod "downwardapi-volume-34133026-e4ce-4cbf-b4bd-ecceda3309a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00832192s
STEP: Saw pod success
Sep 25 02:46:14.578: INFO: Pod "downwardapi-volume-34133026-e4ce-4cbf-b4bd-ecceda3309a2" satisfied condition "success or failure"
Sep 25 02:46:14.581: INFO: Trying to get logs from node biz-k8s-node-2 pod downwardapi-volume-34133026-e4ce-4cbf-b4bd-ecceda3309a2 container client-container: <nil>
STEP: delete the pod
Sep 25 02:46:14.610: INFO: Waiting for pod downwardapi-volume-34133026-e4ce-4cbf-b4bd-ecceda3309a2 to disappear
Sep 25 02:46:14.612: INFO: Pod downwardapi-volume-34133026-e4ce-4cbf-b4bd-ecceda3309a2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:46:14.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5464" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":26,"skipped":339,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:46:14.618: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-540
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep 25 02:46:14.749: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Sep 25 02:46:15.220: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 25 02:46:17.264: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 02:46:19.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 02:46:21.267: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 02:46:23.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 02:46:25.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 02:46:27.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598775, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 02:46:31.703: INFO: Waited 2.422695527s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:46:32.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-540" for this suite.

â€¢ [SLOW TEST:18.211 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":27,"skipped":361,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:46:32.830: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3881
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-8824e0fa-16cb-45ae-9c55-34b6c59af8ea
STEP: Creating secret with name s-test-opt-upd-6c3fafe7-1908-4d96-a7d5-1ecde6ee275b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8824e0fa-16cb-45ae-9c55-34b6c59af8ea
STEP: Updating secret s-test-opt-upd-6c3fafe7-1908-4d96-a7d5-1ecde6ee275b
STEP: Creating secret with name s-test-opt-create-513fd943-ae21-4abf-9da0-6bd769dc04b7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:46:41.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3881" for this suite.

â€¢ [SLOW TEST:8.313 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":28,"skipped":362,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:46:41.143: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-77
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:46:43.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-77" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":29,"skipped":378,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:46:43.341: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9427, will wait for the garbage collector to delete the pods
Sep 25 02:46:47.538: INFO: Deleting Job.batch foo took: 7.216282ms
Sep 25 02:46:47.638: INFO: Terminating Job.batch foo pods took: 100.34303ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:47:23.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9427" for this suite.

â€¢ [SLOW TEST:39.808 seconds]
[sig-apps] Job
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":30,"skipped":387,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:47:23.150: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Sep 25 02:47:23.289: INFO: Waiting up to 5m0s for pod "var-expansion-75f44aeb-0c36-4565-a348-3fb24f6bce8c" in namespace "var-expansion-8557" to be "success or failure"
Sep 25 02:47:23.292: INFO: Pod "var-expansion-75f44aeb-0c36-4565-a348-3fb24f6bce8c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.398282ms
Sep 25 02:47:25.297: INFO: Pod "var-expansion-75f44aeb-0c36-4565-a348-3fb24f6bce8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008076554s
Sep 25 02:47:27.301: INFO: Pod "var-expansion-75f44aeb-0c36-4565-a348-3fb24f6bce8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01189837s
STEP: Saw pod success
Sep 25 02:47:27.301: INFO: Pod "var-expansion-75f44aeb-0c36-4565-a348-3fb24f6bce8c" satisfied condition "success or failure"
Sep 25 02:47:27.304: INFO: Trying to get logs from node biz-k8s-node-3 pod var-expansion-75f44aeb-0c36-4565-a348-3fb24f6bce8c container dapi-container: <nil>
STEP: delete the pod
Sep 25 02:47:27.324: INFO: Waiting for pod var-expansion-75f44aeb-0c36-4565-a348-3fb24f6bce8c to disappear
Sep 25 02:47:27.325: INFO: Pod var-expansion-75f44aeb-0c36-4565-a348-3fb24f6bce8c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:47:27.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8557" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":31,"skipped":422,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:47:27.332: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1952
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Sep 25 02:48:07.492: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:48:07.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1952" for this suite.

â€¢ [SLOW TEST:40.166 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":32,"skipped":422,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:48:07.498: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2988
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Sep 25 02:48:07.640: INFO: Waiting up to 5m0s for pod "client-containers-2760d7a8-38a3-454f-bc1f-355834f6ac50" in namespace "containers-2988" to be "success or failure"
Sep 25 02:48:07.643: INFO: Pod "client-containers-2760d7a8-38a3-454f-bc1f-355834f6ac50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997703ms
Sep 25 02:48:09.647: INFO: Pod "client-containers-2760d7a8-38a3-454f-bc1f-355834f6ac50": Phase="Running", Reason="", readiness=true. Elapsed: 2.007294866s
Sep 25 02:48:11.651: INFO: Pod "client-containers-2760d7a8-38a3-454f-bc1f-355834f6ac50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011801991s
STEP: Saw pod success
Sep 25 02:48:11.652: INFO: Pod "client-containers-2760d7a8-38a3-454f-bc1f-355834f6ac50" satisfied condition "success or failure"
Sep 25 02:48:11.655: INFO: Trying to get logs from node biz-k8s-node-1 pod client-containers-2760d7a8-38a3-454f-bc1f-355834f6ac50 container test-container: <nil>
STEP: delete the pod
Sep 25 02:48:11.684: INFO: Waiting for pod client-containers-2760d7a8-38a3-454f-bc1f-355834f6ac50 to disappear
Sep 25 02:48:11.686: INFO: Pod client-containers-2760d7a8-38a3-454f-bc1f-355834f6ac50 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:48:11.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2988" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":33,"skipped":423,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:48:11.695: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-g54z
STEP: Creating a pod to test atomic-volume-subpath
Sep 25 02:48:11.847: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-g54z" in namespace "subpath-9871" to be "success or failure"
Sep 25 02:48:11.849: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.280493ms
Sep 25 02:48:13.853: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006398842s
Sep 25 02:48:15.858: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Running", Reason="", readiness=true. Elapsed: 4.010509095s
Sep 25 02:48:17.861: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Running", Reason="", readiness=true. Elapsed: 6.014304436s
Sep 25 02:48:19.866: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Running", Reason="", readiness=true. Elapsed: 8.018706646s
Sep 25 02:48:21.869: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Running", Reason="", readiness=true. Elapsed: 10.021686175s
Sep 25 02:48:23.871: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Running", Reason="", readiness=true. Elapsed: 12.023830219s
Sep 25 02:48:25.875: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Running", Reason="", readiness=true. Elapsed: 14.028179051s
Sep 25 02:48:27.879: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Running", Reason="", readiness=true. Elapsed: 16.032363207s
Sep 25 02:48:29.884: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Running", Reason="", readiness=true. Elapsed: 18.03681061s
Sep 25 02:48:31.888: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Running", Reason="", readiness=true. Elapsed: 20.040507216s
Sep 25 02:48:33.893: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Running", Reason="", readiness=true. Elapsed: 22.045746533s
Sep 25 02:48:35.898: INFO: Pod "pod-subpath-test-configmap-g54z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.050638241s
STEP: Saw pod success
Sep 25 02:48:35.898: INFO: Pod "pod-subpath-test-configmap-g54z" satisfied condition "success or failure"
Sep 25 02:48:35.901: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-subpath-test-configmap-g54z container test-container-subpath-configmap-g54z: <nil>
STEP: delete the pod
Sep 25 02:48:35.919: INFO: Waiting for pod pod-subpath-test-configmap-g54z to disappear
Sep 25 02:48:35.923: INFO: Pod pod-subpath-test-configmap-g54z no longer exists
STEP: Deleting pod pod-subpath-test-configmap-g54z
Sep 25 02:48:35.923: INFO: Deleting pod "pod-subpath-test-configmap-g54z" in namespace "subpath-9871"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:48:35.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9871" for this suite.

â€¢ [SLOW TEST:24.237 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":34,"skipped":432,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:48:35.932: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7888
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7888
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 25 02:48:36.070: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 25 02:48:58.166: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.100.84:8080/dial?request=hostname&protocol=http&host=192.168.100.133&port=8080&tries=1'] Namespace:pod-network-test-7888 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:48:58.166: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:48:58.279: INFO: Waiting for responses: map[]
Sep 25 02:48:58.282: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.100.84:8080/dial?request=hostname&protocol=http&host=192.168.100.52&port=8080&tries=1'] Namespace:pod-network-test-7888 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:48:58.282: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:48:58.389: INFO: Waiting for responses: map[]
Sep 25 02:48:58.392: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.100.84:8080/dial?request=hostname&protocol=http&host=192.168.100.83&port=8080&tries=1'] Namespace:pod-network-test-7888 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:48:58.392: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:48:58.491: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:48:58.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7888" for this suite.

â€¢ [SLOW TEST:22.568 seconds]
[sig-network] Networking
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":35,"skipped":433,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:48:58.501: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Sep 25 02:48:58.646: INFO: Waiting up to 5m0s for pod "downward-api-297dd05e-7209-45a2-a274-d267b859bf3e" in namespace "downward-api-2771" to be "success or failure"
Sep 25 02:48:58.649: INFO: Pod "downward-api-297dd05e-7209-45a2-a274-d267b859bf3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.70942ms
Sep 25 02:49:00.654: INFO: Pod "downward-api-297dd05e-7209-45a2-a274-d267b859bf3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007099863s
Sep 25 02:49:02.659: INFO: Pod "downward-api-297dd05e-7209-45a2-a274-d267b859bf3e": Phase="Running", Reason="", readiness=true. Elapsed: 4.01249496s
Sep 25 02:49:04.664: INFO: Pod "downward-api-297dd05e-7209-45a2-a274-d267b859bf3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017647105s
STEP: Saw pod success
Sep 25 02:49:04.664: INFO: Pod "downward-api-297dd05e-7209-45a2-a274-d267b859bf3e" satisfied condition "success or failure"
Sep 25 02:49:04.667: INFO: Trying to get logs from node biz-k8s-node-2 pod downward-api-297dd05e-7209-45a2-a274-d267b859bf3e container dapi-container: <nil>
STEP: delete the pod
Sep 25 02:49:04.702: INFO: Waiting for pod downward-api-297dd05e-7209-45a2-a274-d267b859bf3e to disappear
Sep 25 02:49:04.704: INFO: Pod downward-api-297dd05e-7209-45a2-a274-d267b859bf3e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:49:04.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2771" for this suite.

â€¢ [SLOW TEST:6.209 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":36,"skipped":440,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:49:04.711: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1505
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9946
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4123
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:49:20.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1505" for this suite.
STEP: Destroying namespace "nsdeletetest-9946" for this suite.
Sep 25 02:49:20.151: INFO: Namespace nsdeletetest-9946 was already deleted
STEP: Destroying namespace "nsdeletetest-4123" for this suite.

â€¢ [SLOW TEST:15.443 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":37,"skipped":459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:49:20.155: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 02:49:20.300: INFO: Waiting up to 5m0s for pod "downwardapi-volume-665e8d8d-99aa-476a-b5d0-5eacfe19227b" in namespace "projected-1391" to be "success or failure"
Sep 25 02:49:20.304: INFO: Pod "downwardapi-volume-665e8d8d-99aa-476a-b5d0-5eacfe19227b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.123171ms
Sep 25 02:49:22.308: INFO: Pod "downwardapi-volume-665e8d8d-99aa-476a-b5d0-5eacfe19227b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007175253s
Sep 25 02:49:24.312: INFO: Pod "downwardapi-volume-665e8d8d-99aa-476a-b5d0-5eacfe19227b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011645475s
STEP: Saw pod success
Sep 25 02:49:24.312: INFO: Pod "downwardapi-volume-665e8d8d-99aa-476a-b5d0-5eacfe19227b" satisfied condition "success or failure"
Sep 25 02:49:24.315: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-665e8d8d-99aa-476a-b5d0-5eacfe19227b container client-container: <nil>
STEP: delete the pod
Sep 25 02:49:24.330: INFO: Waiting for pod downwardapi-volume-665e8d8d-99aa-476a-b5d0-5eacfe19227b to disappear
Sep 25 02:49:24.332: INFO: Pod downwardapi-volume-665e8d8d-99aa-476a-b5d0-5eacfe19227b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:49:24.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1391" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":38,"skipped":489,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:49:24.340: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Sep 25 02:49:24.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 cluster-info'
Sep 25 02:49:24.555: INFO: stderr: ""
Sep 25 02:49:24.555: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:49:24.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4346" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":39,"skipped":504,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:49:24.564: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 25 02:49:28.758: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 25 02:49:28.762: INFO: Pod pod-with-poststart-http-hook still exists
Sep 25 02:49:30.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 25 02:49:30.766: INFO: Pod pod-with-poststart-http-hook still exists
Sep 25 02:49:32.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 25 02:49:32.766: INFO: Pod pod-with-poststart-http-hook still exists
Sep 25 02:49:34.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 25 02:49:34.767: INFO: Pod pod-with-poststart-http-hook still exists
Sep 25 02:49:36.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 25 02:49:36.767: INFO: Pod pod-with-poststart-http-hook still exists
Sep 25 02:49:38.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 25 02:49:38.767: INFO: Pod pod-with-poststart-http-hook still exists
Sep 25 02:49:40.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 25 02:49:40.767: INFO: Pod pod-with-poststart-http-hook still exists
Sep 25 02:49:42.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 25 02:49:42.767: INFO: Pod pod-with-poststart-http-hook still exists
Sep 25 02:49:44.762: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 25 02:49:44.767: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:49:44.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5410" for this suite.

â€¢ [SLOW TEST:20.213 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":40,"skipped":519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:49:44.778: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 25 02:49:44.920: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 25 02:49:49.923: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:49:50.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7509" for this suite.

â€¢ [SLOW TEST:6.175 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":41,"skipped":609,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:49:50.954: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 02:49:51.495: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 02:49:53.506: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598991, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598991, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598991, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736598991, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 02:49:56.519: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:49:56.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8274" for this suite.
STEP: Destroying namespace "webhook-8274-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.788 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":42,"skipped":617,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:49:56.742: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:50
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Sep 25 02:50:00.891: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-888863303 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 25 02:50:06.011: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:06.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4874" for this suite.

â€¢ [SLOW TEST:9.281 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":43,"skipped":621,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:06.024: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8783
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-8783
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8783 to expose endpoints map[]
Sep 25 02:50:06.165: INFO: Get endpoints failed (3.230377ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep 25 02:50:07.168: INFO: successfully validated that service multi-endpoint-test in namespace services-8783 exposes endpoints map[] (1.006040719s elapsed)
STEP: Creating pod pod1 in namespace services-8783
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8783 to expose endpoints map[pod1:[100]]
Sep 25 02:50:09.207: INFO: successfully validated that service multi-endpoint-test in namespace services-8783 exposes endpoints map[pod1:[100]] (2.026943556s elapsed)
STEP: Creating pod pod2 in namespace services-8783
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8783 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 25 02:50:12.260: INFO: successfully validated that service multi-endpoint-test in namespace services-8783 exposes endpoints map[pod1:[100] pod2:[101]] (3.042229507s elapsed)
STEP: Deleting pod pod1 in namespace services-8783
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8783 to expose endpoints map[pod2:[101]]
Sep 25 02:50:12.280: INFO: successfully validated that service multi-endpoint-test in namespace services-8783 exposes endpoints map[pod2:[101]] (15.692935ms elapsed)
STEP: Deleting pod pod2 in namespace services-8783
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8783 to expose endpoints map[]
Sep 25 02:50:12.293: INFO: successfully validated that service multi-endpoint-test in namespace services-8783 exposes endpoints map[] (3.11918ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:12.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8783" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:6.284 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":44,"skipped":627,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:12.309: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 02:50:12.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3fecf65e-cafb-4d78-8b67-d3c7c1b13b66" in namespace "downward-api-9144" to be "success or failure"
Sep 25 02:50:12.454: INFO: Pod "downwardapi-volume-3fecf65e-cafb-4d78-8b67-d3c7c1b13b66": Phase="Pending", Reason="", readiness=false. Elapsed: 3.082369ms
Sep 25 02:50:14.463: INFO: Pod "downwardapi-volume-3fecf65e-cafb-4d78-8b67-d3c7c1b13b66": Phase="Running", Reason="", readiness=true. Elapsed: 2.012030105s
Sep 25 02:50:16.468: INFO: Pod "downwardapi-volume-3fecf65e-cafb-4d78-8b67-d3c7c1b13b66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016724141s
STEP: Saw pod success
Sep 25 02:50:16.468: INFO: Pod "downwardapi-volume-3fecf65e-cafb-4d78-8b67-d3c7c1b13b66" satisfied condition "success or failure"
Sep 25 02:50:16.471: INFO: Trying to get logs from node biz-k8s-node-2 pod downwardapi-volume-3fecf65e-cafb-4d78-8b67-d3c7c1b13b66 container client-container: <nil>
STEP: delete the pod
Sep 25 02:50:16.495: INFO: Waiting for pod downwardapi-volume-3fecf65e-cafb-4d78-8b67-d3c7c1b13b66 to disappear
Sep 25 02:50:16.497: INFO: Pod downwardapi-volume-3fecf65e-cafb-4d78-8b67-d3c7c1b13b66 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:16.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9144" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":45,"skipped":641,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:16.503: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7313
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-f8416fab-869d-4ca4-a6bb-f1bcaa863fec
STEP: Creating a pod to test consume configMaps
Sep 25 02:50:16.652: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2927deda-70ed-4b0d-82d6-d42d87b7601f" in namespace "projected-7313" to be "success or failure"
Sep 25 02:50:16.659: INFO: Pod "pod-projected-configmaps-2927deda-70ed-4b0d-82d6-d42d87b7601f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.581701ms
Sep 25 02:50:18.663: INFO: Pod "pod-projected-configmaps-2927deda-70ed-4b0d-82d6-d42d87b7601f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010217049s
Sep 25 02:50:20.667: INFO: Pod "pod-projected-configmaps-2927deda-70ed-4b0d-82d6-d42d87b7601f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014564326s
STEP: Saw pod success
Sep 25 02:50:20.667: INFO: Pod "pod-projected-configmaps-2927deda-70ed-4b0d-82d6-d42d87b7601f" satisfied condition "success or failure"
Sep 25 02:50:20.670: INFO: Trying to get logs from node biz-k8s-node-1 pod pod-projected-configmaps-2927deda-70ed-4b0d-82d6-d42d87b7601f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 02:50:20.687: INFO: Waiting for pod pod-projected-configmaps-2927deda-70ed-4b0d-82d6-d42d87b7601f to disappear
Sep 25 02:50:20.690: INFO: Pod pod-projected-configmaps-2927deda-70ed-4b0d-82d6-d42d87b7601f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:20.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7313" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":46,"skipped":645,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:20.697: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 02:50:20.838: INFO: Waiting up to 5m0s for pod "downwardapi-volume-64eebad3-3628-45c7-ac7e-99c55f3721bb" in namespace "downward-api-3580" to be "success or failure"
Sep 25 02:50:20.841: INFO: Pod "downwardapi-volume-64eebad3-3628-45c7-ac7e-99c55f3721bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944581ms
Sep 25 02:50:22.846: INFO: Pod "downwardapi-volume-64eebad3-3628-45c7-ac7e-99c55f3721bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007356632s
STEP: Saw pod success
Sep 25 02:50:22.846: INFO: Pod "downwardapi-volume-64eebad3-3628-45c7-ac7e-99c55f3721bb" satisfied condition "success or failure"
Sep 25 02:50:22.849: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-64eebad3-3628-45c7-ac7e-99c55f3721bb container client-container: <nil>
STEP: delete the pod
Sep 25 02:50:22.868: INFO: Waiting for pod downwardapi-volume-64eebad3-3628-45c7-ac7e-99c55f3721bb to disappear
Sep 25 02:50:22.872: INFO: Pod downwardapi-volume-64eebad3-3628-45c7-ac7e-99c55f3721bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:22.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3580" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":658,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:22.880: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2050
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2050.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2050.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 25 02:50:27.067: INFO: DNS probes using dns-2050/dns-test-d144fc23-a051-4fe7-8856-99882f0fd41e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:27.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2050" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":48,"skipped":663,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:27.080: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-eb7de302-e95c-4e89-ab5f-c26479f61021
STEP: Creating secret with name secret-projected-all-test-volume-60713389-49ca-47e0-bd24-8de152f5b31a
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 25 02:50:27.229: INFO: Waiting up to 5m0s for pod "projected-volume-6f5646fe-8d7f-4c3e-935f-1b2300b8da95" in namespace "projected-1566" to be "success or failure"
Sep 25 02:50:27.231: INFO: Pod "projected-volume-6f5646fe-8d7f-4c3e-935f-1b2300b8da95": Phase="Pending", Reason="", readiness=false. Elapsed: 1.917728ms
Sep 25 02:50:29.235: INFO: Pod "projected-volume-6f5646fe-8d7f-4c3e-935f-1b2300b8da95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005888057s
Sep 25 02:50:31.239: INFO: Pod "projected-volume-6f5646fe-8d7f-4c3e-935f-1b2300b8da95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010074851s
STEP: Saw pod success
Sep 25 02:50:31.239: INFO: Pod "projected-volume-6f5646fe-8d7f-4c3e-935f-1b2300b8da95" satisfied condition "success or failure"
Sep 25 02:50:31.242: INFO: Trying to get logs from node biz-k8s-node-3 pod projected-volume-6f5646fe-8d7f-4c3e-935f-1b2300b8da95 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 25 02:50:31.258: INFO: Waiting for pod projected-volume-6f5646fe-8d7f-4c3e-935f-1b2300b8da95 to disappear
Sep 25 02:50:31.261: INFO: Pod projected-volume-6f5646fe-8d7f-4c3e-935f-1b2300b8da95 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:31.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1566" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":49,"skipped":671,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:31.267: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-245
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:35.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-245" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":50,"skipped":683,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:35.425: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9904
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 02:50:35.554: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:36.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9904" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":51,"skipped":712,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:36.098: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5800
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 02:50:36.226: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 25 02:50:39.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-5800 create -f -'
Sep 25 02:50:39.552: INFO: stderr: ""
Sep 25 02:50:39.552: INFO: stdout: "e2e-test-crd-publish-openapi-8450-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 25 02:50:39.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-5800 delete e2e-test-crd-publish-openapi-8450-crds test-cr'
Sep 25 02:50:39.660: INFO: stderr: ""
Sep 25 02:50:39.660: INFO: stdout: "e2e-test-crd-publish-openapi-8450-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 25 02:50:39.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-5800 apply -f -'
Sep 25 02:50:39.856: INFO: stderr: ""
Sep 25 02:50:39.856: INFO: stdout: "e2e-test-crd-publish-openapi-8450-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 25 02:50:39.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-5800 delete e2e-test-crd-publish-openapi-8450-crds test-cr'
Sep 25 02:50:39.964: INFO: stderr: ""
Sep 25 02:50:39.964: INFO: stdout: "e2e-test-crd-publish-openapi-8450-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 25 02:50:39.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 explain e2e-test-crd-publish-openapi-8450-crds'
Sep 25 02:50:40.163: INFO: stderr: ""
Sep 25 02:50:40.163: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8450-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:43.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5800" for this suite.

â€¢ [SLOW TEST:7.038 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":52,"skipped":732,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:43.136: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-dd0d1fcd-7b8a-4dd4-88d2-66d46e140ede
Sep 25 02:50:43.268: INFO: Pod name my-hostname-basic-dd0d1fcd-7b8a-4dd4-88d2-66d46e140ede: Found 0 pods out of 1
Sep 25 02:50:48.280: INFO: Pod name my-hostname-basic-dd0d1fcd-7b8a-4dd4-88d2-66d46e140ede: Found 1 pods out of 1
Sep 25 02:50:48.280: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dd0d1fcd-7b8a-4dd4-88d2-66d46e140ede" are running
Sep 25 02:50:48.290: INFO: Pod "my-hostname-basic-dd0d1fcd-7b8a-4dd4-88d2-66d46e140ede-p4g7q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-25 02:50:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-25 02:50:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-25 02:50:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-25 02:50:43 +0000 UTC Reason: Message:}])
Sep 25 02:50:48.290: INFO: Trying to dial the pod
Sep 25 02:50:53.304: INFO: Controller my-hostname-basic-dd0d1fcd-7b8a-4dd4-88d2-66d46e140ede: Got expected result from replica 1 [my-hostname-basic-dd0d1fcd-7b8a-4dd4-88d2-66d46e140ede-p4g7q]: "my-hostname-basic-dd0d1fcd-7b8a-4dd4-88d2-66d46e140ede-p4g7q", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:50:53.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4967" for this suite.

â€¢ [SLOW TEST:10.186 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":53,"skipped":741,"failed":0}
S
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:50:53.322: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:51:07.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9401" for this suite.

â€¢ [SLOW TEST:14.144 seconds]
[sig-apps] Job
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":54,"skipped":742,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:51:07.467: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8284
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Sep 25 02:51:07.598: INFO: namespace kubectl-8284
Sep 25 02:51:07.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-8284'
Sep 25 02:51:07.863: INFO: stderr: ""
Sep 25 02:51:07.863: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep 25 02:51:08.868: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 02:51:08.868: INFO: Found 0 / 1
Sep 25 02:51:09.868: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 02:51:09.868: INFO: Found 0 / 1
Sep 25 02:51:10.868: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 02:51:10.868: INFO: Found 1 / 1
Sep 25 02:51:10.868: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 25 02:51:10.872: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 02:51:10.872: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 25 02:51:10.872: INFO: wait on agnhost-master startup in kubectl-8284 
Sep 25 02:51:10.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 logs agnhost-master-mtkzf agnhost-master --namespace=kubectl-8284'
Sep 25 02:51:10.994: INFO: stderr: ""
Sep 25 02:51:10.994: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep 25 02:51:10.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8284'
Sep 25 02:51:11.099: INFO: stderr: ""
Sep 25 02:51:11.099: INFO: stdout: "service/rm2 exposed\n"
Sep 25 02:51:11.103: INFO: Service rm2 in namespace kubectl-8284 found.
STEP: exposing service
Sep 25 02:51:13.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8284'
Sep 25 02:51:13.231: INFO: stderr: ""
Sep 25 02:51:13.231: INFO: stdout: "service/rm3 exposed\n"
Sep 25 02:51:13.235: INFO: Service rm3 in namespace kubectl-8284 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:51:15.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8284" for this suite.

â€¢ [SLOW TEST:7.784 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":55,"skipped":751,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:51:15.251: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3205
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 25 02:51:19.415: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:51:19.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3205" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":56,"skipped":771,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:51:19.434: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 02:51:19.904: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 02:51:21.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599079, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599079, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599079, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599079, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 02:51:24.926: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:51:35.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2255" for this suite.
STEP: Destroying namespace "webhook-2255-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:15.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":57,"skipped":774,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:51:35.095: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-235
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 25 02:51:40.256: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:51:41.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-235" for this suite.

â€¢ [SLOW TEST:6.196 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":58,"skipped":775,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:51:41.291: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-f995e194-88f4-4cf5-a5c1-d0eb28e5f87a
STEP: Creating a pod to test consume secrets
Sep 25 02:51:41.439: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0a8363a6-1964-4e67-9ea6-13d9d52cbf34" in namespace "projected-7468" to be "success or failure"
Sep 25 02:51:41.442: INFO: Pod "pod-projected-secrets-0a8363a6-1964-4e67-9ea6-13d9d52cbf34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536118ms
Sep 25 02:51:43.446: INFO: Pod "pod-projected-secrets-0a8363a6-1964-4e67-9ea6-13d9d52cbf34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00663936s
STEP: Saw pod success
Sep 25 02:51:43.446: INFO: Pod "pod-projected-secrets-0a8363a6-1964-4e67-9ea6-13d9d52cbf34" satisfied condition "success or failure"
Sep 25 02:51:43.448: INFO: Trying to get logs from node biz-k8s-node-2 pod pod-projected-secrets-0a8363a6-1964-4e67-9ea6-13d9d52cbf34 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 25 02:51:43.465: INFO: Waiting for pod pod-projected-secrets-0a8363a6-1964-4e67-9ea6-13d9d52cbf34 to disappear
Sep 25 02:51:43.468: INFO: Pod pod-projected-secrets-0a8363a6-1964-4e67-9ea6-13d9d52cbf34 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:51:43.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7468" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":59,"skipped":805,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:51:43.475: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7690
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Sep 25 02:51:43.602: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:51:59.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7690" for this suite.

â€¢ [SLOW TEST:16.139 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":60,"skipped":811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:51:59.615: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-9715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 02:51:59.740: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9715
I0925 02:51:59.756483      23 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9715, replica count: 1
I0925 02:52:00.806914      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:52:01.807180      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:52:02.807450      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 25 02:52:02.919: INFO: Created: latency-svc-5ddkd
Sep 25 02:52:02.924: INFO: Got endpoints: latency-svc-5ddkd [16.92157ms]
Sep 25 02:52:02.937: INFO: Created: latency-svc-k8h67
Sep 25 02:52:02.941: INFO: Got endpoints: latency-svc-k8h67 [16.061491ms]
Sep 25 02:52:02.943: INFO: Created: latency-svc-9zsx8
Sep 25 02:52:02.948: INFO: Got endpoints: latency-svc-9zsx8 [23.001355ms]
Sep 25 02:52:02.949: INFO: Created: latency-svc-gblb7
Sep 25 02:52:02.952: INFO: Got endpoints: latency-svc-gblb7 [26.23889ms]
Sep 25 02:52:02.953: INFO: Created: latency-svc-jz88m
Sep 25 02:52:02.957: INFO: Got endpoints: latency-svc-jz88m [31.905154ms]
Sep 25 02:52:02.957: INFO: Created: latency-svc-pvnk8
Sep 25 02:52:02.958: INFO: Created: latency-svc-bqdww
Sep 25 02:52:02.961: INFO: Got endpoints: latency-svc-pvnk8 [35.842242ms]
Sep 25 02:52:02.962: INFO: Created: latency-svc-xfdvn
Sep 25 02:52:02.964: INFO: Got endpoints: latency-svc-bqdww [38.635088ms]
Sep 25 02:52:02.964: INFO: Got endpoints: latency-svc-xfdvn [39.084927ms]
Sep 25 02:52:02.966: INFO: Created: latency-svc-gvpwt
Sep 25 02:52:02.968: INFO: Created: latency-svc-kk8c2
Sep 25 02:52:02.969: INFO: Got endpoints: latency-svc-gvpwt [43.75184ms]
Sep 25 02:52:02.970: INFO: Got endpoints: latency-svc-kk8c2 [44.92559ms]
Sep 25 02:52:02.973: INFO: Created: latency-svc-rxtcl
Sep 25 02:52:02.977: INFO: Got endpoints: latency-svc-rxtcl [51.206033ms]
Sep 25 02:52:02.977: INFO: Created: latency-svc-vl5km
Sep 25 02:52:02.980: INFO: Got endpoints: latency-svc-vl5km [9.65051ms]
Sep 25 02:52:02.982: INFO: Created: latency-svc-dxpx4
Sep 25 02:52:02.986: INFO: Got endpoints: latency-svc-dxpx4 [60.169036ms]
Sep 25 02:52:02.988: INFO: Created: latency-svc-xbwjc
Sep 25 02:52:02.992: INFO: Got endpoints: latency-svc-xbwjc [66.170091ms]
Sep 25 02:52:02.994: INFO: Created: latency-svc-mgbl6
Sep 25 02:52:02.999: INFO: Got endpoints: latency-svc-mgbl6 [73.194478ms]
Sep 25 02:52:02.999: INFO: Created: latency-svc-2fn26
Sep 25 02:52:03.003: INFO: Got endpoints: latency-svc-2fn26 [77.078984ms]
Sep 25 02:52:03.004: INFO: Created: latency-svc-cm925
Sep 25 02:52:03.008: INFO: Got endpoints: latency-svc-cm925 [82.486488ms]
Sep 25 02:52:03.009: INFO: Created: latency-svc-cncq5
Sep 25 02:52:03.016: INFO: Got endpoints: latency-svc-cncq5 [75.092334ms]
Sep 25 02:52:03.024: INFO: Created: latency-svc-z9bqw
Sep 25 02:52:03.026: INFO: Got endpoints: latency-svc-z9bqw [77.52573ms]
Sep 25 02:52:03.027: INFO: Created: latency-svc-kg8dq
Sep 25 02:52:03.029: INFO: Got endpoints: latency-svc-kg8dq [77.684764ms]
Sep 25 02:52:03.030: INFO: Created: latency-svc-7rdwp
Sep 25 02:52:03.032: INFO: Created: latency-svc-9t4sc
Sep 25 02:52:03.033: INFO: Got endpoints: latency-svc-7rdwp [75.812504ms]
Sep 25 02:52:03.034: INFO: Got endpoints: latency-svc-9t4sc [73.452225ms]
Sep 25 02:52:03.036: INFO: Created: latency-svc-n4tw7
Sep 25 02:52:03.039: INFO: Created: latency-svc-fr5ks
Sep 25 02:52:03.039: INFO: Got endpoints: latency-svc-n4tw7 [75.031699ms]
Sep 25 02:52:03.042: INFO: Got endpoints: latency-svc-fr5ks [77.280556ms]
Sep 25 02:52:03.043: INFO: Created: latency-svc-x5s7d
Sep 25 02:52:03.045: INFO: Got endpoints: latency-svc-x5s7d [76.055424ms]
Sep 25 02:52:03.047: INFO: Created: latency-svc-sd9j9
Sep 25 02:52:03.049: INFO: Got endpoints: latency-svc-sd9j9 [72.587423ms]
Sep 25 02:52:03.052: INFO: Created: latency-svc-njprd
Sep 25 02:52:03.054: INFO: Got endpoints: latency-svc-njprd [74.271277ms]
Sep 25 02:52:03.057: INFO: Created: latency-svc-4qv7s
Sep 25 02:52:03.057: INFO: Got endpoints: latency-svc-4qv7s [71.3797ms]
Sep 25 02:52:03.060: INFO: Created: latency-svc-27br2
Sep 25 02:52:03.064: INFO: Got endpoints: latency-svc-27br2 [72.139439ms]
Sep 25 02:52:03.064: INFO: Created: latency-svc-z7gf9
Sep 25 02:52:03.066: INFO: Got endpoints: latency-svc-z7gf9 [66.85505ms]
Sep 25 02:52:03.066: INFO: Created: latency-svc-bgf9z
Sep 25 02:52:03.069: INFO: Created: latency-svc-s75tv
Sep 25 02:52:03.069: INFO: Got endpoints: latency-svc-bgf9z [66.030527ms]
Sep 25 02:52:03.072: INFO: Got endpoints: latency-svc-s75tv [63.746612ms]
Sep 25 02:52:03.073: INFO: Created: latency-svc-898b4
Sep 25 02:52:03.075: INFO: Got endpoints: latency-svc-898b4 [59.564598ms]
Sep 25 02:52:03.077: INFO: Created: latency-svc-twwbl
Sep 25 02:52:03.084: INFO: Created: latency-svc-rgbcl
Sep 25 02:52:03.088: INFO: Created: latency-svc-jxw4w
Sep 25 02:52:03.091: INFO: Created: latency-svc-76m7r
Sep 25 02:52:03.094: INFO: Created: latency-svc-r9kht
Sep 25 02:52:03.097: INFO: Created: latency-svc-67697
Sep 25 02:52:03.098: INFO: Created: latency-svc-9csw5
Sep 25 02:52:03.102: INFO: Created: latency-svc-dznn5
Sep 25 02:52:03.105: INFO: Created: latency-svc-xl52v
Sep 25 02:52:03.109: INFO: Created: latency-svc-bcrc4
Sep 25 02:52:03.119: INFO: Created: latency-svc-9hv9f
Sep 25 02:52:03.123: INFO: Got endpoints: latency-svc-twwbl [96.925196ms]
Sep 25 02:52:03.124: INFO: Created: latency-svc-7lcj4
Sep 25 02:52:03.130: INFO: Created: latency-svc-f45ms
Sep 25 02:52:03.134: INFO: Created: latency-svc-67ddw
Sep 25 02:52:03.138: INFO: Created: latency-svc-k7fvn
Sep 25 02:52:03.141: INFO: Created: latency-svc-98vls
Sep 25 02:52:03.171: INFO: Got endpoints: latency-svc-rgbcl [141.21587ms]
Sep 25 02:52:03.178: INFO: Created: latency-svc-mjnkh
Sep 25 02:52:03.220: INFO: Got endpoints: latency-svc-jxw4w [187.609277ms]
Sep 25 02:52:03.225: INFO: Created: latency-svc-h7pfw
Sep 25 02:52:03.271: INFO: Got endpoints: latency-svc-76m7r [236.753073ms]
Sep 25 02:52:03.277: INFO: Created: latency-svc-ztb7w
Sep 25 02:52:03.321: INFO: Got endpoints: latency-svc-r9kht [282.065542ms]
Sep 25 02:52:03.326: INFO: Created: latency-svc-tf7nj
Sep 25 02:52:03.370: INFO: Got endpoints: latency-svc-67697 [328.065364ms]
Sep 25 02:52:03.377: INFO: Created: latency-svc-27p7g
Sep 25 02:52:03.420: INFO: Got endpoints: latency-svc-9csw5 [375.441892ms]
Sep 25 02:52:03.429: INFO: Created: latency-svc-th52h
Sep 25 02:52:03.470: INFO: Got endpoints: latency-svc-dznn5 [420.747331ms]
Sep 25 02:52:03.475: INFO: Created: latency-svc-m8495
Sep 25 02:52:03.520: INFO: Got endpoints: latency-svc-xl52v [465.94932ms]
Sep 25 02:52:03.527: INFO: Created: latency-svc-xg98m
Sep 25 02:52:03.570: INFO: Got endpoints: latency-svc-bcrc4 [512.666511ms]
Sep 25 02:52:03.575: INFO: Created: latency-svc-2rp6q
Sep 25 02:52:03.622: INFO: Got endpoints: latency-svc-9hv9f [558.27999ms]
Sep 25 02:52:03.629: INFO: Created: latency-svc-9nzfr
Sep 25 02:52:03.671: INFO: Got endpoints: latency-svc-7lcj4 [605.009469ms]
Sep 25 02:52:03.677: INFO: Created: latency-svc-jkc7g
Sep 25 02:52:03.723: INFO: Got endpoints: latency-svc-f45ms [654.26234ms]
Sep 25 02:52:03.731: INFO: Created: latency-svc-lh7m6
Sep 25 02:52:03.772: INFO: Got endpoints: latency-svc-67ddw [699.750605ms]
Sep 25 02:52:03.778: INFO: Created: latency-svc-hx4xk
Sep 25 02:52:03.820: INFO: Got endpoints: latency-svc-k7fvn [744.828933ms]
Sep 25 02:52:03.829: INFO: Created: latency-svc-nmpcj
Sep 25 02:52:03.871: INFO: Got endpoints: latency-svc-98vls [748.569437ms]
Sep 25 02:52:03.877: INFO: Created: latency-svc-g5lsc
Sep 25 02:52:03.920: INFO: Got endpoints: latency-svc-mjnkh [749.364689ms]
Sep 25 02:52:03.925: INFO: Created: latency-svc-wrd76
Sep 25 02:52:03.971: INFO: Got endpoints: latency-svc-h7pfw [750.930388ms]
Sep 25 02:52:03.979: INFO: Created: latency-svc-gfp97
Sep 25 02:52:04.022: INFO: Got endpoints: latency-svc-ztb7w [750.915356ms]
Sep 25 02:52:04.028: INFO: Created: latency-svc-2wvhd
Sep 25 02:52:04.072: INFO: Got endpoints: latency-svc-tf7nj [750.645151ms]
Sep 25 02:52:04.078: INFO: Created: latency-svc-lh2tt
Sep 25 02:52:04.121: INFO: Got endpoints: latency-svc-27p7g [751.205147ms]
Sep 25 02:52:04.128: INFO: Created: latency-svc-h9swk
Sep 25 02:52:04.170: INFO: Got endpoints: latency-svc-th52h [749.457383ms]
Sep 25 02:52:04.175: INFO: Created: latency-svc-7flns
Sep 25 02:52:04.221: INFO: Got endpoints: latency-svc-m8495 [751.032131ms]
Sep 25 02:52:04.228: INFO: Created: latency-svc-gv5gh
Sep 25 02:52:04.270: INFO: Got endpoints: latency-svc-xg98m [749.417821ms]
Sep 25 02:52:04.276: INFO: Created: latency-svc-hlq6t
Sep 25 02:52:04.321: INFO: Got endpoints: latency-svc-2rp6q [750.971161ms]
Sep 25 02:52:04.330: INFO: Created: latency-svc-mmv4m
Sep 25 02:52:04.372: INFO: Got endpoints: latency-svc-9nzfr [749.611207ms]
Sep 25 02:52:04.381: INFO: Created: latency-svc-5clp7
Sep 25 02:52:04.421: INFO: Got endpoints: latency-svc-jkc7g [750.038302ms]
Sep 25 02:52:04.426: INFO: Created: latency-svc-zkngq
Sep 25 02:52:04.471: INFO: Got endpoints: latency-svc-lh7m6 [747.645027ms]
Sep 25 02:52:04.477: INFO: Created: latency-svc-q8x8z
Sep 25 02:52:04.521: INFO: Got endpoints: latency-svc-hx4xk [749.519542ms]
Sep 25 02:52:04.531: INFO: Created: latency-svc-hf9m5
Sep 25 02:52:04.570: INFO: Got endpoints: latency-svc-nmpcj [750.189534ms]
Sep 25 02:52:04.578: INFO: Created: latency-svc-7wcj5
Sep 25 02:52:04.621: INFO: Got endpoints: latency-svc-g5lsc [749.375148ms]
Sep 25 02:52:04.633: INFO: Created: latency-svc-n5m2n
Sep 25 02:52:04.670: INFO: Got endpoints: latency-svc-wrd76 [750.021456ms]
Sep 25 02:52:04.690: INFO: Created: latency-svc-g6zcc
Sep 25 02:52:04.749: INFO: Got endpoints: latency-svc-gfp97 [777.90227ms]
Sep 25 02:52:04.757: INFO: Created: latency-svc-lfd9d
Sep 25 02:52:04.773: INFO: Got endpoints: latency-svc-2wvhd [747.782863ms]
Sep 25 02:52:04.780: INFO: Created: latency-svc-nlpsk
Sep 25 02:52:04.819: INFO: Got endpoints: latency-svc-lh2tt [747.242241ms]
Sep 25 02:52:04.825: INFO: Created: latency-svc-gwdl2
Sep 25 02:52:04.869: INFO: Got endpoints: latency-svc-h9swk [747.934856ms]
Sep 25 02:52:04.875: INFO: Created: latency-svc-nvmkr
Sep 25 02:52:04.920: INFO: Got endpoints: latency-svc-7flns [750.423751ms]
Sep 25 02:52:04.927: INFO: Created: latency-svc-tgxsh
Sep 25 02:52:04.970: INFO: Got endpoints: latency-svc-gv5gh [749.319337ms]
Sep 25 02:52:04.980: INFO: Created: latency-svc-www5v
Sep 25 02:52:05.022: INFO: Got endpoints: latency-svc-hlq6t [752.064146ms]
Sep 25 02:52:05.028: INFO: Created: latency-svc-pd4ft
Sep 25 02:52:05.071: INFO: Got endpoints: latency-svc-mmv4m [748.878485ms]
Sep 25 02:52:05.076: INFO: Created: latency-svc-d8wgj
Sep 25 02:52:05.120: INFO: Got endpoints: latency-svc-5clp7 [747.888046ms]
Sep 25 02:52:05.127: INFO: Created: latency-svc-4mhrz
Sep 25 02:52:05.172: INFO: Got endpoints: latency-svc-zkngq [751.016576ms]
Sep 25 02:52:05.178: INFO: Created: latency-svc-wzc47
Sep 25 02:52:05.222: INFO: Got endpoints: latency-svc-q8x8z [750.982032ms]
Sep 25 02:52:05.227: INFO: Created: latency-svc-n9rts
Sep 25 02:52:05.271: INFO: Got endpoints: latency-svc-hf9m5 [749.72813ms]
Sep 25 02:52:05.276: INFO: Created: latency-svc-stjf4
Sep 25 02:52:05.321: INFO: Got endpoints: latency-svc-7wcj5 [750.098622ms]
Sep 25 02:52:05.330: INFO: Created: latency-svc-c2mms
Sep 25 02:52:05.370: INFO: Got endpoints: latency-svc-n5m2n [749.810341ms]
Sep 25 02:52:05.377: INFO: Created: latency-svc-vcpwv
Sep 25 02:52:05.421: INFO: Got endpoints: latency-svc-g6zcc [750.549554ms]
Sep 25 02:52:05.428: INFO: Created: latency-svc-pccnl
Sep 25 02:52:05.471: INFO: Got endpoints: latency-svc-lfd9d [722.061797ms]
Sep 25 02:52:05.481: INFO: Created: latency-svc-7qgdj
Sep 25 02:52:05.520: INFO: Got endpoints: latency-svc-nlpsk [746.82565ms]
Sep 25 02:52:05.529: INFO: Created: latency-svc-wv5fm
Sep 25 02:52:05.570: INFO: Got endpoints: latency-svc-gwdl2 [750.49444ms]
Sep 25 02:52:05.576: INFO: Created: latency-svc-nbhzh
Sep 25 02:52:05.621: INFO: Got endpoints: latency-svc-nvmkr [751.655512ms]
Sep 25 02:52:05.627: INFO: Created: latency-svc-4m7rn
Sep 25 02:52:05.671: INFO: Got endpoints: latency-svc-tgxsh [750.264586ms]
Sep 25 02:52:05.676: INFO: Created: latency-svc-s6xph
Sep 25 02:52:05.719: INFO: Got endpoints: latency-svc-www5v [748.854472ms]
Sep 25 02:52:05.725: INFO: Created: latency-svc-z8r2l
Sep 25 02:52:05.772: INFO: Got endpoints: latency-svc-pd4ft [749.866221ms]
Sep 25 02:52:05.780: INFO: Created: latency-svc-shqzk
Sep 25 02:52:05.820: INFO: Got endpoints: latency-svc-d8wgj [749.656268ms]
Sep 25 02:52:05.828: INFO: Created: latency-svc-bx2pk
Sep 25 02:52:05.870: INFO: Got endpoints: latency-svc-4mhrz [750.03645ms]
Sep 25 02:52:05.876: INFO: Created: latency-svc-msrnr
Sep 25 02:52:05.922: INFO: Got endpoints: latency-svc-wzc47 [750.001974ms]
Sep 25 02:52:05.928: INFO: Created: latency-svc-j6zn5
Sep 25 02:52:05.970: INFO: Got endpoints: latency-svc-n9rts [747.852302ms]
Sep 25 02:52:05.981: INFO: Created: latency-svc-jnf8b
Sep 25 02:52:06.020: INFO: Got endpoints: latency-svc-stjf4 [748.89802ms]
Sep 25 02:52:06.026: INFO: Created: latency-svc-864l2
Sep 25 02:52:06.071: INFO: Got endpoints: latency-svc-c2mms [750.624791ms]
Sep 25 02:52:06.084: INFO: Created: latency-svc-xjjg8
Sep 25 02:52:06.123: INFO: Got endpoints: latency-svc-vcpwv [751.964381ms]
Sep 25 02:52:06.130: INFO: Created: latency-svc-jwwx6
Sep 25 02:52:06.171: INFO: Got endpoints: latency-svc-pccnl [750.03288ms]
Sep 25 02:52:06.179: INFO: Created: latency-svc-6j9hb
Sep 25 02:52:06.223: INFO: Got endpoints: latency-svc-7qgdj [751.131252ms]
Sep 25 02:52:06.229: INFO: Created: latency-svc-cjrzc
Sep 25 02:52:06.270: INFO: Got endpoints: latency-svc-wv5fm [750.331679ms]
Sep 25 02:52:06.274: INFO: Created: latency-svc-nd4tj
Sep 25 02:52:06.321: INFO: Got endpoints: latency-svc-nbhzh [751.418653ms]
Sep 25 02:52:06.329: INFO: Created: latency-svc-744vh
Sep 25 02:52:06.372: INFO: Got endpoints: latency-svc-4m7rn [750.612788ms]
Sep 25 02:52:06.380: INFO: Created: latency-svc-djvmd
Sep 25 02:52:06.422: INFO: Got endpoints: latency-svc-s6xph [751.027927ms]
Sep 25 02:52:06.429: INFO: Created: latency-svc-km6mw
Sep 25 02:52:06.471: INFO: Got endpoints: latency-svc-z8r2l [751.266361ms]
Sep 25 02:52:06.479: INFO: Created: latency-svc-8s8wq
Sep 25 02:52:06.521: INFO: Got endpoints: latency-svc-shqzk [749.086205ms]
Sep 25 02:52:06.527: INFO: Created: latency-svc-clwn4
Sep 25 02:52:06.570: INFO: Got endpoints: latency-svc-bx2pk [749.767687ms]
Sep 25 02:52:06.575: INFO: Created: latency-svc-h4fwm
Sep 25 02:52:06.621: INFO: Got endpoints: latency-svc-msrnr [750.578324ms]
Sep 25 02:52:06.631: INFO: Created: latency-svc-mp26k
Sep 25 02:52:06.670: INFO: Got endpoints: latency-svc-j6zn5 [747.716675ms]
Sep 25 02:52:06.676: INFO: Created: latency-svc-tk6nj
Sep 25 02:52:06.722: INFO: Got endpoints: latency-svc-jnf8b [752.113806ms]
Sep 25 02:52:06.730: INFO: Created: latency-svc-tv8p2
Sep 25 02:52:06.771: INFO: Got endpoints: latency-svc-864l2 [750.725214ms]
Sep 25 02:52:06.777: INFO: Created: latency-svc-gsknd
Sep 25 02:52:06.821: INFO: Got endpoints: latency-svc-xjjg8 [749.477475ms]
Sep 25 02:52:06.828: INFO: Created: latency-svc-jd67k
Sep 25 02:52:06.871: INFO: Got endpoints: latency-svc-jwwx6 [748.368305ms]
Sep 25 02:52:06.877: INFO: Created: latency-svc-fw4rm
Sep 25 02:52:06.922: INFO: Got endpoints: latency-svc-6j9hb [751.67175ms]
Sep 25 02:52:06.930: INFO: Created: latency-svc-qpzr2
Sep 25 02:52:06.970: INFO: Got endpoints: latency-svc-cjrzc [747.104562ms]
Sep 25 02:52:06.975: INFO: Created: latency-svc-7wxml
Sep 25 02:52:07.021: INFO: Got endpoints: latency-svc-nd4tj [750.869204ms]
Sep 25 02:52:07.029: INFO: Created: latency-svc-cbxmd
Sep 25 02:52:07.071: INFO: Got endpoints: latency-svc-744vh [749.72995ms]
Sep 25 02:52:07.078: INFO: Created: latency-svc-vvmg5
Sep 25 02:52:07.122: INFO: Got endpoints: latency-svc-djvmd [750.172175ms]
Sep 25 02:52:07.128: INFO: Created: latency-svc-m75br
Sep 25 02:52:07.171: INFO: Got endpoints: latency-svc-km6mw [748.754546ms]
Sep 25 02:52:07.178: INFO: Created: latency-svc-blshp
Sep 25 02:52:07.220: INFO: Got endpoints: latency-svc-8s8wq [749.667621ms]
Sep 25 02:52:07.229: INFO: Created: latency-svc-p6mvt
Sep 25 02:52:07.271: INFO: Got endpoints: latency-svc-clwn4 [749.554545ms]
Sep 25 02:52:07.283: INFO: Created: latency-svc-qccgq
Sep 25 02:52:07.320: INFO: Got endpoints: latency-svc-h4fwm [750.178726ms]
Sep 25 02:52:07.326: INFO: Created: latency-svc-l6gnh
Sep 25 02:52:07.369: INFO: Got endpoints: latency-svc-mp26k [748.026392ms]
Sep 25 02:52:07.375: INFO: Created: latency-svc-fz66m
Sep 25 02:52:07.421: INFO: Got endpoints: latency-svc-tk6nj [751.087068ms]
Sep 25 02:52:07.430: INFO: Created: latency-svc-zv2zp
Sep 25 02:52:07.471: INFO: Got endpoints: latency-svc-tv8p2 [749.139678ms]
Sep 25 02:52:07.478: INFO: Created: latency-svc-jnq4t
Sep 25 02:52:07.522: INFO: Got endpoints: latency-svc-gsknd [751.659613ms]
Sep 25 02:52:07.530: INFO: Created: latency-svc-pgxw6
Sep 25 02:52:07.571: INFO: Got endpoints: latency-svc-jd67k [750.478066ms]
Sep 25 02:52:07.577: INFO: Created: latency-svc-d46gn
Sep 25 02:52:07.622: INFO: Got endpoints: latency-svc-fw4rm [750.656223ms]
Sep 25 02:52:07.629: INFO: Created: latency-svc-fghm4
Sep 25 02:52:07.670: INFO: Got endpoints: latency-svc-qpzr2 [747.307558ms]
Sep 25 02:52:07.676: INFO: Created: latency-svc-vrqt7
Sep 25 02:52:07.722: INFO: Got endpoints: latency-svc-7wxml [752.277065ms]
Sep 25 02:52:07.729: INFO: Created: latency-svc-vg42v
Sep 25 02:52:07.771: INFO: Got endpoints: latency-svc-cbxmd [749.202731ms]
Sep 25 02:52:07.778: INFO: Created: latency-svc-xwvnc
Sep 25 02:52:07.819: INFO: Got endpoints: latency-svc-vvmg5 [748.23912ms]
Sep 25 02:52:07.826: INFO: Created: latency-svc-2jh7r
Sep 25 02:52:07.870: INFO: Got endpoints: latency-svc-m75br [747.949306ms]
Sep 25 02:52:07.877: INFO: Created: latency-svc-k75vt
Sep 25 02:52:07.920: INFO: Got endpoints: latency-svc-blshp [749.499966ms]
Sep 25 02:52:07.926: INFO: Created: latency-svc-gxpnz
Sep 25 02:52:07.969: INFO: Got endpoints: latency-svc-p6mvt [748.781383ms]
Sep 25 02:52:07.973: INFO: Created: latency-svc-79l59
Sep 25 02:52:08.021: INFO: Got endpoints: latency-svc-qccgq [750.008473ms]
Sep 25 02:52:08.026: INFO: Created: latency-svc-d5w9t
Sep 25 02:52:08.071: INFO: Got endpoints: latency-svc-l6gnh [750.503844ms]
Sep 25 02:52:08.078: INFO: Created: latency-svc-k924t
Sep 25 02:52:08.121: INFO: Got endpoints: latency-svc-fz66m [751.486736ms]
Sep 25 02:52:08.130: INFO: Created: latency-svc-hmjfl
Sep 25 02:52:08.171: INFO: Got endpoints: latency-svc-zv2zp [750.183791ms]
Sep 25 02:52:08.178: INFO: Created: latency-svc-hmj7z
Sep 25 02:52:08.221: INFO: Got endpoints: latency-svc-jnq4t [749.952817ms]
Sep 25 02:52:08.228: INFO: Created: latency-svc-6n9z4
Sep 25 02:52:08.271: INFO: Got endpoints: latency-svc-pgxw6 [748.409136ms]
Sep 25 02:52:08.279: INFO: Created: latency-svc-znrzn
Sep 25 02:52:08.322: INFO: Got endpoints: latency-svc-d46gn [750.26569ms]
Sep 25 02:52:08.327: INFO: Created: latency-svc-bkbls
Sep 25 02:52:08.370: INFO: Got endpoints: latency-svc-fghm4 [748.172503ms]
Sep 25 02:52:08.377: INFO: Created: latency-svc-7p8m6
Sep 25 02:52:08.421: INFO: Got endpoints: latency-svc-vrqt7 [751.637396ms]
Sep 25 02:52:08.427: INFO: Created: latency-svc-8zdqs
Sep 25 02:52:08.471: INFO: Got endpoints: latency-svc-vg42v [748.746248ms]
Sep 25 02:52:08.479: INFO: Created: latency-svc-l9d8v
Sep 25 02:52:08.522: INFO: Got endpoints: latency-svc-xwvnc [751.022606ms]
Sep 25 02:52:08.530: INFO: Created: latency-svc-6pn6f
Sep 25 02:52:08.570: INFO: Got endpoints: latency-svc-2jh7r [750.299944ms]
Sep 25 02:52:08.575: INFO: Created: latency-svc-n69jw
Sep 25 02:52:08.620: INFO: Got endpoints: latency-svc-k75vt [750.152353ms]
Sep 25 02:52:08.626: INFO: Created: latency-svc-s6lsc
Sep 25 02:52:08.672: INFO: Got endpoints: latency-svc-gxpnz [751.472997ms]
Sep 25 02:52:08.679: INFO: Created: latency-svc-2qpq5
Sep 25 02:52:08.721: INFO: Got endpoints: latency-svc-79l59 [751.256811ms]
Sep 25 02:52:08.727: INFO: Created: latency-svc-695vx
Sep 25 02:52:08.772: INFO: Got endpoints: latency-svc-d5w9t [751.063713ms]
Sep 25 02:52:08.778: INFO: Created: latency-svc-tlbq2
Sep 25 02:52:08.820: INFO: Got endpoints: latency-svc-k924t [748.939667ms]
Sep 25 02:52:08.825: INFO: Created: latency-svc-jkvjv
Sep 25 02:52:08.870: INFO: Got endpoints: latency-svc-hmjfl [748.869636ms]
Sep 25 02:52:08.877: INFO: Created: latency-svc-ft7zr
Sep 25 02:52:08.920: INFO: Got endpoints: latency-svc-hmj7z [748.760988ms]
Sep 25 02:52:08.927: INFO: Created: latency-svc-rsmf4
Sep 25 02:52:08.969: INFO: Got endpoints: latency-svc-6n9z4 [747.753799ms]
Sep 25 02:52:08.976: INFO: Created: latency-svc-q4t4r
Sep 25 02:52:09.021: INFO: Got endpoints: latency-svc-znrzn [750.301802ms]
Sep 25 02:52:09.027: INFO: Created: latency-svc-2bmnq
Sep 25 02:52:09.071: INFO: Got endpoints: latency-svc-bkbls [749.255356ms]
Sep 25 02:52:09.077: INFO: Created: latency-svc-qhsdr
Sep 25 02:52:09.120: INFO: Got endpoints: latency-svc-7p8m6 [750.311056ms]
Sep 25 02:52:09.127: INFO: Created: latency-svc-bmh8l
Sep 25 02:52:09.171: INFO: Got endpoints: latency-svc-8zdqs [749.24411ms]
Sep 25 02:52:09.179: INFO: Created: latency-svc-tc6x4
Sep 25 02:52:09.222: INFO: Got endpoints: latency-svc-l9d8v [750.738413ms]
Sep 25 02:52:09.234: INFO: Created: latency-svc-znxr6
Sep 25 02:52:09.272: INFO: Got endpoints: latency-svc-6pn6f [750.011649ms]
Sep 25 02:52:09.279: INFO: Created: latency-svc-jb4hv
Sep 25 02:52:09.320: INFO: Got endpoints: latency-svc-n69jw [750.510105ms]
Sep 25 02:52:09.329: INFO: Created: latency-svc-nf6hp
Sep 25 02:52:09.371: INFO: Got endpoints: latency-svc-s6lsc [751.259074ms]
Sep 25 02:52:09.378: INFO: Created: latency-svc-zw6g4
Sep 25 02:52:09.421: INFO: Got endpoints: latency-svc-2qpq5 [749.304684ms]
Sep 25 02:52:09.427: INFO: Created: latency-svc-tfsmq
Sep 25 02:52:09.471: INFO: Got endpoints: latency-svc-695vx [750.621589ms]
Sep 25 02:52:09.480: INFO: Created: latency-svc-r7kxk
Sep 25 02:52:09.521: INFO: Got endpoints: latency-svc-tlbq2 [749.355482ms]
Sep 25 02:52:09.527: INFO: Created: latency-svc-422tw
Sep 25 02:52:09.572: INFO: Got endpoints: latency-svc-jkvjv [750.839892ms]
Sep 25 02:52:09.579: INFO: Created: latency-svc-5vcjg
Sep 25 02:52:09.622: INFO: Got endpoints: latency-svc-ft7zr [752.224848ms]
Sep 25 02:52:09.630: INFO: Created: latency-svc-47vwh
Sep 25 02:52:09.671: INFO: Got endpoints: latency-svc-rsmf4 [750.291051ms]
Sep 25 02:52:09.677: INFO: Created: latency-svc-nwthh
Sep 25 02:52:09.724: INFO: Got endpoints: latency-svc-q4t4r [755.323653ms]
Sep 25 02:52:09.731: INFO: Created: latency-svc-vb8vb
Sep 25 02:52:09.771: INFO: Got endpoints: latency-svc-2bmnq [749.402373ms]
Sep 25 02:52:09.777: INFO: Created: latency-svc-cwdx4
Sep 25 02:52:09.821: INFO: Got endpoints: latency-svc-qhsdr [749.550361ms]
Sep 25 02:52:09.827: INFO: Created: latency-svc-wm6vs
Sep 25 02:52:09.871: INFO: Got endpoints: latency-svc-bmh8l [749.987398ms]
Sep 25 02:52:09.877: INFO: Created: latency-svc-nz8sn
Sep 25 02:52:09.921: INFO: Got endpoints: latency-svc-tc6x4 [750.282219ms]
Sep 25 02:52:09.935: INFO: Created: latency-svc-klnsn
Sep 25 02:52:09.971: INFO: Got endpoints: latency-svc-znxr6 [748.928749ms]
Sep 25 02:52:09.976: INFO: Created: latency-svc-979wj
Sep 25 02:52:10.021: INFO: Got endpoints: latency-svc-jb4hv [749.352236ms]
Sep 25 02:52:10.026: INFO: Created: latency-svc-l5f9p
Sep 25 02:52:10.070: INFO: Got endpoints: latency-svc-nf6hp [749.461796ms]
Sep 25 02:52:10.075: INFO: Created: latency-svc-dknwx
Sep 25 02:52:10.121: INFO: Got endpoints: latency-svc-zw6g4 [749.933548ms]
Sep 25 02:52:10.126: INFO: Created: latency-svc-bm42j
Sep 25 02:52:10.171: INFO: Got endpoints: latency-svc-tfsmq [749.893353ms]
Sep 25 02:52:10.179: INFO: Created: latency-svc-drm4n
Sep 25 02:52:10.221: INFO: Got endpoints: latency-svc-r7kxk [749.337209ms]
Sep 25 02:52:10.228: INFO: Created: latency-svc-v62qz
Sep 25 02:52:10.272: INFO: Got endpoints: latency-svc-422tw [750.304418ms]
Sep 25 02:52:10.278: INFO: Created: latency-svc-dz8jv
Sep 25 02:52:10.319: INFO: Got endpoints: latency-svc-5vcjg [747.738707ms]
Sep 25 02:52:10.326: INFO: Created: latency-svc-7zz8g
Sep 25 02:52:10.371: INFO: Got endpoints: latency-svc-47vwh [748.843426ms]
Sep 25 02:52:10.377: INFO: Created: latency-svc-rj6pr
Sep 25 02:52:10.421: INFO: Got endpoints: latency-svc-nwthh [749.93135ms]
Sep 25 02:52:10.428: INFO: Created: latency-svc-gks42
Sep 25 02:52:10.471: INFO: Got endpoints: latency-svc-vb8vb [746.91388ms]
Sep 25 02:52:10.480: INFO: Created: latency-svc-7bx27
Sep 25 02:52:10.520: INFO: Got endpoints: latency-svc-cwdx4 [748.881862ms]
Sep 25 02:52:10.528: INFO: Created: latency-svc-jtxpk
Sep 25 02:52:10.571: INFO: Got endpoints: latency-svc-wm6vs [750.672086ms]
Sep 25 02:52:10.578: INFO: Created: latency-svc-lgzzt
Sep 25 02:52:10.623: INFO: Got endpoints: latency-svc-nz8sn [751.929045ms]
Sep 25 02:52:10.630: INFO: Created: latency-svc-7dhkg
Sep 25 02:52:10.672: INFO: Got endpoints: latency-svc-klnsn [750.70311ms]
Sep 25 02:52:10.680: INFO: Created: latency-svc-24w8z
Sep 25 02:52:10.720: INFO: Got endpoints: latency-svc-979wj [749.237745ms]
Sep 25 02:52:10.727: INFO: Created: latency-svc-fbkf2
Sep 25 02:52:10.771: INFO: Got endpoints: latency-svc-l5f9p [749.826415ms]
Sep 25 02:52:10.820: INFO: Got endpoints: latency-svc-dknwx [749.854839ms]
Sep 25 02:52:10.872: INFO: Got endpoints: latency-svc-bm42j [750.14827ms]
Sep 25 02:52:10.920: INFO: Got endpoints: latency-svc-drm4n [748.825272ms]
Sep 25 02:52:10.973: INFO: Got endpoints: latency-svc-v62qz [752.134648ms]
Sep 25 02:52:11.021: INFO: Got endpoints: latency-svc-dz8jv [749.174205ms]
Sep 25 02:52:11.072: INFO: Got endpoints: latency-svc-7zz8g [752.484192ms]
Sep 25 02:52:11.121: INFO: Got endpoints: latency-svc-rj6pr [749.978128ms]
Sep 25 02:52:11.170: INFO: Got endpoints: latency-svc-gks42 [749.718731ms]
Sep 25 02:52:11.224: INFO: Got endpoints: latency-svc-7bx27 [752.601293ms]
Sep 25 02:52:11.271: INFO: Got endpoints: latency-svc-jtxpk [751.67014ms]
Sep 25 02:52:11.321: INFO: Got endpoints: latency-svc-lgzzt [749.454007ms]
Sep 25 02:52:11.370: INFO: Got endpoints: latency-svc-7dhkg [747.144711ms]
Sep 25 02:52:11.422: INFO: Got endpoints: latency-svc-24w8z [749.7602ms]
Sep 25 02:52:11.472: INFO: Got endpoints: latency-svc-fbkf2 [751.869986ms]
Sep 25 02:52:11.472: INFO: Latencies: [9.65051ms 16.061491ms 23.001355ms 26.23889ms 31.905154ms 35.842242ms 38.635088ms 39.084927ms 43.75184ms 44.92559ms 51.206033ms 59.564598ms 60.169036ms 63.746612ms 66.030527ms 66.170091ms 66.85505ms 71.3797ms 72.139439ms 72.587423ms 73.194478ms 73.452225ms 74.271277ms 75.031699ms 75.092334ms 75.812504ms 76.055424ms 77.078984ms 77.280556ms 77.52573ms 77.684764ms 82.486488ms 96.925196ms 141.21587ms 187.609277ms 236.753073ms 282.065542ms 328.065364ms 375.441892ms 420.747331ms 465.94932ms 512.666511ms 558.27999ms 605.009469ms 654.26234ms 699.750605ms 722.061797ms 744.828933ms 746.82565ms 746.91388ms 747.104562ms 747.144711ms 747.242241ms 747.307558ms 747.645027ms 747.716675ms 747.738707ms 747.753799ms 747.782863ms 747.852302ms 747.888046ms 747.934856ms 747.949306ms 748.026392ms 748.172503ms 748.23912ms 748.368305ms 748.409136ms 748.569437ms 748.746248ms 748.754546ms 748.760988ms 748.781383ms 748.825272ms 748.843426ms 748.854472ms 748.869636ms 748.878485ms 748.881862ms 748.89802ms 748.928749ms 748.939667ms 749.086205ms 749.139678ms 749.174205ms 749.202731ms 749.237745ms 749.24411ms 749.255356ms 749.304684ms 749.319337ms 749.337209ms 749.352236ms 749.355482ms 749.364689ms 749.375148ms 749.402373ms 749.417821ms 749.454007ms 749.457383ms 749.461796ms 749.477475ms 749.499966ms 749.519542ms 749.550361ms 749.554545ms 749.611207ms 749.656268ms 749.667621ms 749.718731ms 749.72813ms 749.72995ms 749.7602ms 749.767687ms 749.810341ms 749.826415ms 749.854839ms 749.866221ms 749.893353ms 749.93135ms 749.933548ms 749.952817ms 749.978128ms 749.987398ms 750.001974ms 750.008473ms 750.011649ms 750.021456ms 750.03288ms 750.03645ms 750.038302ms 750.098622ms 750.14827ms 750.152353ms 750.172175ms 750.178726ms 750.183791ms 750.189534ms 750.264586ms 750.26569ms 750.282219ms 750.291051ms 750.299944ms 750.301802ms 750.304418ms 750.311056ms 750.331679ms 750.423751ms 750.478066ms 750.49444ms 750.503844ms 750.510105ms 750.549554ms 750.578324ms 750.612788ms 750.621589ms 750.624791ms 750.645151ms 750.656223ms 750.672086ms 750.70311ms 750.725214ms 750.738413ms 750.839892ms 750.869204ms 750.915356ms 750.930388ms 750.971161ms 750.982032ms 751.016576ms 751.022606ms 751.027927ms 751.032131ms 751.063713ms 751.087068ms 751.131252ms 751.205147ms 751.256811ms 751.259074ms 751.266361ms 751.418653ms 751.472997ms 751.486736ms 751.637396ms 751.655512ms 751.659613ms 751.67014ms 751.67175ms 751.869986ms 751.929045ms 751.964381ms 752.064146ms 752.113806ms 752.134648ms 752.224848ms 752.277065ms 752.484192ms 752.601293ms 755.323653ms 777.90227ms]
Sep 25 02:52:11.472: INFO: 50 %ile: 749.461796ms
Sep 25 02:52:11.472: INFO: 90 %ile: 751.418653ms
Sep 25 02:52:11.472: INFO: 99 %ile: 755.323653ms
Sep 25 02:52:11.472: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:52:11.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9715" for this suite.

â€¢ [SLOW TEST:11.869 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":61,"skipped":849,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:52:11.484: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Sep 25 02:52:11.613: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-888863303 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:52:11.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5287" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":62,"skipped":850,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:52:11.710: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-1895f7ea-6153-4a01-a229-04e557e6e272 in namespace container-probe-7797
Sep 25 02:52:15.864: INFO: Started pod liveness-1895f7ea-6153-4a01-a229-04e557e6e272 in namespace container-probe-7797
STEP: checking the pod's current state and verifying that restartCount is present
Sep 25 02:52:15.870: INFO: Initial restart count of pod liveness-1895f7ea-6153-4a01-a229-04e557e6e272 is 0
Sep 25 02:52:31.907: INFO: Restart count of pod container-probe-7797/liveness-1895f7ea-6153-4a01-a229-04e557e6e272 is now 1 (16.036994548s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:52:31.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7797" for this suite.

â€¢ [SLOW TEST:20.217 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":63,"skipped":854,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:52:31.927: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1356.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1356.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1356.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1356.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1356.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1356.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1356.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1356.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 119.61.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.61.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.61.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.61.119_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1356.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1356.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1356.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1356.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1356.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1356.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1356.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1356.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1356.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1356.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 119.61.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.61.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.61.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.61.119_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 25 02:52:38.169: INFO: DNS probes using dns-1356/dns-test-68bd040f-fefc-4009-9c82-06baa6390a98 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:52:38.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1356" for this suite.

â€¢ [SLOW TEST:6.285 seconds]
[sig-network] DNS
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":64,"skipped":858,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:52:38.212: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8955
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8955
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8955
Sep 25 02:52:38.358: INFO: Found 0 stateful pods, waiting for 1
Sep 25 02:52:48.363: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 25 02:52:48.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-8955 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 25 02:52:48.564: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 25 02:52:48.564: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 25 02:52:48.564: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 25 02:52:48.568: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 25 02:52:58.573: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 25 02:52:58.573: INFO: Waiting for statefulset status.replicas updated to 0
Sep 25 02:52:58.594: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999244s
Sep 25 02:52:59.599: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989879587s
Sep 25 02:53:00.605: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984294171s
Sep 25 02:53:01.611: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.978884196s
Sep 25 02:53:02.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.972846611s
Sep 25 02:53:03.622: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.967680718s
Sep 25 02:53:04.626: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.962317547s
Sep 25 02:53:05.631: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.95746167s
Sep 25 02:53:06.635: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.95258795s
Sep 25 02:53:07.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.336325ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8955
Sep 25 02:53:08.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-8955 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 25 02:53:08.815: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 25 02:53:08.815: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 25 02:53:08.815: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 25 02:53:08.818: INFO: Found 1 stateful pods, waiting for 3
Sep 25 02:53:18.825: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 02:53:18.825: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 02:53:18.825: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 25 02:53:18.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-8955 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 25 02:53:19.005: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 25 02:53:19.005: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 25 02:53:19.005: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 25 02:53:19.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-8955 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 25 02:53:19.213: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 25 02:53:19.213: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 25 02:53:19.213: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 25 02:53:19.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-8955 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 25 02:53:19.414: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 25 02:53:19.414: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 25 02:53:19.414: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 25 02:53:19.414: INFO: Waiting for statefulset status.replicas updated to 0
Sep 25 02:53:19.417: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 25 02:53:29.425: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 25 02:53:29.425: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 25 02:53:29.425: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 25 02:53:29.439: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999633s
Sep 25 02:53:30.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992697217s
Sep 25 02:53:31.449: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987557037s
Sep 25 02:53:32.455: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982365066s
Sep 25 02:53:33.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976855228s
Sep 25 02:53:34.465: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971773303s
Sep 25 02:53:35.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966582207s
Sep 25 02:53:36.476: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96079112s
Sep 25 02:53:37.481: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955306039s
Sep 25 02:53:38.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.626402ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8955
Sep 25 02:53:39.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-8955 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 25 02:53:39.661: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 25 02:53:39.661: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 25 02:53:39.661: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 25 02:53:39.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-8955 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 25 02:53:39.871: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 25 02:53:39.871: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 25 02:53:39.871: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 25 02:53:39.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-8955 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 25 02:53:40.088: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 25 02:53:40.088: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 25 02:53:40.088: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 25 02:53:40.088: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Sep 25 02:54:00.104: INFO: Deleting all statefulset in ns statefulset-8955
Sep 25 02:54:00.107: INFO: Scaling statefulset ss to 0
Sep 25 02:54:00.116: INFO: Waiting for statefulset status.replicas updated to 0
Sep 25 02:54:00.118: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:54:00.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8955" for this suite.

â€¢ [SLOW TEST:81.923 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":65,"skipped":863,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:54:00.137: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Sep 25 02:54:04.298: INFO: Pod pod-hostip-c6576ed7-7fe2-4d08-9c97-d16c12b93baf has hostIP: 192.168.0.27
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:54:04.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5769" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":66,"skipped":872,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:54:04.309: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1998
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 25 02:54:08.477: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:54:08.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1998" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":67,"skipped":889,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:54:08.499: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2033
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 25 02:54:08.640: INFO: Waiting up to 5m0s for pod "pod-0a9d8aab-3eca-4e78-b41c-80e9e72beb26" in namespace "emptydir-2033" to be "success or failure"
Sep 25 02:54:08.650: INFO: Pod "pod-0a9d8aab-3eca-4e78-b41c-80e9e72beb26": Phase="Pending", Reason="", readiness=false. Elapsed: 10.137106ms
Sep 25 02:54:10.653: INFO: Pod "pod-0a9d8aab-3eca-4e78-b41c-80e9e72beb26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013792456s
Sep 25 02:54:12.657: INFO: Pod "pod-0a9d8aab-3eca-4e78-b41c-80e9e72beb26": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017792751s
Sep 25 02:54:14.660: INFO: Pod "pod-0a9d8aab-3eca-4e78-b41c-80e9e72beb26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020525113s
STEP: Saw pod success
Sep 25 02:54:14.660: INFO: Pod "pod-0a9d8aab-3eca-4e78-b41c-80e9e72beb26" satisfied condition "success or failure"
Sep 25 02:54:14.662: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-0a9d8aab-3eca-4e78-b41c-80e9e72beb26 container test-container: <nil>
STEP: delete the pod
Sep 25 02:54:14.687: INFO: Waiting for pod pod-0a9d8aab-3eca-4e78-b41c-80e9e72beb26 to disappear
Sep 25 02:54:14.689: INFO: Pod pod-0a9d8aab-3eca-4e78-b41c-80e9e72beb26 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:54:14.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2033" for this suite.

â€¢ [SLOW TEST:6.204 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":68,"skipped":897,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:54:14.704: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:54:18.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1988" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":69,"skipped":911,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:54:18.876: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4245
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-99ca68ab-e48c-4a5e-94e2-d40096f04ea9
STEP: Creating a pod to test consume secrets
Sep 25 02:54:19.025: INFO: Waiting up to 5m0s for pod "pod-secrets-26e1759d-3e56-47aa-8945-81dcdde06b2c" in namespace "secrets-4245" to be "success or failure"
Sep 25 02:54:19.027: INFO: Pod "pod-secrets-26e1759d-3e56-47aa-8945-81dcdde06b2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37283ms
Sep 25 02:54:21.030: INFO: Pod "pod-secrets-26e1759d-3e56-47aa-8945-81dcdde06b2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005678065s
STEP: Saw pod success
Sep 25 02:54:21.030: INFO: Pod "pod-secrets-26e1759d-3e56-47aa-8945-81dcdde06b2c" satisfied condition "success or failure"
Sep 25 02:54:21.033: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-secrets-26e1759d-3e56-47aa-8945-81dcdde06b2c container secret-volume-test: <nil>
STEP: delete the pod
Sep 25 02:54:21.043: INFO: Waiting for pod pod-secrets-26e1759d-3e56-47aa-8945-81dcdde06b2c to disappear
Sep 25 02:54:21.044: INFO: Pod pod-secrets-26e1759d-3e56-47aa-8945-81dcdde06b2c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:54:21.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4245" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":70,"skipped":914,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:54:21.052: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2293
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 02:54:21.176: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:54:21.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2293" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":71,"skipped":939,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:54:21.730: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-556p
STEP: Creating a pod to test atomic-volume-subpath
Sep 25 02:54:21.885: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-556p" in namespace "subpath-8939" to be "success or failure"
Sep 25 02:54:21.893: INFO: Pod "pod-subpath-test-secret-556p": Phase="Pending", Reason="", readiness=false. Elapsed: 7.751228ms
Sep 25 02:54:23.896: INFO: Pod "pod-subpath-test-secret-556p": Phase="Running", Reason="", readiness=true. Elapsed: 2.011065394s
Sep 25 02:54:25.901: INFO: Pod "pod-subpath-test-secret-556p": Phase="Running", Reason="", readiness=true. Elapsed: 4.016141707s
Sep 25 02:54:27.906: INFO: Pod "pod-subpath-test-secret-556p": Phase="Running", Reason="", readiness=true. Elapsed: 6.021115926s
Sep 25 02:54:29.911: INFO: Pod "pod-subpath-test-secret-556p": Phase="Running", Reason="", readiness=true. Elapsed: 8.02541285s
Sep 25 02:54:31.915: INFO: Pod "pod-subpath-test-secret-556p": Phase="Running", Reason="", readiness=true. Elapsed: 10.030058567s
Sep 25 02:54:33.920: INFO: Pod "pod-subpath-test-secret-556p": Phase="Running", Reason="", readiness=true. Elapsed: 12.03442488s
Sep 25 02:54:35.924: INFO: Pod "pod-subpath-test-secret-556p": Phase="Running", Reason="", readiness=true. Elapsed: 14.039021883s
Sep 25 02:54:37.929: INFO: Pod "pod-subpath-test-secret-556p": Phase="Running", Reason="", readiness=true. Elapsed: 16.043703164s
Sep 25 02:54:39.933: INFO: Pod "pod-subpath-test-secret-556p": Phase="Running", Reason="", readiness=true. Elapsed: 18.048095319s
Sep 25 02:54:41.938: INFO: Pod "pod-subpath-test-secret-556p": Phase="Running", Reason="", readiness=true. Elapsed: 20.052379417s
Sep 25 02:54:43.942: INFO: Pod "pod-subpath-test-secret-556p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.056918595s
STEP: Saw pod success
Sep 25 02:54:43.942: INFO: Pod "pod-subpath-test-secret-556p" satisfied condition "success or failure"
Sep 25 02:54:43.945: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-subpath-test-secret-556p container test-container-subpath-secret-556p: <nil>
STEP: delete the pod
Sep 25 02:54:43.962: INFO: Waiting for pod pod-subpath-test-secret-556p to disappear
Sep 25 02:54:43.966: INFO: Pod pod-subpath-test-secret-556p no longer exists
STEP: Deleting pod pod-subpath-test-secret-556p
Sep 25 02:54:43.966: INFO: Deleting pod "pod-subpath-test-secret-556p" in namespace "subpath-8939"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:54:43.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8939" for this suite.

â€¢ [SLOW TEST:22.243 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":72,"skipped":948,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:54:43.974: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5852
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5852
I0925 02:54:44.118026      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-5852, replica count: 2
Sep 25 02:54:47.168: INFO: Creating new exec pod
I0925 02:54:47.168747      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 25 02:54:50.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-5852 execpodxz685 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 25 02:54:50.394: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 25 02:54:50.394: INFO: stdout: ""
Sep 25 02:54:50.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-5852 execpodxz685 -- /bin/sh -x -c nc -zv -t -w 2 10.0.63.113 80'
Sep 25 02:54:50.608: INFO: stderr: "+ nc -zv -t -w 2 10.0.63.113 80\nConnection to 10.0.63.113 80 port [tcp/http] succeeded!\n"
Sep 25 02:54:50.608: INFO: stdout: ""
Sep 25 02:54:50.608: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:54:50.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5852" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:6.657 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":73,"skipped":974,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:54:50.632: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7739
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Sep 25 02:54:50.768: INFO: Waiting up to 5m0s for pod "downward-api-882f959a-da96-4021-b9e6-4f89a03f5596" in namespace "downward-api-7739" to be "success or failure"
Sep 25 02:54:50.772: INFO: Pod "downward-api-882f959a-da96-4021-b9e6-4f89a03f5596": Phase="Pending", Reason="", readiness=false. Elapsed: 3.592198ms
Sep 25 02:54:52.777: INFO: Pod "downward-api-882f959a-da96-4021-b9e6-4f89a03f5596": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008563709s
Sep 25 02:54:54.779: INFO: Pod "downward-api-882f959a-da96-4021-b9e6-4f89a03f5596": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010917094s
STEP: Saw pod success
Sep 25 02:54:54.779: INFO: Pod "downward-api-882f959a-da96-4021-b9e6-4f89a03f5596" satisfied condition "success or failure"
Sep 25 02:54:54.782: INFO: Trying to get logs from node biz-k8s-node-1 pod downward-api-882f959a-da96-4021-b9e6-4f89a03f5596 container dapi-container: <nil>
STEP: delete the pod
Sep 25 02:54:54.790: INFO: Waiting for pod downward-api-882f959a-da96-4021-b9e6-4f89a03f5596 to disappear
Sep 25 02:54:54.792: INFO: Pod downward-api-882f959a-da96-4021-b9e6-4f89a03f5596 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:54:54.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7739" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":74,"skipped":984,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:54:54.797: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-3932
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 25 02:54:54.917: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 25 02:55:15.010: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.100.168 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3932 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:55:15.010: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:55:16.106: INFO: Found all expected endpoints: [netserver-0]
Sep 25 02:55:16.110: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.100.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3932 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:55:16.110: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:55:17.218: INFO: Found all expected endpoints: [netserver-1]
Sep 25 02:55:17.223: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.100.121 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3932 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:55:17.223: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:55:18.316: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:55:18.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3932" for this suite.

â€¢ [SLOW TEST:23.528 seconds]
[sig-network] Networking
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":75,"skipped":991,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:55:18.326: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-5037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Sep 25 02:55:18.455: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 25 02:56:18.520: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 02:56:18.524: INFO: Starting informer...
STEP: Starting pods...
Sep 25 02:56:18.745: INFO: Pod1 is running on biz-k8s-node-3. Tainting Node
Sep 25 02:56:22.969: INFO: Pod2 is running on biz-k8s-node-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep 25 02:56:32.957: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 25 02:56:52.956: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:56:52.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5037" for this suite.

â€¢ [SLOW TEST:94.650 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":76,"skipped":1005,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:56:52.976: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1412
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1412
STEP: creating replication controller externalsvc in namespace services-1412
I0925 02:56:53.136089      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-1412, replica count: 2
I0925 02:56:56.187070      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:56:59.187383      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:02.187691      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:05.187987      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:08.188193      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:11.188431      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:14.188755      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:17.189013      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:20.189362      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:23.189800      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:26.190086      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:29.190467      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:32.190660      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 02:57:35.190908      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 25 02:57:35.224: INFO: Creating new exec pod
Sep 25 02:57:39.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=services-1412 execpodc7c8p -- /bin/sh -x -c nslookup nodeport-service'
Sep 25 02:57:39.472: INFO: stderr: "+ nslookup nodeport-service\n"
Sep 25 02:57:39.472: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-1412.svc.cluster.local\tcanonical name = externalsvc.services-1412.svc.cluster.local.\nName:\texternalsvc.services-1412.svc.cluster.local\nAddress: 10.0.38.61\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1412, will wait for the garbage collector to delete the pods
Sep 25 02:57:39.533: INFO: Deleting ReplicationController externalsvc took: 6.988133ms
Sep 25 02:57:40.434: INFO: Terminating ReplicationController externalsvc pods took: 901.033396ms
Sep 25 02:57:53.052: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:57:53.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1412" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:60.089 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":77,"skipped":1010,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:57:53.066: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:58:09.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8345" for this suite.

â€¢ [SLOW TEST:16.241 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":78,"skipped":1020,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:58:09.308: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 25 02:58:09.457: INFO: Waiting up to 5m0s for pod "pod-7e8877b8-b0ea-4f89-a9bf-e418102cec8b" in namespace "emptydir-7188" to be "success or failure"
Sep 25 02:58:09.463: INFO: Pod "pod-7e8877b8-b0ea-4f89-a9bf-e418102cec8b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.393194ms
Sep 25 02:58:11.468: INFO: Pod "pod-7e8877b8-b0ea-4f89-a9bf-e418102cec8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011236634s
Sep 25 02:58:13.472: INFO: Pod "pod-7e8877b8-b0ea-4f89-a9bf-e418102cec8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015334044s
STEP: Saw pod success
Sep 25 02:58:13.472: INFO: Pod "pod-7e8877b8-b0ea-4f89-a9bf-e418102cec8b" satisfied condition "success or failure"
Sep 25 02:58:13.476: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-7e8877b8-b0ea-4f89-a9bf-e418102cec8b container test-container: <nil>
STEP: delete the pod
Sep 25 02:58:13.505: INFO: Waiting for pod pod-7e8877b8-b0ea-4f89-a9bf-e418102cec8b to disappear
Sep 25 02:58:13.508: INFO: Pod pod-7e8877b8-b0ea-4f89-a9bf-e418102cec8b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:58:13.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7188" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":79,"skipped":1040,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:58:13.513: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-844
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Sep 25 02:58:13.650: INFO: Waiting up to 5m0s for pod "var-expansion-1db390b8-4f5a-4c91-9d8a-52e8d9be489f" in namespace "var-expansion-844" to be "success or failure"
Sep 25 02:58:13.653: INFO: Pod "var-expansion-1db390b8-4f5a-4c91-9d8a-52e8d9be489f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.688862ms
Sep 25 02:58:15.656: INFO: Pod "var-expansion-1db390b8-4f5a-4c91-9d8a-52e8d9be489f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006377807s
Sep 25 02:58:17.660: INFO: Pod "var-expansion-1db390b8-4f5a-4c91-9d8a-52e8d9be489f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01005995s
STEP: Saw pod success
Sep 25 02:58:17.660: INFO: Pod "var-expansion-1db390b8-4f5a-4c91-9d8a-52e8d9be489f" satisfied condition "success or failure"
Sep 25 02:58:17.663: INFO: Trying to get logs from node biz-k8s-node-3 pod var-expansion-1db390b8-4f5a-4c91-9d8a-52e8d9be489f container dapi-container: <nil>
STEP: delete the pod
Sep 25 02:58:17.681: INFO: Waiting for pod var-expansion-1db390b8-4f5a-4c91-9d8a-52e8d9be489f to disappear
Sep 25 02:58:17.683: INFO: Pod var-expansion-1db390b8-4f5a-4c91-9d8a-52e8d9be489f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:58:17.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-844" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":80,"skipped":1041,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:58:17.689: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8270
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Sep 25 02:58:17.822: INFO: Waiting up to 5m0s for pod "client-containers-76a44e1c-6824-4d3c-9c80-b82828ced47c" in namespace "containers-8270" to be "success or failure"
Sep 25 02:58:17.825: INFO: Pod "client-containers-76a44e1c-6824-4d3c-9c80-b82828ced47c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.420794ms
Sep 25 02:58:19.829: INFO: Pod "client-containers-76a44e1c-6824-4d3c-9c80-b82828ced47c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006576635s
STEP: Saw pod success
Sep 25 02:58:19.829: INFO: Pod "client-containers-76a44e1c-6824-4d3c-9c80-b82828ced47c" satisfied condition "success or failure"
Sep 25 02:58:19.833: INFO: Trying to get logs from node biz-k8s-node-3 pod client-containers-76a44e1c-6824-4d3c-9c80-b82828ced47c container test-container: <nil>
STEP: delete the pod
Sep 25 02:58:19.847: INFO: Waiting for pod client-containers-76a44e1c-6824-4d3c-9c80-b82828ced47c to disappear
Sep 25 02:58:19.850: INFO: Pod client-containers-76a44e1c-6824-4d3c-9c80-b82828ced47c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:58:19.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8270" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":81,"skipped":1045,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:58:19.859: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9064
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 25 02:58:19.996: INFO: Waiting up to 5m0s for pod "pod-5aa3fcc3-050b-40e8-8441-b38e4d497709" in namespace "emptydir-9064" to be "success or failure"
Sep 25 02:58:20.004: INFO: Pod "pod-5aa3fcc3-050b-40e8-8441-b38e4d497709": Phase="Pending", Reason="", readiness=false. Elapsed: 7.93632ms
Sep 25 02:58:22.008: INFO: Pod "pod-5aa3fcc3-050b-40e8-8441-b38e4d497709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012057899s
STEP: Saw pod success
Sep 25 02:58:22.008: INFO: Pod "pod-5aa3fcc3-050b-40e8-8441-b38e4d497709" satisfied condition "success or failure"
Sep 25 02:58:22.012: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-5aa3fcc3-050b-40e8-8441-b38e4d497709 container test-container: <nil>
STEP: delete the pod
Sep 25 02:58:22.026: INFO: Waiting for pod pod-5aa3fcc3-050b-40e8-8441-b38e4d497709 to disappear
Sep 25 02:58:22.028: INFO: Pod pod-5aa3fcc3-050b-40e8-8441-b38e4d497709 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:58:22.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9064" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":82,"skipped":1062,"failed":0}
SSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:58:22.037: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1638
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:58:22.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1638" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":83,"skipped":1068,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:58:22.202: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9860
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 25 02:58:26.384: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 25 02:58:26.386: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 25 02:58:28.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 25 02:58:28.391: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 25 02:58:30.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 25 02:58:30.390: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 25 02:58:32.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 25 02:58:32.391: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 25 02:58:34.387: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 25 02:58:34.391: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:58:34.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9860" for this suite.

â€¢ [SLOW TEST:12.204 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":84,"skipped":1138,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:58:34.407: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4745
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 25 02:58:34.856: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 25 02:58:36.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599514, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599514, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599514, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599514, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 02:58:39.883: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 02:58:39.887: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:58:41.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4745" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

â€¢ [SLOW TEST:6.776 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":85,"skipped":1142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:58:41.183: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3852
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3852 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3852;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3852 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3852;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3852.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3852.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3852.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3852.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3852.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3852.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3852.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3852.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3852.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3852.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3852.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3852.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3852.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 27.49.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.49.27_udp@PTR;check="$$(dig +tcp +noall +answer +search 27.49.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.49.27_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3852 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3852;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3852 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3852;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3852.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3852.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3852.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3852.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3852.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3852.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3852.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3852.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3852.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3852.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3852.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3852.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3852.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 27.49.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.49.27_udp@PTR;check="$$(dig +tcp +noall +answer +search 27.49.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.49.27_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 25 02:58:45.406: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3852/dns-test-04365057-f265-496f-9f21-522a5c3736df: the server could not find the requested resource (get pods dns-test-04365057-f265-496f-9f21-522a5c3736df)
Sep 25 02:58:45.409: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3852/dns-test-04365057-f265-496f-9f21-522a5c3736df: the server could not find the requested resource (get pods dns-test-04365057-f265-496f-9f21-522a5c3736df)
Sep 25 02:58:45.452: INFO: Lookups using dns-3852/dns-test-04365057-f265-496f-9f21-522a5c3736df failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service]

Sep 25 02:58:50.534: INFO: DNS probes using dns-3852/dns-test-04365057-f265-496f-9f21-522a5c3736df succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:58:50.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3852" for this suite.

â€¢ [SLOW TEST:9.395 seconds]
[sig-network] DNS
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":86,"skipped":1172,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:58:50.578: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 02:58:50.718: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99ee7022-aec1-41b2-b496-e633bc2f5577" in namespace "downward-api-5604" to be "success or failure"
Sep 25 02:58:50.720: INFO: Pod "downwardapi-volume-99ee7022-aec1-41b2-b496-e633bc2f5577": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395821ms
Sep 25 02:58:52.724: INFO: Pod "downwardapi-volume-99ee7022-aec1-41b2-b496-e633bc2f5577": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006878531s
Sep 25 02:58:54.729: INFO: Pod "downwardapi-volume-99ee7022-aec1-41b2-b496-e633bc2f5577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011418791s
STEP: Saw pod success
Sep 25 02:58:54.729: INFO: Pod "downwardapi-volume-99ee7022-aec1-41b2-b496-e633bc2f5577" satisfied condition "success or failure"
Sep 25 02:58:54.732: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-99ee7022-aec1-41b2-b496-e633bc2f5577 container client-container: <nil>
STEP: delete the pod
Sep 25 02:58:54.749: INFO: Waiting for pod downwardapi-volume-99ee7022-aec1-41b2-b496-e633bc2f5577 to disappear
Sep 25 02:58:54.752: INFO: Pod downwardapi-volume-99ee7022-aec1-41b2-b496-e633bc2f5577 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:58:54.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5604" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":87,"skipped":1188,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:58:54.760: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-8234
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 25 02:58:54.885: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 25 02:59:16.984: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.100.180:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8234 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:59:16.984: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:59:17.088: INFO: Found all expected endpoints: [netserver-0]
Sep 25 02:59:17.091: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.100.16:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8234 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:59:17.091: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:59:17.186: INFO: Found all expected endpoints: [netserver-1]
Sep 25 02:59:17.189: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.100.86:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8234 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 02:59:17.189: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 02:59:17.295: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 02:59:17.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8234" for this suite.

â€¢ [SLOW TEST:22.543 seconds]
[sig-network] Networking
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1188,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 02:59:17.303: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 25 02:59:17.439: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-a 705e7806-4595-4baa-9b68-4d40bcc279e7 415364 0 2020-09-25 02:59:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 25 02:59:17.439: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-a 705e7806-4595-4baa-9b68-4d40bcc279e7 415364 0 2020-09-25 02:59:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 25 02:59:27.447: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-a 705e7806-4595-4baa-9b68-4d40bcc279e7 415481 0 2020-09-25 02:59:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 25 02:59:27.448: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-a 705e7806-4595-4baa-9b68-4d40bcc279e7 415481 0 2020-09-25 02:59:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 25 02:59:37.455: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-a 705e7806-4595-4baa-9b68-4d40bcc279e7 415546 0 2020-09-25 02:59:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 25 02:59:37.455: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-a 705e7806-4595-4baa-9b68-4d40bcc279e7 415546 0 2020-09-25 02:59:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 25 02:59:47.463: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-a 705e7806-4595-4baa-9b68-4d40bcc279e7 415609 0 2020-09-25 02:59:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 25 02:59:47.463: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-a 705e7806-4595-4baa-9b68-4d40bcc279e7 415609 0 2020-09-25 02:59:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 25 02:59:57.471: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-b b9265e14-0dd7-46d3-8db3-407efc2637b0 415672 0 2020-09-25 02:59:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 25 02:59:57.471: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-b b9265e14-0dd7-46d3-8db3-407efc2637b0 415672 0 2020-09-25 02:59:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 25 03:00:07.478: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-b b9265e14-0dd7-46d3-8db3-407efc2637b0 415733 0 2020-09-25 02:59:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 25 03:00:07.478: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-626 /api/v1/namespaces/watch-626/configmaps/e2e-watch-test-configmap-b b9265e14-0dd7-46d3-8db3-407efc2637b0 415733 0 2020-09-25 02:59:57 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:00:17.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-626" for this suite.

â€¢ [SLOW TEST:60.186 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":89,"skipped":1220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:00:17.491: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:00:17.970: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:00:19.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599617, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599617, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599617, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599617, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:00:22.993: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:00:35.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5656" for this suite.
STEP: Destroying namespace "webhook-5656-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:17.662 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":90,"skipped":1259,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:00:35.153: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:325
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Sep 25 03:00:35.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-8340'
Sep 25 03:00:35.556: INFO: stderr: ""
Sep 25 03:00:35.556: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 25 03:00:35.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8340'
Sep 25 03:00:35.665: INFO: stderr: ""
Sep 25 03:00:35.666: INFO: stdout: "update-demo-nautilus-d58x4 update-demo-nautilus-jh9tw "
Sep 25 03:00:35.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-d58x4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8340'
Sep 25 03:00:35.761: INFO: stderr: ""
Sep 25 03:00:35.761: INFO: stdout: ""
Sep 25 03:00:35.761: INFO: update-demo-nautilus-d58x4 is created but not running
Sep 25 03:00:40.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8340'
Sep 25 03:00:40.923: INFO: stderr: ""
Sep 25 03:00:40.923: INFO: stdout: "update-demo-nautilus-d58x4 update-demo-nautilus-jh9tw "
Sep 25 03:00:40.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-d58x4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8340'
Sep 25 03:00:41.013: INFO: stderr: ""
Sep 25 03:00:41.013: INFO: stdout: "true"
Sep 25 03:00:41.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-d58x4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8340'
Sep 25 03:00:41.125: INFO: stderr: ""
Sep 25 03:00:41.125: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 25 03:00:41.125: INFO: validating pod update-demo-nautilus-d58x4
Sep 25 03:00:41.130: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 25 03:00:41.130: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 25 03:00:41.130: INFO: update-demo-nautilus-d58x4 is verified up and running
Sep 25 03:00:41.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-jh9tw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8340'
Sep 25 03:00:41.247: INFO: stderr: ""
Sep 25 03:00:41.247: INFO: stdout: "true"
Sep 25 03:00:41.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-jh9tw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8340'
Sep 25 03:00:41.362: INFO: stderr: ""
Sep 25 03:00:41.362: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 25 03:00:41.362: INFO: validating pod update-demo-nautilus-jh9tw
Sep 25 03:00:41.374: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 25 03:00:41.374: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 25 03:00:41.374: INFO: update-demo-nautilus-jh9tw is verified up and running
STEP: using delete to clean up resources
Sep 25 03:00:41.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete --grace-period=0 --force -f - --namespace=kubectl-8340'
Sep 25 03:00:41.476: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 25 03:00:41.476: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 25 03:00:41.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8340'
Sep 25 03:00:41.581: INFO: stderr: "No resources found in kubectl-8340 namespace.\n"
Sep 25 03:00:41.581: INFO: stdout: ""
Sep 25 03:00:41.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -l name=update-demo --namespace=kubectl-8340 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 25 03:00:41.673: INFO: stderr: ""
Sep 25 03:00:41.673: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:00:41.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8340" for this suite.

â€¢ [SLOW TEST:6.530 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:323
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":91,"skipped":1268,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:00:41.683: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 25 03:00:46.358: INFO: Successfully updated pod "pod-update-62915dc6-5cb0-4f10-9f97-8a0edc42aae7"
STEP: verifying the updated pod is in kubernetes
Sep 25 03:00:46.365: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:00:46.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2383" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":92,"skipped":1271,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:00:46.374: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Sep 25 03:00:46.517: INFO: Waiting up to 5m0s for pod "downward-api-83078864-98a0-494f-b7af-0c8561c5ade5" in namespace "downward-api-758" to be "success or failure"
Sep 25 03:00:46.520: INFO: Pod "downward-api-83078864-98a0-494f-b7af-0c8561c5ade5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.203396ms
Sep 25 03:00:48.524: INFO: Pod "downward-api-83078864-98a0-494f-b7af-0c8561c5ade5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007489691s
Sep 25 03:00:50.528: INFO: Pod "downward-api-83078864-98a0-494f-b7af-0c8561c5ade5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011522222s
STEP: Saw pod success
Sep 25 03:00:50.528: INFO: Pod "downward-api-83078864-98a0-494f-b7af-0c8561c5ade5" satisfied condition "success or failure"
Sep 25 03:00:50.531: INFO: Trying to get logs from node biz-k8s-node-3 pod downward-api-83078864-98a0-494f-b7af-0c8561c5ade5 container dapi-container: <nil>
STEP: delete the pod
Sep 25 03:00:50.564: INFO: Waiting for pod downward-api-83078864-98a0-494f-b7af-0c8561c5ade5 to disappear
Sep 25 03:00:50.566: INFO: Pod downward-api-83078864-98a0-494f-b7af-0c8561c5ade5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:00:50.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-758" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":93,"skipped":1277,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:00:50.573: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-204
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:00:50.987: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 25 03:00:52.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599650, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599650, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599650, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599650, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:00:56.011: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:00:56.014: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:00:57.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-204" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

â€¢ [SLOW TEST:6.636 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":94,"skipped":1287,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:00:57.210: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3551
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-a591ba0d-123b-4e72-aa37-dca2500b6505
STEP: Creating configMap with name cm-test-opt-upd-7972b6be-cb7b-4cbd-a27b-4ddde905558b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a591ba0d-123b-4e72-aa37-dca2500b6505
STEP: Updating configmap cm-test-opt-upd-7972b6be-cb7b-4cbd-a27b-4ddde905558b
STEP: Creating configMap with name cm-test-opt-create-03634b0e-2c41-4d6c-a76f-bf9a9a89cb7e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:02:25.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3551" for this suite.

â€¢ [SLOW TEST:88.595 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":95,"skipped":1313,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:02:25.806: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4007
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 25 03:02:26.006: INFO: Waiting up to 5m0s for pod "pod-5260e638-8cca-4a17-a5ea-9250781e6404" in namespace "emptydir-4007" to be "success or failure"
Sep 25 03:02:26.008: INFO: Pod "pod-5260e638-8cca-4a17-a5ea-9250781e6404": Phase="Pending", Reason="", readiness=false. Elapsed: 1.776354ms
Sep 25 03:02:28.022: INFO: Pod "pod-5260e638-8cca-4a17-a5ea-9250781e6404": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016131803s
Sep 25 03:02:30.026: INFO: Pod "pod-5260e638-8cca-4a17-a5ea-9250781e6404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020203058s
STEP: Saw pod success
Sep 25 03:02:30.026: INFO: Pod "pod-5260e638-8cca-4a17-a5ea-9250781e6404" satisfied condition "success or failure"
Sep 25 03:02:30.030: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-5260e638-8cca-4a17-a5ea-9250781e6404 container test-container: <nil>
STEP: delete the pod
Sep 25 03:02:30.043: INFO: Waiting for pod pod-5260e638-8cca-4a17-a5ea-9250781e6404 to disappear
Sep 25 03:02:30.045: INFO: Pod pod-5260e638-8cca-4a17-a5ea-9250781e6404 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:02:30.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4007" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":96,"skipped":1345,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:02:30.052: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Sep 25 03:02:30.188: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:02:37.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4502" for this suite.

â€¢ [SLOW TEST:7.741 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":97,"skipped":1351,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:02:37.794: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-1a2bfd43-4132-485c-a817-93258f3c0e68
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:02:37.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-878" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":98,"skipped":1359,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:02:37.932: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1979
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:02:42.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1979" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":99,"skipped":1370,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:02:42.101: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2854
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:02:42.231: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 25 03:02:45.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-2854 create -f -'
Sep 25 03:02:45.572: INFO: stderr: ""
Sep 25 03:02:45.572: INFO: stdout: "e2e-test-crd-publish-openapi-952-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 25 03:02:45.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-2854 delete e2e-test-crd-publish-openapi-952-crds test-cr'
Sep 25 03:02:45.681: INFO: stderr: ""
Sep 25 03:02:45.681: INFO: stdout: "e2e-test-crd-publish-openapi-952-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 25 03:02:45.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-2854 apply -f -'
Sep 25 03:02:45.877: INFO: stderr: ""
Sep 25 03:02:45.877: INFO: stdout: "e2e-test-crd-publish-openapi-952-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 25 03:02:45.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-2854 delete e2e-test-crd-publish-openapi-952-crds test-cr'
Sep 25 03:02:45.991: INFO: stderr: ""
Sep 25 03:02:45.991: INFO: stdout: "e2e-test-crd-publish-openapi-952-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 25 03:02:45.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 explain e2e-test-crd-publish-openapi-952-crds'
Sep 25 03:02:46.194: INFO: stderr: ""
Sep 25 03:02:46.194: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-952-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:02:49.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2854" for this suite.

â€¢ [SLOW TEST:7.009 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":100,"skipped":1373,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:02:49.112: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:02:49.512: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:02:51.531: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599769, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599769, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599769, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599769, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:02:54.544: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:02:54.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2055" for this suite.
STEP: Destroying namespace "webhook-2055-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.507 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":101,"skipped":1373,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:02:54.619: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:02:55.262: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:02:57.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599775, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599775, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736599775, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:03:00.283: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 25 03:03:00.309: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:03:00.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7526" for this suite.
STEP: Destroying namespace "webhook-7526-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.746 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":102,"skipped":1380,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:03:00.365: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Sep 25 03:03:00.493: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 25 03:03:00.504: INFO: Waiting for terminating namespaces to be deleted...
Sep 25 03:03:00.507: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-1 before test
Sep 25 03:03:00.531: INFO: minio-0 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:03:00.531: INFO: daemon-notify-74b7d7c4b5-zklwv from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container daemon-notify ready: true, restart count 0
Sep 25 03:03:00.531: INFO: store-api-mypage-5d6d59c499-dvczz from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container store-api-mypage ready: true, restart count 0
Sep 25 03:03:00.531: INFO: dex-k8s-authenticator-kubeaddons-74966666f5-gvf92 from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container dex-k8s-authenticator ready: true, restart count 1
Sep 25 03:03:00.531: INFO: coredns-849d6f84b4-cc8tj from kube-system started at 2020-09-24 03:18:43 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container coredns ready: true, restart count 0
Sep 25 03:03:00.531: INFO: metallb-kubeaddons-speaker-cf5wv from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:03:00.531: INFO: builder-api-main-54dfc4474c-gnvlr from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container builder-api-main ready: true, restart count 0
Sep 25 03:03:00.531: INFO: mdcs-api-common-84c884c5c7-r4kt9 from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container mdcs-api-common ready: true, restart count 0
Sep 25 03:03:00.531: INFO: sonobuoy-e2e-job-a16116856b774ea6 from sonobuoy started at 2020-09-25 02:35:55 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container e2e ready: true, restart count 0
Sep 25 03:03:00.531: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:03:00.531: INFO: admin-api-common-596bf5fdb7-6pw8d from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container admin-api-common ready: true, restart count 0
Sep 25 03:03:00.531: INFO: portal-api-dock-5c5497b47f-8qjn6 from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container portal-api-dock ready: true, restart count 0
Sep 25 03:03:00.531: INFO: common-api-auth-c49dcbcc9-4xb9s from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container common-api-auth ready: true, restart count 0
Sep 25 03:03:00.531: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-pqlj2 from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:03:00.531: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:03:00.531: INFO: velero-kubeaddons-5d85fcdcb9-6lt97 from velero started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container velero ready: true, restart count 0
Sep 25 03:03:00.531: INFO: traefik-kubeaddons-68c579bbbd-2qmn4 from kubeaddons started at 2020-09-24 03:26:59 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Sep 25 03:03:00.531: INFO: store-api-appmanage-6b8b464c4c-m528t from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container store-api-appmanage ready: true, restart count 0
Sep 25 03:03:00.531: INFO: common-api-notify-5595dbd9b5-rb8vd from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container common-api-notify ready: true, restart count 0
Sep 25 03:03:00.531: INFO: kube-proxy-ff4h2 from kube-system started at 2020-09-24 03:18:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:03:00.531: INFO: dex-kubeaddons-58884f456-xtvhz from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container main ready: true, restart count 0
Sep 25 03:03:00.531: INFO: calico-node-k6ztw from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:03:00.531: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:03:00.531: INFO: calico-kube-controllers-77c79f7594-5qtpj from kube-system started at 2020-09-24 03:18:35 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 25 03:03:00.531: INFO: kubernetes-dashboard-95959d56b-wmxhc from kubeaddons started at 2020-09-24 03:25:22 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Sep 25 03:03:00.531: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 25 03:03:00.531: INFO: minio-3 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:03:00.531: INFO: local-volume-provisioner-rmgc9 from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:03:00.531: INFO: builder-daemon-main-fcb6b989c-hfwnc from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container builder-daemon-main ready: true, restart count 0
Sep 25 03:03:00.531: INFO: portal-cdn-download-5cd45dfd87-fqvrl from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container portal-cdn-download ready: false, restart count 14
Sep 25 03:03:00.531: INFO: sonobuoy from sonobuoy started at 2020-09-25 02:35:47 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 25 03:03:00.531: INFO: traefik-forward-auth-kubeaddons-69bbb98fb6-fdn8n from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container traefik-forward-auth ready: true, restart count 1
Sep 25 03:03:00.531: INFO: tiller-deploy-6cdf7f9d6f-5mjft from kube-system started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container tiller ready: true, restart count 0
Sep 25 03:03:00.531: INFO: common-api-account-6b7c79d795-d846m from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container common-api-account ready: true, restart count 0
Sep 25 03:03:00.531: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-q9x6d from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
Sep 25 03:03:00.531: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-rw4pq from cert-manager started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.531: INFO: 	Container cert-manager ready: true, restart count 0
Sep 25 03:03:00.531: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-2 before test
Sep 25 03:03:00.561: INFO: traefik-kubeaddons-68c579bbbd-zwq9n from kubeaddons started at 2020-09-24 03:26:37 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Sep 25 03:03:00.562: INFO: store-api-common-bfd5d66c5-2wkcr from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container store-api-common ready: true, restart count 0
Sep 25 03:03:00.562: INFO: builder-api-custom-676877997f-5vft8 from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container builder-api-custom ready: true, restart count 0
Sep 25 03:03:00.562: INFO: kube-proxy-5jx4d from kube-system started at 2020-09-24 03:18:13 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:03:00.562: INFO: reloader-kubeaddons-reloader-7c97f877cf-hpgvn from kubeaddons started at 2020-09-24 03:25:24 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
Sep 25 03:03:00.562: INFO: daemon-task-6c8f8f5ffc-pbmqt from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container daemon-task ready: true, restart count 0
Sep 25 03:03:00.562: INFO: coredns-849d6f84b4-qdfhl from kube-system started at 2020-09-24 03:18:43 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container coredns ready: true, restart count 0
Sep 25 03:03:00.562: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-g6ss8 from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:03:00.562: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:03:00.562: INFO: cert-manager-kubeaddons-7d7f98fbc6-fxzfx from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container cert-manager ready: true, restart count 0
Sep 25 03:03:00.562: INFO: dstorageclass-controller-manager-5c966c767f-ngg5l from kubeaddons started at 2020-09-24 03:26:27 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:03:00.562: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:03:00.562: INFO: portal-cdn-upload-567d9969fc-9b6dv from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container portal-cdn-upload ready: false, restart count 5
Sep 25 03:03:00.562: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-qc42h from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:03:00.562: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:03:00.562: INFO: daemon-account-866d44497f-5q7dl from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container daemon-account ready: true, restart count 0
Sep 25 03:03:00.562: INFO: dev-biz-portal-front-687d79db75-q4rwx from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container dev-biz-portal-front ready: false, restart count 14
Sep 25 03:03:00.562: INFO: minio-2 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:03:00.562: INFO: core-sockjs-79b764df9f-24fs4 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container core-sockjs ready: true, restart count 0
Sep 25 03:03:00.562: INFO: workflow-api-draft-5899bb55c9-d4dhd from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container workflow-api-draft ready: true, restart count 0
Sep 25 03:03:00.562: INFO: calico-node-mj6vw from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:03:00.562: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:03:00.562: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-92k4x from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container cainjector ready: true, restart count 0
Sep 25 03:03:00.562: INFO: opsportal-kubeaddons-kommander-ui-6d64cc5d54-c8pct from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
Sep 25 03:03:00.562: INFO: kubeaddons-controller-manager-659bd9c576-jfg77 from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:03:00.562: INFO: local-volume-provisioner-n7q6g from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:03:00.562: INFO: common-api-oauth-6cd64bdc5-nkwk5 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container common-api-oauth ready: true, restart count 0
Sep 25 03:03:00.562: INFO: core-router-57d6544c9-j52x2 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container core-router ready: true, restart count 0
Sep 25 03:03:00.562: INFO: workflow-api-common-76dd6fb889-gbn6z from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container workflow-api-common ready: true, restart count 0
Sep 25 03:03:00.562: INFO: workflow-daemon-draft-5955b46c89-t7p8l from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container workflow-daemon-draft ready: true, restart count 0
Sep 25 03:03:00.562: INFO: metallb-kubeaddons-speaker-pvqct from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:03:00.562: INFO: metallb-kubeaddons-controller-f84b74d86-8hz2m from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container controller ready: true, restart count 0
Sep 25 03:03:00.562: INFO: store-api-bizgroup-695cc488b5-4pmrh from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container store-api-bizgroup ready: true, restart count 0
Sep 25 03:03:00.562: INFO: opsportal-landing-6f6865b688-v7dhz from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container opsportal-landing ready: true, restart count 0
Sep 25 03:03:00.562: INFO: gatekeeper-kubeaddons-fdc87db85-9q9kx from kubeaddons started at 2020-09-24 03:26:34 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:03:00.562: INFO: portal-api-page-766cc7bf79-vcwbs from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.562: INFO: 	Container portal-api-page ready: true, restart count 0
Sep 25 03:03:00.562: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-3 before test
Sep 25 03:03:00.573: INFO: opsportal-kubeaddons-kommander-ui-6d64cc5d54-gxbm8 from kubeaddons started at 2020-09-24 03:25:44 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: false, restart count 0
Sep 25 03:03:00.573: INFO: workflow-api-draft-5899bb55c9-j5dgh from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container workflow-api-draft ready: true, restart count 0
Sep 25 03:03:00.573: INFO: calico-node-bzvbt from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:03:00.573: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:03:00.573: INFO: kubeaddons-controller-manager-659bd9c576-g2wjv from kubeaddons started at 2020-09-24 03:25:02 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:03:00.573: INFO: portal-api-dock-5c5497b47f-6tctr from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container portal-api-dock ready: true, restart count 0
Sep 25 03:03:00.573: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-2cctd from kubeaddons started at 2020-09-24 03:27:26 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:03:00.573: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:03:00.573: INFO: minio-1 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container minio ready: false, restart count 0
Sep 25 03:03:00.573: INFO: common-api-notify-5595dbd9b5-l4l6r from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container common-api-notify ready: true, restart count 0
Sep 25 03:03:00.573: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-vbvdn from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:03:00.573: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:03:00.573: INFO: busybox-host-aliases2051fa63-5100-4864-950e-83ef0a834acd from kubelet-test-1979 started at 2020-09-25 03:02:38 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container busybox-host-aliases2051fa63-5100-4864-950e-83ef0a834acd ready: true, restart count 0
Sep 25 03:03:00.573: INFO: portal-cdn-upload-567d9969fc-d4sq2 from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container portal-cdn-upload ready: false, restart count 12
Sep 25 03:03:00.573: INFO: dex-kubeaddons-58884f456-gtkb7 from kubeaddons started at 2020-09-24 03:27:50 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container main ready: true, restart count 1
Sep 25 03:03:00.573: INFO: traefik-forward-auth-kubeaddons-69bbb98fb6-g4fpr from kubeaddons started at 2020-09-24 03:28:06 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container traefik-forward-auth ready: false, restart count 0
Sep 25 03:03:00.573: INFO: builder-api-custom-676877997f-5dj9d from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container builder-api-custom ready: true, restart count 0
Sep 25 03:03:00.573: INFO: velero-kubeaddons-5d85fcdcb9-gwwkq from velero started at 2020-09-24 03:28:07 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container velero ready: true, restart count 3
Sep 25 03:03:00.573: INFO: dex-k8s-authenticator-kubeaddons-74966666f5-9qtxz from kubeaddons started at 2020-09-24 03:28:01 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container dex-k8s-authenticator ready: false, restart count 0
Sep 25 03:03:00.573: INFO: tiller-deploy-6cdf7f9d6f-qqfpj from kube-system started at 2020-09-24 03:25:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container tiller ready: false, restart count 0
Sep 25 03:03:00.573: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-pzrmp from kubeaddons started at 2020-09-24 03:27:52 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container kube-oidc-proxy ready: false, restart count 0
Sep 25 03:03:00.573: INFO: metallb-kubeaddons-speaker-4cn7m from kubeaddons started at 2020-09-25 02:56:52 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:03:00.573: INFO: local-volume-provisioner-972q8 from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:03:00.573: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-jvmxg from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container cert-manager ready: true, restart count 1
Sep 25 03:03:00.573: INFO: opsportal-landing-6f6865b688-8sfw4 from kubeaddons started at 2020-09-24 03:25:44 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container opsportal-landing ready: true, restart count 0
Sep 25 03:03:00.573: INFO: admin-api-common-596bf5fdb7-dkf5t from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container admin-api-common ready: true, restart count 0
Sep 25 03:03:00.573: INFO: daemon-task-6c8f8f5ffc-wfsxt from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container daemon-task ready: true, restart count 0
Sep 25 03:03:00.573: INFO: store-api-common-bfd5d66c5-wcnh9 from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container store-api-common ready: true, restart count 0
Sep 25 03:03:00.573: INFO: sample-webhook-deployment-5f65f8c764-r95rp from webhook-7526 started at 2020-09-25 03:02:55 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container sample-webhook ready: true, restart count 0
Sep 25 03:03:00.573: INFO: kube-proxy-lpjq2 from kube-system started at 2020-09-24 03:18:13 +0000 UTC (1 container statuses recorded)
Sep 25 03:03:00.573: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-20f94952-3771-4041-8166-637229918b0f 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-20f94952-3771-4041-8166-637229918b0f off the node biz-k8s-node-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-20f94952-3771-4041-8166-637229918b0f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:08:08.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1912" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:308.308 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":103,"skipped":1400,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:08:08.673: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8783
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:08:08.814: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e51c3eb9-37e3-4ba0-adec-e9586a9fae4f" in namespace "downward-api-8783" to be "success or failure"
Sep 25 03:08:08.821: INFO: Pod "downwardapi-volume-e51c3eb9-37e3-4ba0-adec-e9586a9fae4f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.705096ms
Sep 25 03:08:10.825: INFO: Pod "downwardapi-volume-e51c3eb9-37e3-4ba0-adec-e9586a9fae4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010984061s
Sep 25 03:08:12.829: INFO: Pod "downwardapi-volume-e51c3eb9-37e3-4ba0-adec-e9586a9fae4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015630754s
STEP: Saw pod success
Sep 25 03:08:12.829: INFO: Pod "downwardapi-volume-e51c3eb9-37e3-4ba0-adec-e9586a9fae4f" satisfied condition "success or failure"
Sep 25 03:08:12.833: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-e51c3eb9-37e3-4ba0-adec-e9586a9fae4f container client-container: <nil>
STEP: delete the pod
Sep 25 03:08:12.868: INFO: Waiting for pod downwardapi-volume-e51c3eb9-37e3-4ba0-adec-e9586a9fae4f to disappear
Sep 25 03:08:12.870: INFO: Pod downwardapi-volume-e51c3eb9-37e3-4ba0-adec-e9586a9fae4f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:08:12.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8783" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":104,"skipped":1414,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:08:12.877: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:08:13.371: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:08:15.381: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600093, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600093, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600093, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600093, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:08:18.397: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:08:18.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2062" for this suite.
STEP: Destroying namespace "webhook-2062-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.567 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":105,"skipped":1415,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:08:18.444: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8703
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-k8vq
STEP: Creating a pod to test atomic-volume-subpath
Sep 25 03:08:18.602: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-k8vq" in namespace "subpath-8703" to be "success or failure"
Sep 25 03:08:18.608: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Pending", Reason="", readiness=false. Elapsed: 5.14542ms
Sep 25 03:08:20.612: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 2.009238386s
Sep 25 03:08:22.616: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 4.013451229s
Sep 25 03:08:24.620: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 6.017640363s
Sep 25 03:08:26.626: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 8.023285598s
Sep 25 03:08:28.630: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 10.027801522s
Sep 25 03:08:30.635: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 12.032292009s
Sep 25 03:08:32.639: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 14.036333265s
Sep 25 03:08:34.643: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 16.040354002s
Sep 25 03:08:36.647: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 18.044070139s
Sep 25 03:08:38.651: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 20.048430245s
Sep 25 03:08:40.655: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Running", Reason="", readiness=true. Elapsed: 22.052913571s
Sep 25 03:08:42.660: INFO: Pod "pod-subpath-test-downwardapi-k8vq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.057180848s
STEP: Saw pod success
Sep 25 03:08:42.660: INFO: Pod "pod-subpath-test-downwardapi-k8vq" satisfied condition "success or failure"
Sep 25 03:08:42.662: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-subpath-test-downwardapi-k8vq container test-container-subpath-downwardapi-k8vq: <nil>
STEP: delete the pod
Sep 25 03:08:42.678: INFO: Waiting for pod pod-subpath-test-downwardapi-k8vq to disappear
Sep 25 03:08:42.680: INFO: Pod pod-subpath-test-downwardapi-k8vq no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-k8vq
Sep 25 03:08:42.680: INFO: Deleting pod "pod-subpath-test-downwardapi-k8vq" in namespace "subpath-8703"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:08:42.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8703" for this suite.

â€¢ [SLOW TEST:24.243 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":106,"skipped":1415,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:08:42.689: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-ef2e17a5-fa30-42c9-817c-d6f17d7d16b9 in namespace container-probe-7366
Sep 25 03:08:46.834: INFO: Started pod busybox-ef2e17a5-fa30-42c9-817c-d6f17d7d16b9 in namespace container-probe-7366
STEP: checking the pod's current state and verifying that restartCount is present
Sep 25 03:08:46.837: INFO: Initial restart count of pod busybox-ef2e17a5-fa30-42c9-817c-d6f17d7d16b9 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:12:47.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7366" for this suite.

â€¢ [SLOW TEST:244.683 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":107,"skipped":1432,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:12:47.372: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-fd7cdf81-7aca-4c6d-87cb-6d2dfd1cbc58
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:12:47.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5959" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":108,"skipped":1456,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:12:47.512: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:12:47.662: INFO: Create a RollingUpdate DaemonSet
Sep 25 03:12:47.665: INFO: Check that daemon pods launch on every node of the cluster
Sep 25 03:12:47.668: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:12:47.670: INFO: Number of nodes with available pods: 0
Sep 25 03:12:47.670: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:12:48.677: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:12:48.681: INFO: Number of nodes with available pods: 0
Sep 25 03:12:48.681: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:12:49.676: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:12:49.679: INFO: Number of nodes with available pods: 0
Sep 25 03:12:49.679: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:12:50.676: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:12:50.680: INFO: Number of nodes with available pods: 0
Sep 25 03:12:50.680: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:12:51.676: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:12:51.680: INFO: Number of nodes with available pods: 3
Sep 25 03:12:51.680: INFO: Number of running nodes: 3, number of available pods: 3
Sep 25 03:12:51.680: INFO: Update the DaemonSet to trigger a rollout
Sep 25 03:12:51.687: INFO: Updating DaemonSet daemon-set
Sep 25 03:13:03.705: INFO: Roll back the DaemonSet before rollout is complete
Sep 25 03:13:03.712: INFO: Updating DaemonSet daemon-set
Sep 25 03:13:03.712: INFO: Make sure DaemonSet rollback is complete
Sep 25 03:13:03.714: INFO: Wrong image for pod: daemon-set-9s52b. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 25 03:13:03.714: INFO: Pod daemon-set-9s52b is not available
Sep 25 03:13:03.718: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:13:04.723: INFO: Wrong image for pod: daemon-set-9s52b. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 25 03:13:04.723: INFO: Pod daemon-set-9s52b is not available
Sep 25 03:13:04.727: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:13:05.723: INFO: Wrong image for pod: daemon-set-9s52b. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 25 03:13:05.723: INFO: Pod daemon-set-9s52b is not available
Sep 25 03:13:05.728: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:13:06.723: INFO: Pod daemon-set-sxg6x is not available
Sep 25 03:13:06.727: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6140, will wait for the garbage collector to delete the pods
Sep 25 03:13:06.793: INFO: Deleting DaemonSet.extensions daemon-set took: 6.392032ms
Sep 25 03:13:07.693: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.269007ms
Sep 25 03:13:09.896: INFO: Number of nodes with available pods: 0
Sep 25 03:13:09.896: INFO: Number of running nodes: 0, number of available pods: 0
Sep 25 03:13:09.899: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6140/daemonsets","resourceVersion":"420712"},"items":null}

Sep 25 03:13:09.903: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6140/pods","resourceVersion":"420712"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:13:09.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6140" for this suite.

â€¢ [SLOW TEST:22.411 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":109,"skipped":1477,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:13:09.924: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-6204
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:13:10.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6204" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":110,"skipped":1486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:13:10.062: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6916
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 25 03:13:14.230: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:13:14.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6916" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":111,"skipped":1531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:13:14.253: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Sep 25 03:13:15.423: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:13:15.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5145" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":112,"skipped":1580,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:13:15.435: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2053
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9979
STEP: Creating secret with name secret-test-527a1056-e0d7-4ebf-b21e-cc5609ac3c1e
STEP: Creating a pod to test consume secrets
Sep 25 03:13:15.717: INFO: Waiting up to 5m0s for pod "pod-secrets-eb5ad9ab-0cb6-4368-a3d3-4859a0e3f2d9" in namespace "secrets-2053" to be "success or failure"
Sep 25 03:13:15.722: INFO: Pod "pod-secrets-eb5ad9ab-0cb6-4368-a3d3-4859a0e3f2d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.411644ms
Sep 25 03:13:17.726: INFO: Pod "pod-secrets-eb5ad9ab-0cb6-4368-a3d3-4859a0e3f2d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008526396s
Sep 25 03:13:19.730: INFO: Pod "pod-secrets-eb5ad9ab-0cb6-4368-a3d3-4859a0e3f2d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012656242s
STEP: Saw pod success
Sep 25 03:13:19.730: INFO: Pod "pod-secrets-eb5ad9ab-0cb6-4368-a3d3-4859a0e3f2d9" satisfied condition "success or failure"
Sep 25 03:13:19.733: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-secrets-eb5ad9ab-0cb6-4368-a3d3-4859a0e3f2d9 container secret-volume-test: <nil>
STEP: delete the pod
Sep 25 03:13:19.761: INFO: Waiting for pod pod-secrets-eb5ad9ab-0cb6-4368-a3d3-4859a0e3f2d9 to disappear
Sep 25 03:13:19.763: INFO: Pod pod-secrets-eb5ad9ab-0cb6-4368-a3d3-4859a0e3f2d9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:13:19.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2053" for this suite.
STEP: Destroying namespace "secret-namespace-9979" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":113,"skipped":1650,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:13:19.773: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Sep 25 03:13:19.912: INFO: Waiting up to 5m0s for pod "downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722" in namespace "downward-api-6238" to be "success or failure"
Sep 25 03:13:19.915: INFO: Pod "downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722": Phase="Pending", Reason="", readiness=false. Elapsed: 3.619299ms
Sep 25 03:13:21.919: INFO: Pod "downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007585697s
Sep 25 03:13:23.924: INFO: Pod "downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011774297s
Sep 25 03:13:25.928: INFO: Pod "downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015795711s
Sep 25 03:13:27.932: INFO: Pod "downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020025293s
Sep 25 03:13:29.936: INFO: Pod "downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.024018142s
STEP: Saw pod success
Sep 25 03:13:29.936: INFO: Pod "downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722" satisfied condition "success or failure"
Sep 25 03:13:29.939: INFO: Trying to get logs from node biz-k8s-node-3 pod downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722 container dapi-container: <nil>
STEP: delete the pod
Sep 25 03:13:29.958: INFO: Waiting for pod downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722 to disappear
Sep 25 03:13:29.960: INFO: Pod downward-api-0409d4ec-80a4-48e2-8ef0-593d54f07722 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:13:29.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6238" for this suite.

â€¢ [SLOW TEST:10.196 seconds]
[sig-node] Downward API
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:33
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":114,"skipped":1666,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:13:29.969: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6400
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:13:30.098: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 25 03:13:33.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-6400 create -f -'
Sep 25 03:13:33.414: INFO: stderr: ""
Sep 25 03:13:33.415: INFO: stdout: "e2e-test-crd-publish-openapi-1040-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 25 03:13:33.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-6400 delete e2e-test-crd-publish-openapi-1040-crds test-foo'
Sep 25 03:13:33.525: INFO: stderr: ""
Sep 25 03:13:33.525: INFO: stdout: "e2e-test-crd-publish-openapi-1040-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 25 03:13:33.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-6400 apply -f -'
Sep 25 03:13:33.727: INFO: stderr: ""
Sep 25 03:13:33.727: INFO: stdout: "e2e-test-crd-publish-openapi-1040-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 25 03:13:33.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-6400 delete e2e-test-crd-publish-openapi-1040-crds test-foo'
Sep 25 03:13:33.812: INFO: stderr: ""
Sep 25 03:13:33.812: INFO: stdout: "e2e-test-crd-publish-openapi-1040-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 25 03:13:33.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-6400 create -f -'
Sep 25 03:13:33.984: INFO: rc: 1
Sep 25 03:13:33.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-6400 apply -f -'
Sep 25 03:13:34.146: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 25 03:13:34.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-6400 create -f -'
Sep 25 03:13:34.327: INFO: rc: 1
Sep 25 03:13:34.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-6400 apply -f -'
Sep 25 03:13:34.522: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 25 03:13:34.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 explain e2e-test-crd-publish-openapi-1040-crds'
Sep 25 03:13:34.734: INFO: stderr: ""
Sep 25 03:13:34.734: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1040-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 25 03:13:34.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 explain e2e-test-crd-publish-openapi-1040-crds.metadata'
Sep 25 03:13:34.942: INFO: stderr: ""
Sep 25 03:13:34.942: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1040-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 25 03:13:34.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 explain e2e-test-crd-publish-openapi-1040-crds.spec'
Sep 25 03:13:35.133: INFO: stderr: ""
Sep 25 03:13:35.133: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1040-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 25 03:13:35.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 explain e2e-test-crd-publish-openapi-1040-crds.spec.bars'
Sep 25 03:13:35.324: INFO: stderr: ""
Sep 25 03:13:35.324: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1040-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 25 03:13:35.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 explain e2e-test-crd-publish-openapi-1040-crds.spec.bars2'
Sep 25 03:13:35.507: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:13:38.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6400" for this suite.

â€¢ [SLOW TEST:8.463 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":115,"skipped":1668,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:13:38.432: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1796
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 25 03:13:38.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2113'
Sep 25 03:13:38.671: INFO: stderr: ""
Sep 25 03:13:38.671: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 25 03:13:43.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pod e2e-test-httpd-pod --namespace=kubectl-2113 -o json'
Sep 25 03:13:43.810: INFO: stderr: ""
Sep 25 03:13:43.810: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.100.119/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.100.119/32\"\n        },\n        \"creationTimestamp\": \"2020-09-25T03:13:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2113\",\n        \"resourceVersion\": \"421127\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2113/pods/e2e-test-httpd-pod\",\n        \"uid\": \"55907ffc-6db2-4a1d-8a01-f6132e895606\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-ml5tw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"biz-k8s-node-3\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-ml5tw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-ml5tw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-25T03:13:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-25T03:13:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-25T03:13:41Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-09-25T03:13:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://bb927f5b2877134a8e55a09b6b2e7dc5501bcb792aff9c02db191d2a55daac4a\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-09-25T03:13:41Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.31\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.100.119\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.100.119\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-09-25T03:13:38Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 25 03:13:43.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 replace -f - --namespace=kubectl-2113'
Sep 25 03:13:44.080: INFO: stderr: ""
Sep 25 03:13:44.080: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1801
Sep 25 03:13:44.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete pods e2e-test-httpd-pod --namespace=kubectl-2113'
Sep 25 03:13:47.747: INFO: stderr: ""
Sep 25 03:13:47.747: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:13:47.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2113" for this suite.

â€¢ [SLOW TEST:9.322 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1792
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":116,"skipped":1671,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:13:47.755: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Sep 25 03:13:47.883: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:13:58.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2821" for this suite.

â€¢ [SLOW TEST:10.364 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":117,"skipped":1690,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:13:58.120: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 25 03:13:58.265: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2622 /api/v1/namespaces/watch-2622/configmaps/e2e-watch-test-resource-version f2e0ebda-9f99-43dc-be5e-81b3adfe7d51 421276 0 2020-09-25 03:13:58 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 25 03:13:58.266: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2622 /api/v1/namespaces/watch-2622/configmaps/e2e-watch-test-resource-version f2e0ebda-9f99-43dc-be5e-81b3adfe7d51 421277 0 2020-09-25 03:13:58 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:13:58.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2622" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":118,"skipped":1706,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:13:58.271: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:13:58.406: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f64e0da-7154-4bbc-bef9-d3908f9d2e87" in namespace "projected-667" to be "success or failure"
Sep 25 03:13:58.409: INFO: Pod "downwardapi-volume-9f64e0da-7154-4bbc-bef9-d3908f9d2e87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21833ms
Sep 25 03:14:00.413: INFO: Pod "downwardapi-volume-9f64e0da-7154-4bbc-bef9-d3908f9d2e87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006743903s
Sep 25 03:14:02.417: INFO: Pod "downwardapi-volume-9f64e0da-7154-4bbc-bef9-d3908f9d2e87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010741149s
STEP: Saw pod success
Sep 25 03:14:02.417: INFO: Pod "downwardapi-volume-9f64e0da-7154-4bbc-bef9-d3908f9d2e87" satisfied condition "success or failure"
Sep 25 03:14:02.420: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-9f64e0da-7154-4bbc-bef9-d3908f9d2e87 container client-container: <nil>
STEP: delete the pod
Sep 25 03:14:02.436: INFO: Waiting for pod downwardapi-volume-9f64e0da-7154-4bbc-bef9-d3908f9d2e87 to disappear
Sep 25 03:14:02.438: INFO: Pod downwardapi-volume-9f64e0da-7154-4bbc-bef9-d3908f9d2e87 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:14:02.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-667" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":119,"skipped":1721,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:14:02.445: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1074
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:182
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:14:02.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1074" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":120,"skipped":1764,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:14:02.592: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9868
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:14:02.731: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 25 03:14:07.733: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 25 03:14:07.733: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 25 03:14:09.737: INFO: Creating deployment "test-rollover-deployment"
Sep 25 03:14:09.745: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 25 03:14:11.755: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 25 03:14:11.762: INFO: Ensure that both replica sets have 1 created replica
Sep 25 03:14:11.768: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 25 03:14:11.775: INFO: Updating deployment test-rollover-deployment
Sep 25 03:14:11.775: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 25 03:14:13.783: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 25 03:14:13.789: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 25 03:14:13.796: INFO: all replica sets need to contain the pod-template-hash label
Sep 25 03:14:13.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600453, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 03:14:15.804: INFO: all replica sets need to contain the pod-template-hash label
Sep 25 03:14:15.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600453, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 03:14:17.804: INFO: all replica sets need to contain the pod-template-hash label
Sep 25 03:14:17.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600453, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 03:14:19.804: INFO: all replica sets need to contain the pod-template-hash label
Sep 25 03:14:19.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600453, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 03:14:21.804: INFO: all replica sets need to contain the pod-template-hash label
Sep 25 03:14:21.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600453, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600449, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 25 03:14:23.804: INFO: 
Sep 25 03:14:23.804: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Sep 25 03:14:23.814: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9868 /apis/apps/v1/namespaces/deployment-9868/deployments/test-rollover-deployment f9842370-983c-43d4-943c-386c122e2241 421578 2 2020-09-25 03:14:09 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0067c5cd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-09-25 03:14:09 +0000 UTC,LastTransitionTime:2020-09-25 03:14:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-09-25 03:14:23 +0000 UTC,LastTransitionTime:2020-09-25 03:14:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 25 03:14:23.817: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-9868 /apis/apps/v1/namespaces/deployment-9868/replicasets/test-rollover-deployment-574d6dfbff f0a4edb9-0a84-4109-9590-ffd40c399292 421567 2 2020-09-25 03:14:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment f9842370-983c-43d4-943c-386c122e2241 0xc0068021d7 0xc0068021d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006802248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 25 03:14:23.817: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 25 03:14:23.817: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9868 /apis/apps/v1/namespaces/deployment-9868/replicasets/test-rollover-controller 2015d2bb-407f-4811-8854-f5eef0775abd 421576 2 2020-09-25 03:14:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment f9842370-983c-43d4-943c-386c122e2241 0xc006802107 0xc006802108}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006802168 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 25 03:14:23.818: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-9868 /apis/apps/v1/namespaces/deployment-9868/replicasets/test-rollover-deployment-f6c94f66c 3d62f8ff-565e-4643-96c4-569a2e938523 421490 2 2020-09-25 03:14:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment f9842370-983c-43d4-943c-386c122e2241 0xc0068022e0 0xc0068022e1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006802368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 25 03:14:23.821: INFO: Pod "test-rollover-deployment-574d6dfbff-sww5w" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-sww5w test-rollover-deployment-574d6dfbff- deployment-9868 /api/v1/namespaces/deployment-9868/pods/test-rollover-deployment-574d6dfbff-sww5w 6a30706b-545c-49fd-8161-77f9dee8969f 421516 0 2020-09-25 03:14:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:192.168.100.67/32 cni.projectcalico.org/podIPs:192.168.100.67/32] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff f0a4edb9-0a84-4109-9590-ffd40c399292 0xc00667d9b7 0xc00667d9b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rwkxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rwkxr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rwkxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:14:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:14:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:14:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:14:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:192.168.100.67,StartTime:2020-09-25 03:14:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:14:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://10270fb4abc28f6631c7b2a374efe3165ea9206b2969bc978501e99ecb4bcf3a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:14:23.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9868" for this suite.

â€¢ [SLOW TEST:21.236 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":121,"skipped":1783,"failed":0}
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:14:23.828: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-60
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Sep 25 03:14:29.976: INFO: 2 pods remaining
Sep 25 03:14:29.976: INFO: 2 pods has nil DeletionTimestamp
Sep 25 03:14:29.976: INFO: 
STEP: Gathering metrics
Sep 25 03:14:30.985: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:14:30.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-60" for this suite.

â€¢ [SLOW TEST:7.162 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":122,"skipped":1783,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:14:30.991: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-84f5fdd8-1327-45b2-a8f6-aeab4b431027
STEP: Creating a pod to test consume secrets
Sep 25 03:14:31.134: INFO: Waiting up to 5m0s for pod "pod-secrets-3b8d2f17-40af-4da5-b959-6aabdfa9818c" in namespace "secrets-9433" to be "success or failure"
Sep 25 03:14:31.136: INFO: Pod "pod-secrets-3b8d2f17-40af-4da5-b959-6aabdfa9818c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.890251ms
Sep 25 03:14:33.142: INFO: Pod "pod-secrets-3b8d2f17-40af-4da5-b959-6aabdfa9818c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007593373s
Sep 25 03:14:35.146: INFO: Pod "pod-secrets-3b8d2f17-40af-4da5-b959-6aabdfa9818c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012450519s
STEP: Saw pod success
Sep 25 03:14:35.147: INFO: Pod "pod-secrets-3b8d2f17-40af-4da5-b959-6aabdfa9818c" satisfied condition "success or failure"
Sep 25 03:14:35.150: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-secrets-3b8d2f17-40af-4da5-b959-6aabdfa9818c container secret-volume-test: <nil>
STEP: delete the pod
Sep 25 03:14:35.163: INFO: Waiting for pod pod-secrets-3b8d2f17-40af-4da5-b959-6aabdfa9818c to disappear
Sep 25 03:14:35.165: INFO: Pod pod-secrets-3b8d2f17-40af-4da5-b959-6aabdfa9818c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:14:35.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9433" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":123,"skipped":1785,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:14:35.171: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7017
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 25 03:14:35.308: INFO: Waiting up to 5m0s for pod "pod-eedfba61-4a9b-48f5-93ad-de1fceed949b" in namespace "emptydir-7017" to be "success or failure"
Sep 25 03:14:35.313: INFO: Pod "pod-eedfba61-4a9b-48f5-93ad-de1fceed949b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.955651ms
Sep 25 03:14:37.318: INFO: Pod "pod-eedfba61-4a9b-48f5-93ad-de1fceed949b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009455445s
Sep 25 03:14:39.323: INFO: Pod "pod-eedfba61-4a9b-48f5-93ad-de1fceed949b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014015807s
STEP: Saw pod success
Sep 25 03:14:39.323: INFO: Pod "pod-eedfba61-4a9b-48f5-93ad-de1fceed949b" satisfied condition "success or failure"
Sep 25 03:14:39.326: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-eedfba61-4a9b-48f5-93ad-de1fceed949b container test-container: <nil>
STEP: delete the pod
Sep 25 03:14:39.342: INFO: Waiting for pod pod-eedfba61-4a9b-48f5-93ad-de1fceed949b to disappear
Sep 25 03:14:39.344: INFO: Pod pod-eedfba61-4a9b-48f5-93ad-de1fceed949b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:14:39.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7017" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":124,"skipped":1785,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:14:39.350: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-7694108c-df53-4afb-99b1-191be8f320a9
STEP: Creating a pod to test consume configMaps
Sep 25 03:14:39.487: INFO: Waiting up to 5m0s for pod "pod-configmaps-e527e7a6-904b-467a-943c-276c0d9911c4" in namespace "configmap-2801" to be "success or failure"
Sep 25 03:14:39.491: INFO: Pod "pod-configmaps-e527e7a6-904b-467a-943c-276c0d9911c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.987903ms
Sep 25 03:14:41.495: INFO: Pod "pod-configmaps-e527e7a6-904b-467a-943c-276c0d9911c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007277061s
Sep 25 03:14:43.499: INFO: Pod "pod-configmaps-e527e7a6-904b-467a-943c-276c0d9911c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011758904s
STEP: Saw pod success
Sep 25 03:14:43.499: INFO: Pod "pod-configmaps-e527e7a6-904b-467a-943c-276c0d9911c4" satisfied condition "success or failure"
Sep 25 03:14:43.503: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-configmaps-e527e7a6-904b-467a-943c-276c0d9911c4 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:14:43.517: INFO: Waiting for pod pod-configmaps-e527e7a6-904b-467a-943c-276c0d9911c4 to disappear
Sep 25 03:14:43.520: INFO: Pod pod-configmaps-e527e7a6-904b-467a-943c-276c0d9911c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:14:43.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2801" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":1797,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:14:43.526: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:325
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Sep 25 03:14:43.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-8656'
Sep 25 03:14:43.857: INFO: stderr: ""
Sep 25 03:14:43.857: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 25 03:14:43.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8656'
Sep 25 03:14:43.953: INFO: stderr: ""
Sep 25 03:14:43.953: INFO: stdout: "update-demo-nautilus-hc7qh update-demo-nautilus-mmvz5 "
Sep 25 03:14:43.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-hc7qh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8656'
Sep 25 03:14:44.048: INFO: stderr: ""
Sep 25 03:14:44.048: INFO: stdout: ""
Sep 25 03:14:44.048: INFO: update-demo-nautilus-hc7qh is created but not running
Sep 25 03:14:49.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8656'
Sep 25 03:14:49.149: INFO: stderr: ""
Sep 25 03:14:49.149: INFO: stdout: "update-demo-nautilus-hc7qh update-demo-nautilus-mmvz5 "
Sep 25 03:14:49.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-hc7qh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8656'
Sep 25 03:14:49.242: INFO: stderr: ""
Sep 25 03:14:49.242: INFO: stdout: "true"
Sep 25 03:14:49.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-hc7qh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8656'
Sep 25 03:14:49.340: INFO: stderr: ""
Sep 25 03:14:49.341: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 25 03:14:49.341: INFO: validating pod update-demo-nautilus-hc7qh
Sep 25 03:14:49.346: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 25 03:14:49.346: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 25 03:14:49.346: INFO: update-demo-nautilus-hc7qh is verified up and running
Sep 25 03:14:49.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-mmvz5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8656'
Sep 25 03:14:49.447: INFO: stderr: ""
Sep 25 03:14:49.447: INFO: stdout: "true"
Sep 25 03:14:49.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-mmvz5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8656'
Sep 25 03:14:49.542: INFO: stderr: ""
Sep 25 03:14:49.542: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 25 03:14:49.542: INFO: validating pod update-demo-nautilus-mmvz5
Sep 25 03:14:49.547: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 25 03:14:49.547: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 25 03:14:49.547: INFO: update-demo-nautilus-mmvz5 is verified up and running
STEP: rolling-update to new replication controller
Sep 25 03:14:49.551: INFO: scanned /root for discovery docs: <nil>
Sep 25 03:14:49.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8656'
Sep 25 03:15:12.993: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 25 03:15:12.993: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 25 03:15:12.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8656'
Sep 25 03:15:13.073: INFO: stderr: ""
Sep 25 03:15:13.073: INFO: stdout: "update-demo-kitten-45vgn update-demo-kitten-bh5bd "
Sep 25 03:15:13.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-kitten-45vgn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8656'
Sep 25 03:15:13.171: INFO: stderr: ""
Sep 25 03:15:13.171: INFO: stdout: "true"
Sep 25 03:15:13.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-kitten-45vgn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8656'
Sep 25 03:15:13.258: INFO: stderr: ""
Sep 25 03:15:13.258: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 25 03:15:13.258: INFO: validating pod update-demo-kitten-45vgn
Sep 25 03:15:13.264: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 25 03:15:13.264: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 25 03:15:13.264: INFO: update-demo-kitten-45vgn is verified up and running
Sep 25 03:15:13.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-kitten-bh5bd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8656'
Sep 25 03:15:13.375: INFO: stderr: ""
Sep 25 03:15:13.375: INFO: stdout: "true"
Sep 25 03:15:13.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-kitten-bh5bd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8656'
Sep 25 03:15:13.487: INFO: stderr: ""
Sep 25 03:15:13.487: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 25 03:15:13.487: INFO: validating pod update-demo-kitten-bh5bd
Sep 25 03:15:13.491: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 25 03:15:13.491: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 25 03:15:13.491: INFO: update-demo-kitten-bh5bd is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:15:13.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8656" for this suite.

â€¢ [SLOW TEST:29.971 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:323
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":126,"skipped":1809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:15:13.499: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep 25 03:15:13.637: INFO: Created pod &Pod{ObjectMeta:{dns-7473  dns-7473 /api/v1/namespaces/dns-7473/pods/dns-7473 3e855e3d-28b6-4901-87de-dab4ec26da28 422390 0 2020-09-25 03:15:13 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wjbpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wjbpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wjbpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Sep 25 03:15:17.646: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7473 PodName:dns-7473 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 03:15:17.646: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Verifying customized DNS server is configured on pod...
Sep 25 03:15:17.755: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7473 PodName:dns-7473 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 03:15:17.755: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 03:15:17.862: INFO: Deleting pod dns-7473...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:15:17.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7473" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":127,"skipped":1871,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:15:17.902: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3359
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:15:18.079: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 25 03:15:21.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-3359 create -f -'
Sep 25 03:15:21.940: INFO: stderr: ""
Sep 25 03:15:21.940: INFO: stdout: "e2e-test-crd-publish-openapi-5638-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 25 03:15:21.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-3359 delete e2e-test-crd-publish-openapi-5638-crds test-cr'
Sep 25 03:15:22.048: INFO: stderr: ""
Sep 25 03:15:22.048: INFO: stdout: "e2e-test-crd-publish-openapi-5638-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 25 03:15:22.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-3359 apply -f -'
Sep 25 03:15:22.278: INFO: stderr: ""
Sep 25 03:15:22.278: INFO: stdout: "e2e-test-crd-publish-openapi-5638-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 25 03:15:22.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=crd-publish-openapi-3359 delete e2e-test-crd-publish-openapi-5638-crds test-cr'
Sep 25 03:15:22.400: INFO: stderr: ""
Sep 25 03:15:22.400: INFO: stdout: "e2e-test-crd-publish-openapi-5638-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 25 03:15:22.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 explain e2e-test-crd-publish-openapi-5638-crds'
Sep 25 03:15:22.612: INFO: stderr: ""
Sep 25 03:15:22.612: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5638-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:15:25.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3359" for this suite.

â€¢ [SLOW TEST:7.670 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":128,"skipped":1888,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:15:25.572: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:15:36.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3581" for this suite.

â€¢ [SLOW TEST:11.166 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":129,"skipped":1917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:15:36.739: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-6201
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Sep 25 03:15:36.869: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 25 03:16:36.929: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:16:36.932: INFO: Starting informer...
STEP: Starting pod...
Sep 25 03:16:37.149: INFO: Pod is running on biz-k8s-node-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep 25 03:16:37.170: INFO: Pod wasn't evicted. Proceeding
Sep 25 03:16:37.170: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep 25 03:17:52.183: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:17:52.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6201" for this suite.

â€¢ [SLOW TEST:135.458 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":130,"skipped":1944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:17:52.198: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:17:52.331: INFO: Creating deployment "test-recreate-deployment"
Sep 25 03:17:52.336: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 25 03:17:52.342: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 25 03:17:54.348: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 25 03:17:54.350: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 25 03:17:54.354: INFO: Updating deployment test-recreate-deployment
Sep 25 03:17:54.354: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Sep 25 03:17:54.393: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9640 /apis/apps/v1/namespaces/deployment-9640/deployments/test-recreate-deployment 966e1228-3f01-4f2c-a51f-afa0e0ca0598 423423 2 2020-09-25 03:17:52 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a4a9b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-09-25 03:17:54 +0000 UTC,LastTransitionTime:2020-09-25 03:17:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-09-25 03:17:54 +0000 UTC,LastTransitionTime:2020-09-25 03:17:52 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 25 03:17:54.395: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-9640 /apis/apps/v1/namespaces/deployment-9640/replicasets/test-recreate-deployment-5f94c574ff f936584a-6431-416c-be8f-a054656fa0eb 423422 1 2020-09-25 03:17:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 966e1228-3f01-4f2c-a51f-afa0e0ca0598 0xc003a4ad77 0xc003a4ad78}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a4add8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 25 03:17:54.395: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 25 03:17:54.395: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-9640 /apis/apps/v1/namespaces/deployment-9640/replicasets/test-recreate-deployment-799c574856 f377af0e-70fa-4a1c-92d2-975b264da6a6 423412 2 2020-09-25 03:17:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 966e1228-3f01-4f2c-a51f-afa0e0ca0598 0xc003a4ae47 0xc003a4ae48}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a4aeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 25 03:17:54.398: INFO: Pod "test-recreate-deployment-5f94c574ff-c6sck" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-c6sck test-recreate-deployment-5f94c574ff- deployment-9640 /api/v1/namespaces/deployment-9640/pods/test-recreate-deployment-5f94c574ff-c6sck f65981d1-01e9-4210-b1bf-6ee32824f43f 423424 0 2020-09-25 03:17:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff f936584a-6431-416c-be8f-a054656fa0eb 0xc003a4b337 0xc003a4b338}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-p8vnj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-p8vnj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-p8vnj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:17:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:17:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:17:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:17:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:17:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:17:54.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9640" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":131,"skipped":1987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:17:54.403: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-765
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-765
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Sep 25 03:17:54.534: INFO: Found 0 stateful pods, waiting for 3
Sep 25 03:18:04.539: INFO: Found 2 stateful pods, waiting for 3
Sep 25 03:18:14.539: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:18:14.540: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:18:14.540: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:18:14.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-765 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 25 03:18:14.741: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 25 03:18:14.741: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 25 03:18:14.741: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 25 03:18:24.773: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 25 03:18:34.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-765 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 25 03:18:34.995: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 25 03:18:34.995: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 25 03:18:34.995: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 25 03:18:45.016: INFO: Waiting for StatefulSet statefulset-765/ss2 to complete update
Sep 25 03:18:45.016: INFO: Waiting for Pod statefulset-765/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 25 03:18:45.016: INFO: Waiting for Pod statefulset-765/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 25 03:18:55.024: INFO: Waiting for StatefulSet statefulset-765/ss2 to complete update
Sep 25 03:18:55.024: INFO: Waiting for Pod statefulset-765/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 25 03:19:05.023: INFO: Waiting for StatefulSet statefulset-765/ss2 to complete update
Sep 25 03:19:05.023: INFO: Waiting for Pod statefulset-765/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 25 03:19:15.024: INFO: Waiting for StatefulSet statefulset-765/ss2 to complete update
STEP: Rolling back to a previous revision
Sep 25 03:19:25.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-765 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 25 03:19:25.228: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 25 03:19:25.229: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 25 03:19:25.229: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 25 03:19:35.262: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 25 03:19:45.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-765 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 25 03:19:45.484: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 25 03:19:45.484: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 25 03:19:45.484: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 25 03:20:05.504: INFO: Waiting for StatefulSet statefulset-765/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Sep 25 03:20:15.517: INFO: Deleting all statefulset in ns statefulset-765
Sep 25 03:20:15.520: INFO: Scaling statefulset ss2 to 0
Sep 25 03:20:45.534: INFO: Waiting for statefulset status.replicas updated to 0
Sep 25 03:20:45.538: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:20:45.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-765" for this suite.

â€¢ [SLOW TEST:171.153 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":132,"skipped":2013,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:20:45.557: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Sep 25 03:20:45.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 api-versions'
Sep 25 03:20:45.783: INFO: stderr: ""
Sep 25 03:20:45.783: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\nconfig.gatekeeper.sh/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndex.coreos.com/v1\ndex.mesosphere.io/v1alpha1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nkommander.mesosphere.io/v1beta1\nkubeaddons.mesosphere.io/v1beta1\nkubeaddons.mesosphere.io/v1beta2\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplates.gatekeeper.sh/v1alpha1\ntemplates.gatekeeper.sh/v1beta1\nv1\nvelero.io/v1\nwebhook.certmanager.k8s.io/v1beta1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:20:45.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3444" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":133,"skipped":2022,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:20:45.791: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:20:46.400: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:20:48.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600846, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600846, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600846, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600846, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:20:51.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 25 03:20:53.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 attach --namespace=webhook-1382 to-be-attached-pod -i -c=container1'
Sep 25 03:20:53.594: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:20:53.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1382" for this suite.
STEP: Destroying namespace "webhook-1382-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.853 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":134,"skipped":2026,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:20:53.646: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2245
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 25 03:20:53.785: INFO: Waiting up to 5m0s for pod "pod-1f7c9f99-d754-4c6f-92d7-344eb69ade12" in namespace "emptydir-2245" to be "success or failure"
Sep 25 03:20:53.789: INFO: Pod "pod-1f7c9f99-d754-4c6f-92d7-344eb69ade12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.60425ms
Sep 25 03:20:55.792: INFO: Pod "pod-1f7c9f99-d754-4c6f-92d7-344eb69ade12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007344274s
STEP: Saw pod success
Sep 25 03:20:55.792: INFO: Pod "pod-1f7c9f99-d754-4c6f-92d7-344eb69ade12" satisfied condition "success or failure"
Sep 25 03:20:55.794: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-1f7c9f99-d754-4c6f-92d7-344eb69ade12 container test-container: <nil>
STEP: delete the pod
Sep 25 03:20:55.820: INFO: Waiting for pod pod-1f7c9f99-d754-4c6f-92d7-344eb69ade12 to disappear
Sep 25 03:20:55.822: INFO: Pod pod-1f7c9f99-d754-4c6f-92d7-344eb69ade12 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:20:55.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2245" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":135,"skipped":2027,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:20:55.829: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:21:13.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1964" for this suite.

â€¢ [SLOW TEST:17.183 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":136,"skipped":2035,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:21:13.013: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9722
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:21:13.143: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:21:17.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9722" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":137,"skipped":2036,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:21:17.266: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:21:17.435: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"da0969c7-aea9-47db-b387-c814abadd4f9", Controller:(*bool)(0xc004c8a12a), BlockOwnerDeletion:(*bool)(0xc004c8a12b)}}
Sep 25 03:21:17.444: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"7fabcc3a-5b86-4087-b5ad-3b7953f5d98f", Controller:(*bool)(0xc006a67ab2), BlockOwnerDeletion:(*bool)(0xc006a67ab3)}}
Sep 25 03:21:17.451: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"15325072-3554-416d-9911-e111aa5cc341", Controller:(*bool)(0xc006a67c9a), BlockOwnerDeletion:(*bool)(0xc006a67c9b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:21:22.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5203" for this suite.

â€¢ [SLOW TEST:5.199 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":138,"skipped":2039,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:21:22.466: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Sep 25 03:21:22.602: INFO: Waiting up to 5m0s for pod "client-containers-d582106a-670d-4cc2-915c-469b7f5ba8e7" in namespace "containers-8289" to be "success or failure"
Sep 25 03:21:22.605: INFO: Pod "client-containers-d582106a-670d-4cc2-915c-469b7f5ba8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.930341ms
Sep 25 03:21:24.609: INFO: Pod "client-containers-d582106a-670d-4cc2-915c-469b7f5ba8e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007576341s
Sep 25 03:21:26.615: INFO: Pod "client-containers-d582106a-670d-4cc2-915c-469b7f5ba8e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01320231s
STEP: Saw pod success
Sep 25 03:21:26.615: INFO: Pod "client-containers-d582106a-670d-4cc2-915c-469b7f5ba8e7" satisfied condition "success or failure"
Sep 25 03:21:26.618: INFO: Trying to get logs from node biz-k8s-node-3 pod client-containers-d582106a-670d-4cc2-915c-469b7f5ba8e7 container test-container: <nil>
STEP: delete the pod
Sep 25 03:21:26.632: INFO: Waiting for pod client-containers-d582106a-670d-4cc2-915c-469b7f5ba8e7 to disappear
Sep 25 03:21:26.633: INFO: Pod client-containers-d582106a-670d-4cc2-915c-469b7f5ba8e7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:21:26.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8289" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":139,"skipped":2060,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:21:26.643: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1869
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:21:37.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1869" for this suite.

â€¢ [SLOW TEST:11.181 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":140,"skipped":2063,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:21:37.824: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 25 03:21:37.972: INFO: Waiting up to 5m0s for pod "pod-8743632c-8da8-45cc-b6ec-412876008a0c" in namespace "emptydir-1848" to be "success or failure"
Sep 25 03:21:37.976: INFO: Pod "pod-8743632c-8da8-45cc-b6ec-412876008a0c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.957126ms
Sep 25 03:21:39.981: INFO: Pod "pod-8743632c-8da8-45cc-b6ec-412876008a0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008144712s
Sep 25 03:21:41.986: INFO: Pod "pod-8743632c-8da8-45cc-b6ec-412876008a0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013249479s
STEP: Saw pod success
Sep 25 03:21:41.986: INFO: Pod "pod-8743632c-8da8-45cc-b6ec-412876008a0c" satisfied condition "success or failure"
Sep 25 03:21:41.989: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-8743632c-8da8-45cc-b6ec-412876008a0c container test-container: <nil>
STEP: delete the pod
Sep 25 03:21:42.004: INFO: Waiting for pod pod-8743632c-8da8-45cc-b6ec-412876008a0c to disappear
Sep 25 03:21:42.007: INFO: Pod pod-8743632c-8da8-45cc-b6ec-412876008a0c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:21:42.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1848" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":141,"skipped":2070,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:21:42.014: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-b7696687-8347-4724-9a2b-f9abd287842c
STEP: Creating a pod to test consume secrets
Sep 25 03:21:42.162: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ac740ec6-1c65-4345-aeae-3f41c3124471" in namespace "projected-8075" to be "success or failure"
Sep 25 03:21:42.166: INFO: Pod "pod-projected-secrets-ac740ec6-1c65-4345-aeae-3f41c3124471": Phase="Pending", Reason="", readiness=false. Elapsed: 3.63741ms
Sep 25 03:21:44.171: INFO: Pod "pod-projected-secrets-ac740ec6-1c65-4345-aeae-3f41c3124471": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00855404s
Sep 25 03:21:46.176: INFO: Pod "pod-projected-secrets-ac740ec6-1c65-4345-aeae-3f41c3124471": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013397378s
STEP: Saw pod success
Sep 25 03:21:46.176: INFO: Pod "pod-projected-secrets-ac740ec6-1c65-4345-aeae-3f41c3124471" satisfied condition "success or failure"
Sep 25 03:21:46.179: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-projected-secrets-ac740ec6-1c65-4345-aeae-3f41c3124471 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 25 03:21:46.195: INFO: Waiting for pod pod-projected-secrets-ac740ec6-1c65-4345-aeae-3f41c3124471 to disappear
Sep 25 03:21:46.197: INFO: Pod pod-projected-secrets-ac740ec6-1c65-4345-aeae-3f41c3124471 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:21:46.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8075" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":142,"skipped":2076,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:21:46.206: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Sep 25 03:21:46.346: INFO: Waiting up to 5m0s for pod "downward-api-bd4fae88-a089-4a76-b26f-56de7fa4bacd" in namespace "downward-api-1650" to be "success or failure"
Sep 25 03:21:46.349: INFO: Pod "downward-api-bd4fae88-a089-4a76-b26f-56de7fa4bacd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597132ms
Sep 25 03:21:48.354: INFO: Pod "downward-api-bd4fae88-a089-4a76-b26f-56de7fa4bacd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007895095s
Sep 25 03:21:50.358: INFO: Pod "downward-api-bd4fae88-a089-4a76-b26f-56de7fa4bacd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011200607s
STEP: Saw pod success
Sep 25 03:21:50.358: INFO: Pod "downward-api-bd4fae88-a089-4a76-b26f-56de7fa4bacd" satisfied condition "success or failure"
Sep 25 03:21:50.360: INFO: Trying to get logs from node biz-k8s-node-3 pod downward-api-bd4fae88-a089-4a76-b26f-56de7fa4bacd container dapi-container: <nil>
STEP: delete the pod
Sep 25 03:21:50.371: INFO: Waiting for pod downward-api-bd4fae88-a089-4a76-b26f-56de7fa4bacd to disappear
Sep 25 03:21:50.373: INFO: Pod downward-api-bd4fae88-a089-4a76-b26f-56de7fa4bacd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:21:50.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1650" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":143,"skipped":2090,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:21:50.379: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 25 03:21:50.517: INFO: Waiting up to 5m0s for pod "pod-4d03dc27-4966-4776-ac0c-507be475e374" in namespace "emptydir-907" to be "success or failure"
Sep 25 03:21:50.520: INFO: Pod "pod-4d03dc27-4966-4776-ac0c-507be475e374": Phase="Pending", Reason="", readiness=false. Elapsed: 2.684429ms
Sep 25 03:21:52.523: INFO: Pod "pod-4d03dc27-4966-4776-ac0c-507be475e374": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006388087s
Sep 25 03:21:54.527: INFO: Pod "pod-4d03dc27-4966-4776-ac0c-507be475e374": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010141354s
STEP: Saw pod success
Sep 25 03:21:54.527: INFO: Pod "pod-4d03dc27-4966-4776-ac0c-507be475e374" satisfied condition "success or failure"
Sep 25 03:21:54.530: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-4d03dc27-4966-4776-ac0c-507be475e374 container test-container: <nil>
STEP: delete the pod
Sep 25 03:21:54.546: INFO: Waiting for pod pod-4d03dc27-4966-4776-ac0c-507be475e374 to disappear
Sep 25 03:21:54.547: INFO: Pod pod-4d03dc27-4966-4776-ac0c-507be475e374 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:21:54.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-907" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":144,"skipped":2102,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:21:54.554: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep 25 03:22:04.707: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:22:04.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6834" for this suite.

â€¢ [SLOW TEST:10.160 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":145,"skipped":2122,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:22:04.715: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:22:04.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37c5cf7b-01dd-4ce7-b999-a1453272e30e" in namespace "downward-api-7851" to be "success or failure"
Sep 25 03:22:04.869: INFO: Pod "downwardapi-volume-37c5cf7b-01dd-4ce7-b999-a1453272e30e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.027644ms
Sep 25 03:22:06.872: INFO: Pod "downwardapi-volume-37c5cf7b-01dd-4ce7-b999-a1453272e30e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008863599s
Sep 25 03:22:08.877: INFO: Pod "downwardapi-volume-37c5cf7b-01dd-4ce7-b999-a1453272e30e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013316461s
STEP: Saw pod success
Sep 25 03:22:08.877: INFO: Pod "downwardapi-volume-37c5cf7b-01dd-4ce7-b999-a1453272e30e" satisfied condition "success or failure"
Sep 25 03:22:08.880: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-37c5cf7b-01dd-4ce7-b999-a1453272e30e container client-container: <nil>
STEP: delete the pod
Sep 25 03:22:08.901: INFO: Waiting for pod downwardapi-volume-37c5cf7b-01dd-4ce7-b999-a1453272e30e to disappear
Sep 25 03:22:08.903: INFO: Pod downwardapi-volume-37c5cf7b-01dd-4ce7-b999-a1453272e30e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:22:08.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7851" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":146,"skipped":2126,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:22:08.911: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7599
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Sep 25 03:22:09.567: INFO: created pod pod-service-account-defaultsa
Sep 25 03:22:09.567: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 25 03:22:09.581: INFO: created pod pod-service-account-mountsa
Sep 25 03:22:09.581: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 25 03:22:09.589: INFO: created pod pod-service-account-nomountsa
Sep 25 03:22:09.589: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 25 03:22:09.598: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 25 03:22:09.598: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 25 03:22:09.607: INFO: created pod pod-service-account-mountsa-mountspec
Sep 25 03:22:09.607: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 25 03:22:09.617: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 25 03:22:09.617: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 25 03:22:09.624: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 25 03:22:09.624: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 25 03:22:09.629: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 25 03:22:09.629: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 25 03:22:09.637: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 25 03:22:09.637: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:22:09.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7599" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":147,"skipped":2132,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:22:09.645: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:22:25.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8171" for this suite.

â€¢ [SLOW TEST:16.227 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":148,"skipped":2136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:22:25.873: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1026
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1490
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 25 03:22:26.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1026'
Sep 25 03:22:26.129: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 25 03:22:26.129: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1496
Sep 25 03:22:26.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1026'
Sep 25 03:22:26.240: INFO: stderr: ""
Sep 25 03:22:26.240: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:22:26.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1026" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":149,"skipped":2173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:22:26.250: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4721
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-118157e4-923b-4cc0-98e9-143c50a3f312
STEP: Creating a pod to test consume configMaps
Sep 25 03:22:26.386: INFO: Waiting up to 5m0s for pod "pod-configmaps-0784486e-0a6b-4cb3-8b9e-456a71edafb8" in namespace "configmap-4721" to be "success or failure"
Sep 25 03:22:26.389: INFO: Pod "pod-configmaps-0784486e-0a6b-4cb3-8b9e-456a71edafb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631092ms
Sep 25 03:22:28.393: INFO: Pod "pod-configmaps-0784486e-0a6b-4cb3-8b9e-456a71edafb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006767607s
Sep 25 03:22:30.399: INFO: Pod "pod-configmaps-0784486e-0a6b-4cb3-8b9e-456a71edafb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012776121s
STEP: Saw pod success
Sep 25 03:22:30.399: INFO: Pod "pod-configmaps-0784486e-0a6b-4cb3-8b9e-456a71edafb8" satisfied condition "success or failure"
Sep 25 03:22:30.401: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-configmaps-0784486e-0a6b-4cb3-8b9e-456a71edafb8 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:22:30.419: INFO: Waiting for pod pod-configmaps-0784486e-0a6b-4cb3-8b9e-456a71edafb8 to disappear
Sep 25 03:22:30.422: INFO: Pod pod-configmaps-0784486e-0a6b-4cb3-8b9e-456a71edafb8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:22:30.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4721" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":150,"skipped":2207,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:22:30.428: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Sep 25 03:22:30.554: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 25 03:22:30.566: INFO: Waiting for terminating namespaces to be deleted...
Sep 25 03:22:30.568: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-1 before test
Sep 25 03:22:30.599: INFO: minio-0 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:22:30.599: INFO: daemon-notify-74b7d7c4b5-zklwv from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container daemon-notify ready: true, restart count 0
Sep 25 03:22:30.599: INFO: store-api-mypage-5d6d59c499-dvczz from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container store-api-mypage ready: true, restart count 0
Sep 25 03:22:30.599: INFO: dex-k8s-authenticator-kubeaddons-74966666f5-gvf92 from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container dex-k8s-authenticator ready: true, restart count 1
Sep 25 03:22:30.599: INFO: sonobuoy-e2e-job-a16116856b774ea6 from sonobuoy started at 2020-09-25 02:35:55 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container e2e ready: true, restart count 0
Sep 25 03:22:30.599: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:22:30.599: INFO: admin-api-common-596bf5fdb7-6pw8d from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container admin-api-common ready: true, restart count 0
Sep 25 03:22:30.599: INFO: portal-api-dock-5c5497b47f-8qjn6 from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container portal-api-dock ready: true, restart count 0
Sep 25 03:22:30.599: INFO: coredns-849d6f84b4-cc8tj from kube-system started at 2020-09-24 03:18:43 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container coredns ready: true, restart count 0
Sep 25 03:22:30.599: INFO: metallb-kubeaddons-speaker-cf5wv from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:22:30.599: INFO: builder-api-main-54dfc4474c-gnvlr from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container builder-api-main ready: true, restart count 0
Sep 25 03:22:30.599: INFO: mdcs-api-common-84c884c5c7-r4kt9 from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container mdcs-api-common ready: true, restart count 0
Sep 25 03:22:30.599: INFO: common-api-auth-c49dcbcc9-4xb9s from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container common-api-auth ready: true, restart count 0
Sep 25 03:22:30.599: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-pqlj2 from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:22:30.599: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:22:30.599: INFO: velero-kubeaddons-5d85fcdcb9-6lt97 from velero started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container velero ready: true, restart count 0
Sep 25 03:22:30.599: INFO: traefik-kubeaddons-68c579bbbd-2qmn4 from kubeaddons started at 2020-09-24 03:26:59 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Sep 25 03:22:30.599: INFO: store-api-appmanage-6b8b464c4c-m528t from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container store-api-appmanage ready: true, restart count 0
Sep 25 03:22:30.599: INFO: common-api-notify-5595dbd9b5-rb8vd from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container common-api-notify ready: true, restart count 0
Sep 25 03:22:30.599: INFO: kube-proxy-ff4h2 from kube-system started at 2020-09-24 03:18:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:22:30.599: INFO: dex-kubeaddons-58884f456-xtvhz from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container main ready: true, restart count 0
Sep 25 03:22:30.599: INFO: local-volume-provisioner-rmgc9 from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:22:30.599: INFO: builder-daemon-main-fcb6b989c-hfwnc from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container builder-daemon-main ready: true, restart count 0
Sep 25 03:22:30.599: INFO: portal-cdn-download-5cd45dfd87-fqvrl from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container portal-cdn-download ready: false, restart count 17
Sep 25 03:22:30.599: INFO: sonobuoy from sonobuoy started at 2020-09-25 02:35:47 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 25 03:22:30.599: INFO: calico-node-k6ztw from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:22:30.599: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:22:30.599: INFO: calico-kube-controllers-77c79f7594-5qtpj from kube-system started at 2020-09-24 03:18:35 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 25 03:22:30.599: INFO: kubernetes-dashboard-95959d56b-wmxhc from kubeaddons started at 2020-09-24 03:25:22 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Sep 25 03:22:30.599: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 25 03:22:30.599: INFO: minio-3 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:22:30.599: INFO: traefik-forward-auth-kubeaddons-69bbb98fb6-fdn8n from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container traefik-forward-auth ready: true, restart count 1
Sep 25 03:22:30.599: INFO: tiller-deploy-6cdf7f9d6f-5mjft from kube-system started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container tiller ready: true, restart count 0
Sep 25 03:22:30.599: INFO: common-api-account-6b7c79d795-d846m from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container common-api-account ready: true, restart count 0
Sep 25 03:22:30.599: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-q9x6d from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
Sep 25 03:22:30.599: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-rw4pq from cert-manager started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.599: INFO: 	Container cert-manager ready: true, restart count 0
Sep 25 03:22:30.599: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-2 before test
Sep 25 03:22:30.630: INFO: local-volume-provisioner-n7q6g from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.630: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:22:30.630: INFO: common-api-oauth-6cd64bdc5-nkwk5 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.630: INFO: 	Container common-api-oauth ready: true, restart count 0
Sep 25 03:22:30.630: INFO: core-router-57d6544c9-j52x2 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.630: INFO: 	Container core-router ready: true, restart count 0
Sep 25 03:22:30.630: INFO: workflow-api-common-76dd6fb889-gbn6z from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.630: INFO: 	Container workflow-api-common ready: true, restart count 0
Sep 25 03:22:30.630: INFO: workflow-daemon-draft-5955b46c89-t7p8l from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.630: INFO: 	Container workflow-daemon-draft ready: true, restart count 0
Sep 25 03:22:30.630: INFO: metallb-kubeaddons-speaker-pvqct from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.630: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:22:30.630: INFO: metallb-kubeaddons-controller-f84b74d86-8hz2m from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.630: INFO: 	Container controller ready: true, restart count 0
Sep 25 03:22:30.631: INFO: store-api-bizgroup-695cc488b5-4pmrh from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container store-api-bizgroup ready: true, restart count 0
Sep 25 03:22:30.631: INFO: opsportal-landing-6f6865b688-v7dhz from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container opsportal-landing ready: true, restart count 0
Sep 25 03:22:30.631: INFO: gatekeeper-kubeaddons-fdc87db85-9q9kx from kubeaddons started at 2020-09-24 03:26:34 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:22:30.631: INFO: portal-api-page-766cc7bf79-vcwbs from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container portal-api-page ready: true, restart count 0
Sep 25 03:22:30.631: INFO: traefik-kubeaddons-68c579bbbd-zwq9n from kubeaddons started at 2020-09-24 03:26:37 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Sep 25 03:22:30.631: INFO: store-api-common-bfd5d66c5-2wkcr from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container store-api-common ready: true, restart count 0
Sep 25 03:22:30.631: INFO: builder-api-custom-676877997f-5vft8 from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container builder-api-custom ready: true, restart count 0
Sep 25 03:22:30.631: INFO: kube-proxy-5jx4d from kube-system started at 2020-09-24 03:18:13 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:22:30.631: INFO: reloader-kubeaddons-reloader-7c97f877cf-hpgvn from kubeaddons started at 2020-09-24 03:25:24 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
Sep 25 03:22:30.631: INFO: daemon-task-6c8f8f5ffc-pbmqt from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container daemon-task ready: true, restart count 0
Sep 25 03:22:30.631: INFO: coredns-849d6f84b4-qdfhl from kube-system started at 2020-09-24 03:18:43 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container coredns ready: true, restart count 0
Sep 25 03:22:30.631: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-g6ss8 from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:22:30.631: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:22:30.631: INFO: cert-manager-kubeaddons-7d7f98fbc6-fxzfx from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container cert-manager ready: true, restart count 0
Sep 25 03:22:30.631: INFO: dstorageclass-controller-manager-5c966c767f-ngg5l from kubeaddons started at 2020-09-24 03:26:27 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:22:30.631: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:22:30.631: INFO: portal-cdn-upload-567d9969fc-9b6dv from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container portal-cdn-upload ready: false, restart count 9
Sep 25 03:22:30.631: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-qc42h from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:22:30.631: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:22:30.631: INFO: daemon-account-866d44497f-5q7dl from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container daemon-account ready: true, restart count 0
Sep 25 03:22:30.631: INFO: dev-biz-portal-front-687d79db75-q4rwx from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container dev-biz-portal-front ready: false, restart count 17
Sep 25 03:22:30.631: INFO: minio-2 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:22:30.631: INFO: core-sockjs-79b764df9f-24fs4 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container core-sockjs ready: true, restart count 0
Sep 25 03:22:30.631: INFO: workflow-api-draft-5899bb55c9-d4dhd from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container workflow-api-draft ready: true, restart count 0
Sep 25 03:22:30.631: INFO: calico-node-mj6vw from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:22:30.631: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:22:30.631: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-92k4x from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container cainjector ready: true, restart count 0
Sep 25 03:22:30.631: INFO: opsportal-kubeaddons-kommander-ui-6d64cc5d54-c8pct from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
Sep 25 03:22:30.631: INFO: kubeaddons-controller-manager-659bd9c576-jfg77 from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.631: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:22:30.631: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-3 before test
Sep 25 03:22:30.644: INFO: velero-kubeaddons-5d85fcdcb9-gwwkq from velero started at 2020-09-24 03:28:07 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container velero ready: true, restart count 3
Sep 25 03:22:30.644: INFO: dex-k8s-authenticator-kubeaddons-74966666f5-9qtxz from kubeaddons started at 2020-09-24 03:28:01 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container dex-k8s-authenticator ready: false, restart count 0
Sep 25 03:22:30.644: INFO: tiller-deploy-6cdf7f9d6f-qqfpj from kube-system started at 2020-09-24 03:25:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container tiller ready: false, restart count 0
Sep 25 03:22:30.644: INFO: dex-kubeaddons-58884f456-gtkb7 from kubeaddons started at 2020-09-24 03:27:50 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container main ready: true, restart count 1
Sep 25 03:22:30.644: INFO: traefik-forward-auth-kubeaddons-69bbb98fb6-g4fpr from kubeaddons started at 2020-09-24 03:28:06 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container traefik-forward-auth ready: false, restart count 0
Sep 25 03:22:30.644: INFO: builder-api-custom-676877997f-5dj9d from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container builder-api-custom ready: true, restart count 0
Sep 25 03:22:30.644: INFO: local-volume-provisioner-972q8 from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:22:30.644: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-pzrmp from kubeaddons started at 2020-09-24 03:27:52 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container kube-oidc-proxy ready: false, restart count 0
Sep 25 03:22:30.644: INFO: store-api-common-bfd5d66c5-wcnh9 from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container store-api-common ready: true, restart count 0
Sep 25 03:22:30.644: INFO: kube-proxy-lpjq2 from kube-system started at 2020-09-24 03:18:13 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:22:30.644: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-jvmxg from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container cert-manager ready: true, restart count 1
Sep 25 03:22:30.644: INFO: opsportal-landing-6f6865b688-8sfw4 from kubeaddons started at 2020-09-24 03:25:44 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container opsportal-landing ready: true, restart count 0
Sep 25 03:22:30.644: INFO: admin-api-common-596bf5fdb7-dkf5t from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container admin-api-common ready: true, restart count 0
Sep 25 03:22:30.644: INFO: daemon-task-6c8f8f5ffc-wfsxt from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container daemon-task ready: true, restart count 0
Sep 25 03:22:30.644: INFO: calico-node-bzvbt from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:22:30.644: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:22:30.644: INFO: opsportal-kubeaddons-kommander-ui-6d64cc5d54-gxbm8 from kubeaddons started at 2020-09-24 03:25:44 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: false, restart count 0
Sep 25 03:22:30.644: INFO: workflow-api-draft-5899bb55c9-j5dgh from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container workflow-api-draft ready: true, restart count 0
Sep 25 03:22:30.644: INFO: kubeaddons-controller-manager-659bd9c576-g2wjv from kubeaddons started at 2020-09-24 03:25:02 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:22:30.644: INFO: metallb-kubeaddons-speaker-lbzvt from kubeaddons started at 2020-09-25 03:16:37 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:22:30.644: INFO: portal-api-dock-5c5497b47f-6tctr from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container portal-api-dock ready: true, restart count 0
Sep 25 03:22:30.644: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-2cctd from kubeaddons started at 2020-09-24 03:27:26 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:22:30.644: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:22:30.644: INFO: minio-1 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container minio ready: false, restart count 0
Sep 25 03:22:30.644: INFO: common-api-notify-5595dbd9b5-l4l6r from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container common-api-notify ready: true, restart count 0
Sep 25 03:22:30.644: INFO: portal-cdn-upload-567d9969fc-d4sq2 from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container portal-cdn-upload ready: false, restart count 12
Sep 25 03:22:30.644: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-vbvdn from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:22:30.644: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:22:30.644: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1637e8cae49682f1], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:22:31.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-255" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":151,"skipped":2208,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:22:31.689: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 25 03:22:31.835: INFO: Waiting up to 5m0s for pod "pod-4436be47-692f-430b-8c80-c3fc1dcf8093" in namespace "emptydir-2385" to be "success or failure"
Sep 25 03:22:31.838: INFO: Pod "pod-4436be47-692f-430b-8c80-c3fc1dcf8093": Phase="Pending", Reason="", readiness=false. Elapsed: 3.27926ms
Sep 25 03:22:33.842: INFO: Pod "pod-4436be47-692f-430b-8c80-c3fc1dcf8093": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007540351s
Sep 25 03:22:35.847: INFO: Pod "pod-4436be47-692f-430b-8c80-c3fc1dcf8093": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012271205s
STEP: Saw pod success
Sep 25 03:22:35.847: INFO: Pod "pod-4436be47-692f-430b-8c80-c3fc1dcf8093" satisfied condition "success or failure"
Sep 25 03:22:35.851: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-4436be47-692f-430b-8c80-c3fc1dcf8093 container test-container: <nil>
STEP: delete the pod
Sep 25 03:22:35.869: INFO: Waiting for pod pod-4436be47-692f-430b-8c80-c3fc1dcf8093 to disappear
Sep 25 03:22:35.872: INFO: Pod pod-4436be47-692f-430b-8c80-c3fc1dcf8093 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:22:35.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2385" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":152,"skipped":2217,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:22:35.882: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6714
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-ab5723f0-c3e2-4583-bffb-3e37dbf56d24
STEP: Creating a pod to test consume configMaps
Sep 25 03:22:36.029: INFO: Waiting up to 5m0s for pod "pod-configmaps-b8ca8757-67ee-4891-a791-6a64b971b307" in namespace "configmap-6714" to be "success or failure"
Sep 25 03:22:36.032: INFO: Pod "pod-configmaps-b8ca8757-67ee-4891-a791-6a64b971b307": Phase="Pending", Reason="", readiness=false. Elapsed: 3.093282ms
Sep 25 03:22:38.036: INFO: Pod "pod-configmaps-b8ca8757-67ee-4891-a791-6a64b971b307": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007123892s
Sep 25 03:22:40.041: INFO: Pod "pod-configmaps-b8ca8757-67ee-4891-a791-6a64b971b307": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011425604s
STEP: Saw pod success
Sep 25 03:22:40.041: INFO: Pod "pod-configmaps-b8ca8757-67ee-4891-a791-6a64b971b307" satisfied condition "success or failure"
Sep 25 03:22:40.044: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-configmaps-b8ca8757-67ee-4891-a791-6a64b971b307 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:22:40.061: INFO: Waiting for pod pod-configmaps-b8ca8757-67ee-4891-a791-6a64b971b307 to disappear
Sep 25 03:22:40.065: INFO: Pod pod-configmaps-b8ca8757-67ee-4891-a791-6a64b971b307 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:22:40.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6714" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":153,"skipped":2233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:22:40.074: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1629
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep 25 03:22:46.231: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1629 PodName:pod-sharedvolume-5d588288-a737-4e2a-a01e-c98556b3c550 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 03:22:46.231: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 03:22:46.332: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:22:46.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1629" for this suite.

â€¢ [SLOW TEST:6.267 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":154,"skipped":2258,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:22:46.341: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:23:10.506: INFO: Container started at 2020-09-25 03:22:47 +0000 UTC, pod became ready at 2020-09-25 03:23:10 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:23:10.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4094" for this suite.

â€¢ [SLOW TEST:24.172 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":155,"skipped":2267,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:23:10.513: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:23:11.019: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:23:13.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600991, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600991, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600991, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736600991, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:23:16.041: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:23:16.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2002" for this suite.
STEP: Destroying namespace "webhook-2002-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.628 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":156,"skipped":2267,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:23:16.142: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1760
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 25 03:23:16.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3953'
Sep 25 03:23:16.363: INFO: stderr: ""
Sep 25 03:23:16.363: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1765
Sep 25 03:23:16.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete pods e2e-test-httpd-pod --namespace=kubectl-3953'
Sep 25 03:23:22.974: INFO: stderr: ""
Sep 25 03:23:22.974: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:23:22.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3953" for this suite.

â€¢ [SLOW TEST:6.840 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1756
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":157,"skipped":2288,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:23:22.983: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2968
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 25 03:23:26.138: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:23:26.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2968" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":158,"skipped":2303,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:23:26.150: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 25 03:23:28.299: INFO: &Pod{ObjectMeta:{send-events-4acd2bfc-98a0-4bfe-9a31-4e90dba0ee10  events-6122 /api/v1/namespaces/events-6122/pods/send-events-4acd2bfc-98a0-4bfe-9a31-4e90dba0ee10 f5b25233-fada-4b8f-8ed7-352a598619db 426729 0 2020-09-25 03:23:26 +0000 UTC <nil> <nil> map[name:foo time:277299221] map[cni.projectcalico.org/podIP:192.168.100.108/32 cni.projectcalico.org/podIPs:192.168.100.108/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hzbmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hzbmm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hzbmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:23:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:23:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:23:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:23:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:192.168.100.108,StartTime:2020-09-25 03:23:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:23:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://39c01b8826566eb1e4076dbeda4fe3a0e838f2a1137de71ec2d94a7abaae58f8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 25 03:23:30.304: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 25 03:23:32.309: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:23:32.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6122" for this suite.

â€¢ [SLOW TEST:6.174 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":159,"skipped":2319,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:23:32.324: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-147daacd-d7ec-4f85-96e9-ec4ddcb2150e
STEP: Creating a pod to test consume secrets
Sep 25 03:23:32.477: INFO: Waiting up to 5m0s for pod "pod-secrets-e0784c69-17fc-4540-a100-cef818cc61df" in namespace "secrets-9534" to be "success or failure"
Sep 25 03:23:32.479: INFO: Pod "pod-secrets-e0784c69-17fc-4540-a100-cef818cc61df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12874ms
Sep 25 03:23:34.484: INFO: Pod "pod-secrets-e0784c69-17fc-4540-a100-cef818cc61df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006813711s
STEP: Saw pod success
Sep 25 03:23:34.484: INFO: Pod "pod-secrets-e0784c69-17fc-4540-a100-cef818cc61df" satisfied condition "success or failure"
Sep 25 03:23:34.487: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-secrets-e0784c69-17fc-4540-a100-cef818cc61df container secret-volume-test: <nil>
STEP: delete the pod
Sep 25 03:23:34.500: INFO: Waiting for pod pod-secrets-e0784c69-17fc-4540-a100-cef818cc61df to disappear
Sep 25 03:23:34.502: INFO: Pod pod-secrets-e0784c69-17fc-4540-a100-cef818cc61df no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:23:34.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9534" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":160,"skipped":2325,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:23:34.509: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1904
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7415
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:23:40.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6975" for this suite.
STEP: Destroying namespace "nsdeletetest-1904" for this suite.
Sep 25 03:23:40.926: INFO: Namespace nsdeletetest-1904 was already deleted
STEP: Destroying namespace "nsdeletetest-7415" for this suite.

â€¢ [SLOW TEST:6.421 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":161,"skipped":2340,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:23:40.931: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:23:41.059: INFO: Creating ReplicaSet my-hostname-basic-bbc115cd-db9f-463f-a714-a365d1b1dfde
Sep 25 03:23:41.078: INFO: Pod name my-hostname-basic-bbc115cd-db9f-463f-a714-a365d1b1dfde: Found 0 pods out of 1
Sep 25 03:23:46.080: INFO: Pod name my-hostname-basic-bbc115cd-db9f-463f-a714-a365d1b1dfde: Found 1 pods out of 1
Sep 25 03:23:46.080: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-bbc115cd-db9f-463f-a714-a365d1b1dfde" is running
Sep 25 03:23:46.083: INFO: Pod "my-hostname-basic-bbc115cd-db9f-463f-a714-a365d1b1dfde-4fh2m" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-25 03:23:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-25 03:23:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-25 03:23:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-09-25 03:23:41 +0000 UTC Reason: Message:}])
Sep 25 03:23:46.083: INFO: Trying to dial the pod
Sep 25 03:23:51.097: INFO: Controller my-hostname-basic-bbc115cd-db9f-463f-a714-a365d1b1dfde: Got expected result from replica 1 [my-hostname-basic-bbc115cd-db9f-463f-a714-a365d1b1dfde-4fh2m]: "my-hostname-basic-bbc115cd-db9f-463f-a714-a365d1b1dfde-4fh2m", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:23:51.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8236" for this suite.

â€¢ [SLOW TEST:10.177 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":162,"skipped":2354,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:23:51.109: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 25 03:23:51.261: INFO: Waiting up to 5m0s for pod "pod-0d49e230-5622-466d-a2f7-e7137de68063" in namespace "emptydir-4784" to be "success or failure"
Sep 25 03:23:51.264: INFO: Pod "pod-0d49e230-5622-466d-a2f7-e7137de68063": Phase="Pending", Reason="", readiness=false. Elapsed: 3.65932ms
Sep 25 03:23:53.268: INFO: Pod "pod-0d49e230-5622-466d-a2f7-e7137de68063": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006980408s
STEP: Saw pod success
Sep 25 03:23:53.268: INFO: Pod "pod-0d49e230-5622-466d-a2f7-e7137de68063" satisfied condition "success or failure"
Sep 25 03:23:53.271: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-0d49e230-5622-466d-a2f7-e7137de68063 container test-container: <nil>
STEP: delete the pod
Sep 25 03:23:53.291: INFO: Waiting for pod pod-0d49e230-5622-466d-a2f7-e7137de68063 to disappear
Sep 25 03:23:53.292: INFO: Pod pod-0d49e230-5622-466d-a2f7-e7137de68063 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:23:53.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4784" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":163,"skipped":2374,"failed":0}
SSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:23:53.299: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-4897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Sep 25 03:23:53.439: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4897" to be "success or failure"
Sep 25 03:23:53.443: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968738ms
Sep 25 03:23:55.447: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008207639s
Sep 25 03:23:57.452: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012747267s
STEP: Saw pod success
Sep 25 03:23:57.452: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep 25 03:23:57.455: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep 25 03:23:57.472: INFO: Waiting for pod pod-host-path-test to disappear
Sep 25 03:23:57.478: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:23:57.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4897" for this suite.
â€¢{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":164,"skipped":2379,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:23:57.490: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6649
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6649.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6649.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6649.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6649.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6649.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6649.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 25 03:24:01.670: INFO: DNS probes using dns-6649/dns-test-b28cfc9c-d6d6-4bbf-bcb6-89e4e0ae90e8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:01.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6649" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":165,"skipped":2386,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:01.693: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 25 03:24:01.836: INFO: Waiting up to 5m0s for pod "pod-694ff780-9655-4f36-b6e3-772ba885b88f" in namespace "emptydir-2114" to be "success or failure"
Sep 25 03:24:01.841: INFO: Pod "pod-694ff780-9655-4f36-b6e3-772ba885b88f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.121497ms
Sep 25 03:24:03.844: INFO: Pod "pod-694ff780-9655-4f36-b6e3-772ba885b88f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00766673s
Sep 25 03:24:05.849: INFO: Pod "pod-694ff780-9655-4f36-b6e3-772ba885b88f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012193707s
STEP: Saw pod success
Sep 25 03:24:05.849: INFO: Pod "pod-694ff780-9655-4f36-b6e3-772ba885b88f" satisfied condition "success or failure"
Sep 25 03:24:05.852: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-694ff780-9655-4f36-b6e3-772ba885b88f container test-container: <nil>
STEP: delete the pod
Sep 25 03:24:05.869: INFO: Waiting for pod pod-694ff780-9655-4f36-b6e3-772ba885b88f to disappear
Sep 25 03:24:05.871: INFO: Pod pod-694ff780-9655-4f36-b6e3-772ba885b88f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:05.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2114" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":166,"skipped":2401,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:05.878: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7926
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-4b0aabfc-5d4a-489b-869b-94e35b65a9d9
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:12.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7926" for this suite.

â€¢ [SLOW TEST:6.183 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":167,"skipped":2402,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:12.062: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:24:12.202: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:16.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1984" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":168,"skipped":2413,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:16.248: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1035
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-76d13b57-219c-4a84-85aa-85ec53a73dc7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-76d13b57-219c-4a84-85aa-85ec53a73dc7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:20.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1035" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":169,"skipped":2425,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:20.449: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 25 03:24:25.109: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5703 pod-service-account-98670a08-8926-484c-b6f0-f827af0dae4b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 25 03:24:25.289: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5703 pod-service-account-98670a08-8926-484c-b6f0-f827af0dae4b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 25 03:24:25.483: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5703 pod-service-account-98670a08-8926-484c-b6f0-f827af0dae4b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:25.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5703" for this suite.

â€¢ [SLOW TEST:5.224 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":170,"skipped":2432,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:25.674: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1587
[It] should support rolling-update to same image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 25 03:24:25.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-7779'
Sep 25 03:24:25.907: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 25 03:24:25.907: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Sep 25 03:24:25.913: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Sep 25 03:24:25.913: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep 25 03:24:25.921: INFO: scanned /root for discovery docs: <nil>
Sep 25 03:24:25.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7779'
Sep 25 03:24:42.710: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 25 03:24:42.710: INFO: stdout: "Created e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f\nScaling up e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Sep 25 03:24:42.710: INFO: stdout: "Created e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f\nScaling up e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Sep 25 03:24:42.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-7779'
Sep 25 03:24:42.814: INFO: stderr: ""
Sep 25 03:24:42.814: INFO: stdout: "e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f-x29d8 e2e-test-httpd-rc-x4x2r "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Sep 25 03:24:47.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-7779'
Sep 25 03:24:47.926: INFO: stderr: ""
Sep 25 03:24:47.926: INFO: stdout: "e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f-x29d8 "
Sep 25 03:24:47.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f-x29d8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7779'
Sep 25 03:24:48.024: INFO: stderr: ""
Sep 25 03:24:48.024: INFO: stdout: "true"
Sep 25 03:24:48.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f-x29d8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7779'
Sep 25 03:24:48.146: INFO: stderr: ""
Sep 25 03:24:48.146: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Sep 25 03:24:48.146: INFO: e2e-test-httpd-rc-4c3f512710cd359af6e9b4237f49563f-x29d8 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1593
Sep 25 03:24:48.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete rc e2e-test-httpd-rc --namespace=kubectl-7779'
Sep 25 03:24:48.260: INFO: stderr: ""
Sep 25 03:24:48.260: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:48.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7779" for this suite.

â€¢ [SLOW TEST:22.594 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1582
    should support rolling-update to same image [Deprecated] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image [Deprecated] [Conformance]","total":280,"completed":171,"skipped":2433,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:48.268: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-80047066-bb67-48c8-8796-467578aecedc
STEP: Creating a pod to test consume secrets
Sep 25 03:24:48.408: INFO: Waiting up to 5m0s for pod "pod-secrets-62db926a-f2b1-4334-99e5-cfa1afdb8a60" in namespace "secrets-2961" to be "success or failure"
Sep 25 03:24:48.410: INFO: Pod "pod-secrets-62db926a-f2b1-4334-99e5-cfa1afdb8a60": Phase="Pending", Reason="", readiness=false. Elapsed: 1.695558ms
Sep 25 03:24:50.413: INFO: Pod "pod-secrets-62db926a-f2b1-4334-99e5-cfa1afdb8a60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004658837s
STEP: Saw pod success
Sep 25 03:24:50.413: INFO: Pod "pod-secrets-62db926a-f2b1-4334-99e5-cfa1afdb8a60" satisfied condition "success or failure"
Sep 25 03:24:50.415: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-secrets-62db926a-f2b1-4334-99e5-cfa1afdb8a60 container secret-volume-test: <nil>
STEP: delete the pod
Sep 25 03:24:50.429: INFO: Waiting for pod pod-secrets-62db926a-f2b1-4334-99e5-cfa1afdb8a60 to disappear
Sep 25 03:24:50.431: INFO: Pod pod-secrets-62db926a-f2b1-4334-99e5-cfa1afdb8a60 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:50.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2961" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":172,"skipped":2439,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:50.439: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-eea7c4a5-c695-444c-a06b-c212102a1f73
STEP: Creating a pod to test consume configMaps
Sep 25 03:24:50.587: INFO: Waiting up to 5m0s for pod "pod-configmaps-06418ddb-5fe8-4ebc-a7be-365ac08b8265" in namespace "configmap-9846" to be "success or failure"
Sep 25 03:24:50.588: INFO: Pod "pod-configmaps-06418ddb-5fe8-4ebc-a7be-365ac08b8265": Phase="Pending", Reason="", readiness=false. Elapsed: 1.806237ms
Sep 25 03:24:52.592: INFO: Pod "pod-configmaps-06418ddb-5fe8-4ebc-a7be-365ac08b8265": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005632337s
STEP: Saw pod success
Sep 25 03:24:52.592: INFO: Pod "pod-configmaps-06418ddb-5fe8-4ebc-a7be-365ac08b8265" satisfied condition "success or failure"
Sep 25 03:24:52.595: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-configmaps-06418ddb-5fe8-4ebc-a7be-365ac08b8265 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:24:52.611: INFO: Waiting for pod pod-configmaps-06418ddb-5fe8-4ebc-a7be-365ac08b8265 to disappear
Sep 25 03:24:52.614: INFO: Pod pod-configmaps-06418ddb-5fe8-4ebc-a7be-365ac08b8265 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:52.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9846" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":173,"skipped":2454,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:52.623: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4464
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-b70b0d5a-1cc7-4af8-b744-1b972e280ac8
STEP: Creating a pod to test consume secrets
Sep 25 03:24:52.766: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-49646e67-cc90-4be0-b0b5-21c6a29c3fc1" in namespace "projected-4464" to be "success or failure"
Sep 25 03:24:52.768: INFO: Pod "pod-projected-secrets-49646e67-cc90-4be0-b0b5-21c6a29c3fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.000339ms
Sep 25 03:24:54.772: INFO: Pod "pod-projected-secrets-49646e67-cc90-4be0-b0b5-21c6a29c3fc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006246744s
STEP: Saw pod success
Sep 25 03:24:54.772: INFO: Pod "pod-projected-secrets-49646e67-cc90-4be0-b0b5-21c6a29c3fc1" satisfied condition "success or failure"
Sep 25 03:24:54.775: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-projected-secrets-49646e67-cc90-4be0-b0b5-21c6a29c3fc1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 25 03:24:54.790: INFO: Waiting for pod pod-projected-secrets-49646e67-cc90-4be0-b0b5-21c6a29c3fc1 to disappear
Sep 25 03:24:54.793: INFO: Pod pod-projected-secrets-49646e67-cc90-4be0-b0b5-21c6a29c3fc1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:54.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4464" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":174,"skipped":2461,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:54.800: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7654
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a job from an image when restart is OnFailure [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 25 03:24:54.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7654'
Sep 25 03:24:55.049: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 25 03:24:55.049: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Sep 25 03:24:55.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete jobs e2e-test-httpd-job --namespace=kubectl-7654'
Sep 25 03:24:55.166: INFO: stderr: ""
Sep 25 03:24:55.166: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:24:55.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7654" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure [Deprecated] [Conformance]","total":280,"completed":175,"skipped":2467,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:24:55.175: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4194
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4194
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-4194
Sep 25 03:24:55.320: INFO: Found 0 stateful pods, waiting for 1
Sep 25 03:25:05.325: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Sep 25 03:25:05.340: INFO: Deleting all statefulset in ns statefulset-4194
Sep 25 03:25:05.343: INFO: Scaling statefulset ss to 0
Sep 25 03:25:15.356: INFO: Waiting for statefulset status.replicas updated to 0
Sep 25 03:25:15.359: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:25:15.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4194" for this suite.

â€¢ [SLOW TEST:20.203 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":176,"skipped":2546,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:25:15.379: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:25:15.507: INFO: Creating deployment "webserver-deployment"
Sep 25 03:25:15.512: INFO: Waiting for observed generation 1
Sep 25 03:25:17.520: INFO: Waiting for all required pods to come up
Sep 25 03:25:17.526: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 25 03:25:25.538: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 25 03:25:25.545: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 25 03:25:25.552: INFO: Updating deployment webserver-deployment
Sep 25 03:25:25.552: INFO: Waiting for observed generation 2
Sep 25 03:25:27.560: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 25 03:25:27.564: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 25 03:25:27.567: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 25 03:25:27.574: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 25 03:25:27.574: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 25 03:25:27.577: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 25 03:25:27.581: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 25 03:25:27.581: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 25 03:25:27.586: INFO: Updating deployment webserver-deployment
Sep 25 03:25:27.586: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 25 03:25:27.593: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 25 03:25:27.595: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Sep 25 03:25:29.606: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6460 /apis/apps/v1/namespaces/deployment-6460/deployments/webserver-deployment a97a0ec7-5162-4402-bb3c-c7402f2afc7e 428522 3 2020-09-25 03:25:15 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0028027f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-09-25 03:25:27 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-09-25 03:25:28 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 25 03:25:29.610: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-6460 /apis/apps/v1/namespaces/deployment-6460/replicasets/webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 428519 3 2020-09-25 03:25:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a97a0ec7-5162-4402-bb3c-c7402f2afc7e 0xc002803187 0xc002803188}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0028031f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 25 03:25:29.610: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 25 03:25:29.610: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-6460 /apis/apps/v1/namespaces/deployment-6460/replicasets/webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 428489 3 2020-09-25 03:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a97a0ec7-5162-4402-bb3c-c7402f2afc7e 0xc0028030c7 0xc0028030c8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002803128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 25 03:25:29.616: INFO: Pod "webserver-deployment-595b5b9587-45pg8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-45pg8 webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-45pg8 9811f03f-4db5-4891-b815-98bdd18d8673 428598 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.127/32 cni.projectcalico.org/podIPs:192.168.100.127/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc002803a77 0xc002803a78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.617: INFO: Pod "webserver-deployment-595b5b9587-6vl2t" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6vl2t webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-6vl2t 96b14341-16e1-42ce-91b1-53cb368e6317 428515 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.38/32 cni.projectcalico.org/podIPs:192.168.100.38/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc002803c57 0xc002803c58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.29,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.617: INFO: Pod "webserver-deployment-595b5b9587-7rdw5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7rdw5 webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-7rdw5 3a50271e-2bfa-4cfe-9a6e-67f7fa69615b 428185 0 2020-09-25 03:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.22/32 cni.projectcalico.org/podIPs:192.168.100.22/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc002803ed7 0xc002803ed8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.29,PodIP:192.168.100.22,StartTime:2020-09-25 03:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:25:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://1ed04221a5f7b3202bd8b5871471f6888ee3cb48afba96912f39f380389d210f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.617: INFO: Pod "webserver-deployment-595b5b9587-bcf5h" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bcf5h webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-bcf5h 17a6c29d-2157-43a3-90f0-cfc4a630c2b8 428239 0 2020-09-25 03:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.123/32 cni.projectcalico.org/podIPs:192.168.100.123/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032f4227 0xc0032f4228}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:192.168.100.123,StartTime:2020-09-25 03:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:25:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://9f0b9693f27ad2a625df17e312f840c62e5d431bb87cb6196a27e0ed25fb996f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.617: INFO: Pod "webserver-deployment-595b5b9587-bgcxc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bgcxc webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-bgcxc 59082bc4-abd9-43f4-a461-2c0f8285116d 428574 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.147/32 cni.projectcalico.org/podIPs:192.168.100.147/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032f46c7 0xc0032f46c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.618: INFO: Pod "webserver-deployment-595b5b9587-bstrv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bstrv webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-bstrv 1b632f83-049b-4eff-8b80-2154c3e73ccf 428177 0 2020-09-25 03:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.140/32 cni.projectcalico.org/podIPs:192.168.100.140/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032f4c77 0xc0032f4c78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:192.168.100.140,StartTime:2020-09-25 03:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:25:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://05976849ca06ccc404a1cfe2e03f99625c699c5224ae2a068a7ff119af9c9a72,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.618: INFO: Pod "webserver-deployment-595b5b9587-c4rk7" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-c4rk7 webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-c4rk7 9e15f557-1c4a-47b2-a55b-49aee4b00d94 428226 0 2020-09-25 03:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.142/32 cni.projectcalico.org/podIPs:192.168.100.142/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032f4de7 0xc0032f4de8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:192.168.100.142,StartTime:2020-09-25 03:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:25:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ada2c289e95e8321a4b5cb0d63f0d417830b41fbff949afafe4e2792ff04a1b1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.618: INFO: Pod "webserver-deployment-595b5b9587-dw7pv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dw7pv webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-dw7pv fb463989-7aba-4dce-b863-2c66e0029d94 428570 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.40/32 cni.projectcalico.org/podIPs:192.168.100.40/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032f51e7 0xc0032f51e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.29,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.619: INFO: Pod "webserver-deployment-595b5b9587-g95dt" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-g95dt webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-g95dt aaae7f00-a39f-403d-ab43-e0d7dab688f7 428189 0 2020-09-25 03:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.91/32 cni.projectcalico.org/podIPs:192.168.100.91/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032f5607 0xc0032f5608}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:192.168.100.91,StartTime:2020-09-25 03:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:25:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a380b851aeff75c316391aacfe96e07c17062ee94e03a8e0a3a4eff300c0d79c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.619: INFO: Pod "webserver-deployment-595b5b9587-jqpzt" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jqpzt webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-jqpzt c3b5db54-e7ac-4689-89c7-5a1a9a94a410 428262 0 2020-09-25 03:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.23/32 cni.projectcalico.org/podIPs:192.168.100.23/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032f59b7 0xc0032f59b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.29,PodIP:192.168.100.23,StartTime:2020-09-25 03:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:25:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7413b94bef40fa8a4ab3eeb4e99d8acb5614dd2195ea8adf13f08cb2c0cb1bdf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.619: INFO: Pod "webserver-deployment-595b5b9587-lzrqx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lzrqx webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-lzrqx f8a8a8b2-a993-4093-a07a-9f036af79500 428593 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.65/32 cni.projectcalico.org/podIPs:192.168.100.65/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032f5d27 0xc0032f5d28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.619: INFO: Pod "webserver-deployment-595b5b9587-m6cp8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-m6cp8 webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-m6cp8 0ef455df-df0a-46e3-a182-120130db32c6 428577 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.125/32 cni.projectcalico.org/podIPs:192.168.100.125/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032fc467 0xc0032fc468}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.620: INFO: Pod "webserver-deployment-595b5b9587-mz7vb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mz7vb webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-mz7vb a6f5aa66-b854-4c83-8448-a0eb77d406d9 428233 0 2020-09-25 03:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.20/32 cni.projectcalico.org/podIPs:192.168.100.20/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032fc807 0xc0032fc808}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.29,PodIP:192.168.100.20,StartTime:2020-09-25 03:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:25:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://a06835fb783243436f9c6a47c543eb8ada405e0f2ab78099a463cfbabeecd913,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.620: INFO: Pod "webserver-deployment-595b5b9587-nv88z" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nv88z webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-nv88z ec7c846e-56de-490b-bd0d-8280f4277f4a 428514 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.110/32 cni.projectcalico.org/podIPs:192.168.100.110/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032fc977 0xc0032fc978}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.620: INFO: Pod "webserver-deployment-595b5b9587-p5qlx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p5qlx webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-p5qlx b28d76c1-5de4-486f-b8d6-95f0ec70c8ed 428492 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.113/32 cni.projectcalico.org/podIPs:192.168.100.113/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032fcaf7 0xc0032fcaf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.620: INFO: Pod "webserver-deployment-595b5b9587-qbsf8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qbsf8 webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-qbsf8 7567ac19-1101-4cd8-8550-ef75e6980d47 428499 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.143/32 cni.projectcalico.org/podIPs:192.168.100.143/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032fcc57 0xc0032fcc58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.621: INFO: Pod "webserver-deployment-595b5b9587-rhp5f" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rhp5f webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-rhp5f 79ebb6ff-262f-4ef8-a988-ce27e9086e5c 428551 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.145/32 cni.projectcalico.org/podIPs:192.168.100.145/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032fcdb7 0xc0032fcdb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.621: INFO: Pod "webserver-deployment-595b5b9587-rq4fg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rq4fg webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-rq4fg 5e504532-4ffd-422d-b289-21d2b086c2f9 428579 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.42/32 cni.projectcalico.org/podIPs:192.168.100.42/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032fcf07 0xc0032fcf08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.29,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.621: INFO: Pod "webserver-deployment-595b5b9587-wqk7x" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wqk7x webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-wqk7x 3c933dde-f23b-44e3-b4fe-c7d979fb3af8 428586 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.115/32 cni.projectcalico.org/podIPs:192.168.100.115/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032fd057 0xc0032fd058}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.621: INFO: Pod "webserver-deployment-595b5b9587-zz8nj" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zz8nj webserver-deployment-595b5b9587- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-595b5b9587-zz8nj 5fa75be3-c7f4-450a-bdca-0279b0ccb7a4 428255 0 2020-09-25 03:25:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:192.168.100.139/32 cni.projectcalico.org/podIPs:192.168.100.139/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 f7258281-c109-4cf2-989e-d6b2f1f92626 0xc0032fd1d7 0xc0032fd1d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:192.168.100.139,StartTime:2020-09-25 03:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:25:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://7131d536f767f851d0e8993ea4bca293161b68fb74b2fcf44bd92ef932350418,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.621: INFO: Pod "webserver-deployment-c7997dcc8-2s6sc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2s6sc webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-2s6sc 9d799f1d-1aac-4c2f-9b57-b2fa561ea21f 428568 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.112/32 cni.projectcalico.org/podIPs:192.168.100.112/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc0032fd347 0xc0032fd348}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.621: INFO: Pod "webserver-deployment-c7997dcc8-46mzj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-46mzj webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-46mzj 0d600fbd-1ffc-4f84-9d4b-18ba2a14914b 428512 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.151/32 cni.projectcalico.org/podIPs:192.168.100.151/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc0032fd4b7 0xc0032fd4b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.622: INFO: Pod "webserver-deployment-c7997dcc8-4rdt6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4rdt6 webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-4rdt6 10c589af-f7cd-4764-86e3-a5f2611ca50e 428355 0 2020-09-25 03:25:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.138/32 cni.projectcalico.org/podIPs:192.168.100.138/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc0032fd627 0xc0032fd628}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:,StartTime:2020-09-25 03:25:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.622: INFO: Pod "webserver-deployment-c7997dcc8-6kb4k" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6kb4k webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-6kb4k 8f383aa3-7b6e-4275-9a4c-7989327763cb 428487 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.137/32 cni.projectcalico.org/podIPs:192.168.100.137/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc0032fd797 0xc0032fd798}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.622: INFO: Pod "webserver-deployment-c7997dcc8-6ww96" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6ww96 webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-6ww96 eaf7f58b-502c-47a4-ac7c-70a7ce23871d 428359 0 2020-09-25 03:25:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.124/32 cni.projectcalico.org/podIPs:192.168.100.124/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc0032fd907 0xc0032fd908}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.622: INFO: Pod "webserver-deployment-c7997dcc8-7grnt" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7grnt webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-7grnt 1fdbdb7c-0098-4817-b043-7ff8d58eb26b 428545 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.109/32 cni.projectcalico.org/podIPs:192.168.100.109/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc0032fda77 0xc0032fda78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.622: INFO: Pod "webserver-deployment-c7997dcc8-bl5h8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bl5h8 webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-bl5h8 220b736c-4a3e-4ba9-9444-836d53ce5b15 428550 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.39/32 cni.projectcalico.org/podIPs:192.168.100.39/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc0032fdbe7 0xc0032fdbe8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.29,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.622: INFO: Pod "webserver-deployment-c7997dcc8-d9dbj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-d9dbj webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-d9dbj f9bf39e4-175e-4f04-9eca-e25783023d59 428524 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.41/32 cni.projectcalico.org/podIPs:192.168.100.41/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc0032fdd57 0xc0032fdd58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.29,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.623: INFO: Pod "webserver-deployment-c7997dcc8-gspm6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gspm6 webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-gspm6 c7616925-f24e-4156-b58b-1dacf1c200a0 428532 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.116/32 cni.projectcalico.org/podIPs:192.168.100.116/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc0032fdec7 0xc0032fdec8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.623: INFO: Pod "webserver-deployment-c7997dcc8-jr4xq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jr4xq webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-jr4xq 9a38c8cf-1051-4e32-a571-5de491c1db06 428350 0 2020-09-25 03:25:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.146/32 cni.projectcalico.org/podIPs:192.168.100.146/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc003388037 0xc003388038}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.27,PodIP:,StartTime:2020-09-25 03:25:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.623: INFO: Pod "webserver-deployment-c7997dcc8-kdkxj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-kdkxj webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-kdkxj be2b3756-5bac-4347-8f55-8ac2844b3220 428490 0 2020-09-25 03:25:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.28/32 cni.projectcalico.org/podIPs:192.168.100.28/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc0033881a7 0xc0033881a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.29,PodIP:,StartTime:2020-09-25 03:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.623: INFO: Pod "webserver-deployment-c7997dcc8-kt88t" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-kt88t webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-kt88t ec76f886-36d3-4749-aa8a-443d774e43da 428354 0 2020-09-25 03:25:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.27/32 cni.projectcalico.org/podIPs:192.168.100.27/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc003388317 0xc003388318}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.29,PodIP:,StartTime:2020-09-25 03:25:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 25 03:25:29.623: INFO: Pod "webserver-deployment-c7997dcc8-n6d5f" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-n6d5f webserver-deployment-c7997dcc8- deployment-6460 /api/v1/namespaces/deployment-6460/pods/webserver-deployment-c7997dcc8-n6d5f eb07f3f1-2d72-4070-bf2a-da5f0f1ac5f8 428351 0 2020-09-25 03:25:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:192.168.100.111/32 cni.projectcalico.org/podIPs:192.168.100.111/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 db5f6ca0-0781-4f81-8180-58a8133766bb 0xc003388487 0xc003388488}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c45mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c45mm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c45mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:25:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:,StartTime:2020-09-25 03:25:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:25:29.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6460" for this suite.

â€¢ [SLOW TEST:14.254 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":177,"skipped":2585,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:25:29.634: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:25:29.779: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a" in namespace "projected-7431" to be "success or failure"
Sep 25 03:25:29.783: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.710564ms
Sep 25 03:25:31.787: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007920793s
Sep 25 03:25:33.792: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01287758s
Sep 25 03:25:35.802: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022721984s
Sep 25 03:25:37.806: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027296037s
Sep 25 03:25:39.811: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031635488s
Sep 25 03:25:41.815: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035674031s
Sep 25 03:25:43.819: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.039825496s
Sep 25 03:25:45.823: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.043912234s
Sep 25 03:25:47.830: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.051084667s
Sep 25 03:25:49.835: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.055632715s
Sep 25 03:25:51.839: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.059986372s
Sep 25 03:25:53.843: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.06406581s
Sep 25 03:25:55.847: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.068340779s
Sep 25 03:25:57.851: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.071933781s
Sep 25 03:25:59.855: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.076366906s
STEP: Saw pod success
Sep 25 03:25:59.855: INFO: Pod "downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a" satisfied condition "success or failure"
Sep 25 03:25:59.859: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a container client-container: <nil>
STEP: delete the pod
Sep 25 03:25:59.876: INFO: Waiting for pod downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a to disappear
Sep 25 03:25:59.878: INFO: Pod downwardapi-volume-3dc45e56-9c36-4789-9d2b-ea61f05d0a5a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:25:59.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7431" for this suite.

â€¢ [SLOW TEST:30.253 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":178,"skipped":2600,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:25:59.886: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:26:00.340: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:26:02.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601160, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601160, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601160, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601160, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:26:05.363: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:26:05.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4252" for this suite.
STEP: Destroying namespace "webhook-4252-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.552 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":179,"skipped":2600,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:26:05.439: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-193
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-04aa1888-dbb3-43c8-a34e-8bb8e74bafce
STEP: Creating a pod to test consume secrets
Sep 25 03:26:05.579: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-74cf2e2a-0d99-4598-aa56-1c1e7b91563e" in namespace "projected-193" to be "success or failure"
Sep 25 03:26:05.584: INFO: Pod "pod-projected-secrets-74cf2e2a-0d99-4598-aa56-1c1e7b91563e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.549969ms
Sep 25 03:26:07.588: INFO: Pod "pod-projected-secrets-74cf2e2a-0d99-4598-aa56-1c1e7b91563e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009078873s
Sep 25 03:26:09.592: INFO: Pod "pod-projected-secrets-74cf2e2a-0d99-4598-aa56-1c1e7b91563e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012864385s
STEP: Saw pod success
Sep 25 03:26:09.592: INFO: Pod "pod-projected-secrets-74cf2e2a-0d99-4598-aa56-1c1e7b91563e" satisfied condition "success or failure"
Sep 25 03:26:09.594: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-projected-secrets-74cf2e2a-0d99-4598-aa56-1c1e7b91563e container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 25 03:26:09.608: INFO: Waiting for pod pod-projected-secrets-74cf2e2a-0d99-4598-aa56-1c1e7b91563e to disappear
Sep 25 03:26:09.611: INFO: Pod pod-projected-secrets-74cf2e2a-0d99-4598-aa56-1c1e7b91563e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:26:09.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-193" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":180,"skipped":2607,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:26:09.617: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep 25 03:26:40.277: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:26:40.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1340" for this suite.

â€¢ [SLOW TEST:30.668 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":181,"skipped":2616,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:26:40.285: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7570
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 25 03:26:44.467: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 25 03:26:44.470: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 25 03:26:46.470: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 25 03:26:46.475: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 25 03:26:48.470: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 25 03:26:48.474: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 25 03:26:50.470: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 25 03:26:50.475: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 25 03:26:52.470: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 25 03:26:52.475: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 25 03:26:54.470: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 25 03:26:54.474: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:26:54.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7570" for this suite.

â€¢ [SLOW TEST:14.198 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":182,"skipped":2632,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:26:54.484: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9778
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 25 03:26:54.626: INFO: Waiting up to 5m0s for pod "pod-03bdc312-3707-4f0e-9151-9bb713770070" in namespace "emptydir-9778" to be "success or failure"
Sep 25 03:26:54.629: INFO: Pod "pod-03bdc312-3707-4f0e-9151-9bb713770070": Phase="Pending", Reason="", readiness=false. Elapsed: 3.127986ms
Sep 25 03:26:56.632: INFO: Pod "pod-03bdc312-3707-4f0e-9151-9bb713770070": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006791427s
Sep 25 03:26:58.638: INFO: Pod "pod-03bdc312-3707-4f0e-9151-9bb713770070": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012874999s
STEP: Saw pod success
Sep 25 03:26:58.639: INFO: Pod "pod-03bdc312-3707-4f0e-9151-9bb713770070" satisfied condition "success or failure"
Sep 25 03:26:58.642: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-03bdc312-3707-4f0e-9151-9bb713770070 container test-container: <nil>
STEP: delete the pod
Sep 25 03:26:58.661: INFO: Waiting for pod pod-03bdc312-3707-4f0e-9151-9bb713770070 to disappear
Sep 25 03:26:58.667: INFO: Pod pod-03bdc312-3707-4f0e-9151-9bb713770070 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:26:58.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9778" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":2647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:26:58.675: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2033
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-fd4b4801-7b91-4c6b-b993-729b7874b34d
STEP: Creating a pod to test consume configMaps
Sep 25 03:26:58.822: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-72e362d7-48fe-4ef1-b93c-fb8d0c5ba457" in namespace "projected-2033" to be "success or failure"
Sep 25 03:26:58.825: INFO: Pod "pod-projected-configmaps-72e362d7-48fe-4ef1-b93c-fb8d0c5ba457": Phase="Pending", Reason="", readiness=false. Elapsed: 3.060952ms
Sep 25 03:27:00.830: INFO: Pod "pod-projected-configmaps-72e362d7-48fe-4ef1-b93c-fb8d0c5ba457": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007939197s
Sep 25 03:27:02.834: INFO: Pod "pod-projected-configmaps-72e362d7-48fe-4ef1-b93c-fb8d0c5ba457": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0122509s
STEP: Saw pod success
Sep 25 03:27:02.834: INFO: Pod "pod-projected-configmaps-72e362d7-48fe-4ef1-b93c-fb8d0c5ba457" satisfied condition "success or failure"
Sep 25 03:27:02.838: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-projected-configmaps-72e362d7-48fe-4ef1-b93c-fb8d0c5ba457 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:27:02.856: INFO: Waiting for pod pod-projected-configmaps-72e362d7-48fe-4ef1-b93c-fb8d0c5ba457 to disappear
Sep 25 03:27:02.858: INFO: Pod pod-projected-configmaps-72e362d7-48fe-4ef1-b93c-fb8d0c5ba457 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:27:02.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2033" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":184,"skipped":2669,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:27:02.866: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:27:03.014: INFO: Waiting up to 5m0s for pod "downwardapi-volume-231538f2-e09f-4860-b36c-11922151733f" in namespace "downward-api-1456" to be "success or failure"
Sep 25 03:27:03.017: INFO: Pod "downwardapi-volume-231538f2-e09f-4860-b36c-11922151733f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.029773ms
Sep 25 03:27:05.021: INFO: Pod "downwardapi-volume-231538f2-e09f-4860-b36c-11922151733f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006922709s
Sep 25 03:27:07.024: INFO: Pod "downwardapi-volume-231538f2-e09f-4860-b36c-11922151733f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010507589s
STEP: Saw pod success
Sep 25 03:27:07.024: INFO: Pod "downwardapi-volume-231538f2-e09f-4860-b36c-11922151733f" satisfied condition "success or failure"
Sep 25 03:27:07.027: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-231538f2-e09f-4860-b36c-11922151733f container client-container: <nil>
STEP: delete the pod
Sep 25 03:27:07.045: INFO: Waiting for pod downwardapi-volume-231538f2-e09f-4860-b36c-11922151733f to disappear
Sep 25 03:27:07.047: INFO: Pod downwardapi-volume-231538f2-e09f-4860-b36c-11922151733f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:27:07.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1456" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":2686,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:27:07.054: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:27:07.191: INFO: Waiting up to 5m0s for pod "downwardapi-volume-641c7700-758d-4d1c-844f-bb0819a909dd" in namespace "downward-api-6875" to be "success or failure"
Sep 25 03:27:07.194: INFO: Pod "downwardapi-volume-641c7700-758d-4d1c-844f-bb0819a909dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.390992ms
Sep 25 03:27:09.196: INFO: Pod "downwardapi-volume-641c7700-758d-4d1c-844f-bb0819a909dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0050324s
STEP: Saw pod success
Sep 25 03:27:09.196: INFO: Pod "downwardapi-volume-641c7700-758d-4d1c-844f-bb0819a909dd" satisfied condition "success or failure"
Sep 25 03:27:09.198: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-641c7700-758d-4d1c-844f-bb0819a909dd container client-container: <nil>
STEP: delete the pod
Sep 25 03:27:09.208: INFO: Waiting for pod downwardapi-volume-641c7700-758d-4d1c-844f-bb0819a909dd to disappear
Sep 25 03:27:09.215: INFO: Pod downwardapi-volume-641c7700-758d-4d1c-844f-bb0819a909dd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:27:09.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6875" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":186,"skipped":2686,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:27:09.223: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6425
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-016d5676-764f-428d-97df-c7dacf4a21de
STEP: Creating configMap with name cm-test-opt-upd-5c76d169-fa2c-494a-a067-99a0f63c301e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-016d5676-764f-428d-97df-c7dacf4a21de
STEP: Updating configmap cm-test-opt-upd-5c76d169-fa2c-494a-a067-99a0f63c301e
STEP: Creating configMap with name cm-test-opt-create-22841496-4504-4508-b8db-34b9cc67bcce
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:27:17.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6425" for this suite.

â€¢ [SLOW TEST:8.222 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":187,"skipped":2689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:27:17.447: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:27:17.904: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:27:19.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601237, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601237, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601237, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601237, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:27:22.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:27:22.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8021" for this suite.
STEP: Destroying namespace "webhook-8021-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.572 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":188,"skipped":2712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:27:23.020: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:27:23.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-6327'
Sep 25 03:27:23.487: INFO: stderr: ""
Sep 25 03:27:23.487: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Sep 25 03:27:23.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-6327'
Sep 25 03:27:23.696: INFO: stderr: ""
Sep 25 03:27:23.696: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep 25 03:27:24.700: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 03:27:24.701: INFO: Found 0 / 1
Sep 25 03:27:25.701: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 03:27:25.701: INFO: Found 0 / 1
Sep 25 03:27:26.700: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 03:27:26.700: INFO: Found 1 / 1
Sep 25 03:27:26.700: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 25 03:27:26.702: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 03:27:26.702: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 25 03:27:26.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 describe pod agnhost-master-9dhm2 --namespace=kubectl-6327'
Sep 25 03:27:26.811: INFO: stderr: ""
Sep 25 03:27:26.811: INFO: stdout: "Name:         agnhost-master-9dhm2\nNamespace:    kubectl-6327\nPriority:     0\nNode:         biz-k8s-node-3/192.168.0.31\nStart Time:   Fri, 25 Sep 2020 03:27:23 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 192.168.100.87/32\n              cni.projectcalico.org/podIPs: 192.168.100.87/32\nStatus:       Running\nIP:           192.168.100.87\nIPs:\n  IP:           192.168.100.87\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   containerd://6449c9ed0f74bc7a6587d223976a4211562b83e0a2008825e086c1e0678f358d\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 25 Sep 2020 03:27:24 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tw44g (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tw44g:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tw44g\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                     Message\n  ----    ------     ----  ----                     -------\n  Normal  Scheduled  3s    default-scheduler        Successfully assigned kubectl-6327/agnhost-master-9dhm2 to biz-k8s-node-3\n  Normal  Pulling    2s    kubelet, biz-k8s-node-3  Pulling image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\"\n  Normal  Pulled     2s    kubelet, biz-k8s-node-3  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\"\n  Normal  Created    2s    kubelet, biz-k8s-node-3  Created container agnhost-master\n  Normal  Started    2s    kubelet, biz-k8s-node-3  Started container agnhost-master\n"
Sep 25 03:27:26.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 describe rc agnhost-master --namespace=kubectl-6327'
Sep 25 03:27:26.927: INFO: stderr: ""
Sep 25 03:27:26.927: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-6327\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-9dhm2\n"
Sep 25 03:27:26.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 describe service agnhost-master --namespace=kubectl-6327'
Sep 25 03:27:27.049: INFO: stderr: ""
Sep 25 03:27:27.049: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-6327\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.0.24.140\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.100.87:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 25 03:27:27.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 describe node biz-k8s-master'
Sep 25 03:27:27.215: INFO: stderr: ""
Sep 25 03:27:27.215: INFO: stdout: "Name:               biz-k8s-master\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    konvoy.mesosphere.com/inventory_hostname=192.168.0.4\n                    konvoy.mesosphere.com/node_pool=konvoy.mesosphere.com_control-plane\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=biz-k8s-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.0.4/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.100.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 24 Sep 2020 03:17:21 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  biz-k8s-master\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 25 Sep 2020 03:27:18 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 24 Sep 2020 03:18:32 +0000   Thu, 24 Sep 2020 03:18:32 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 25 Sep 2020 03:26:25 +0000   Thu, 24 Sep 2020 03:17:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 25 Sep 2020 03:26:25 +0000   Thu, 24 Sep 2020 03:17:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 25 Sep 2020 03:26:25 +0000   Thu, 24 Sep 2020 03:17:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 25 Sep 2020 03:26:25 +0000   Thu, 24 Sep 2020 03:18:33 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.0.4\n  Hostname:    biz-k8s-master\nCapacity:\n  cpu:                4\n  ephemeral-storage:  82231712Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8009200Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  75784745654\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7906800Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 88565045c80d4d6bbc11d28f93b696e9\n  System UUID:                30A94D56-B9F0-1BAB-F88D-56A95508738D\n  Boot ID:                    9b347ab3-cf47-49db-a5c9-af5d8953f455\n  Kernel Version:             3.10.0-957.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.7\n  Kubelet Version:            v1.17.11\n  Kube-Proxy Version:         v1.17.11\nPodCIDR:                      192.168.100.0/24\nPodCIDRs:                     192.168.100.0/24\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  konvoy                      auto-provisioning-cm-547bcf54c-6lc2x                       500m (12%)    500m (12%)  128Mi (1%)       256Mi (3%)     24h\n  konvoy                      auto-provisioning-tfcb-567d8c9755-lrg4n                    100m (2%)     100m (2%)   20Mi (0%)        30Mi (0%)      24h\n  konvoy                      auto-provisioning-webhook-76bc77784c-t9cxn                 100m (2%)     100m (2%)   128Mi (1%)       256Mi (3%)     24h\n  kube-system                 calico-node-sthxr                                          300m (7%)     0 (0%)      32M (0%)         0 (0%)         24h\n  kube-system                 etcd-biz-k8s-master                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 keepalived-biz-k8s-master                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 kube-apiserver-biz-k8s-master                              250m (6%)     0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 kube-controller-manager-biz-k8s-master                     200m (5%)     0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 kube-proxy-hwncr                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 kube-scheduler-biz-k8s-master                              100m (2%)     0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 local-volume-provisioner-kdz9j                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  kubeaddons                  metallb-kubeaddons-speaker-6dct7                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-f9bkt    0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                1550m (38%)    700m (17%)\n  memory             313874Ki (3%)  542Mi (7%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
Sep 25 03:27:27.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 describe namespace kubectl-6327'
Sep 25 03:27:27.326: INFO: stderr: ""
Sep 25 03:27:27.326: INFO: stdout: "Name:         kubectl-6327\nLabels:       e2e-framework=kubectl\n              e2e-run=9279dc4d-74b6-48d5-b084-db66b9fd2bf4\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:27:27.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6327" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":189,"skipped":2771,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:27:27.335: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-232e2110-cdfe-4ab5-bbf7-5f70ac3cfd6d in namespace container-probe-8735
Sep 25 03:27:31.481: INFO: Started pod liveness-232e2110-cdfe-4ab5-bbf7-5f70ac3cfd6d in namespace container-probe-8735
STEP: checking the pod's current state and verifying that restartCount is present
Sep 25 03:27:31.484: INFO: Initial restart count of pod liveness-232e2110-cdfe-4ab5-bbf7-5f70ac3cfd6d is 0
Sep 25 03:27:45.516: INFO: Restart count of pod container-probe-8735/liveness-232e2110-cdfe-4ab5-bbf7-5f70ac3cfd6d is now 1 (14.032679008s elapsed)
Sep 25 03:28:05.557: INFO: Restart count of pod container-probe-8735/liveness-232e2110-cdfe-4ab5-bbf7-5f70ac3cfd6d is now 2 (34.07286547s elapsed)
Sep 25 03:28:27.601: INFO: Restart count of pod container-probe-8735/liveness-232e2110-cdfe-4ab5-bbf7-5f70ac3cfd6d is now 3 (56.117273459s elapsed)
Sep 25 03:28:45.639: INFO: Restart count of pod container-probe-8735/liveness-232e2110-cdfe-4ab5-bbf7-5f70ac3cfd6d is now 4 (1m14.154825516s elapsed)
Sep 25 03:29:59.806: INFO: Restart count of pod container-probe-8735/liveness-232e2110-cdfe-4ab5-bbf7-5f70ac3cfd6d is now 5 (2m28.322304723s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:29:59.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8735" for this suite.

â€¢ [SLOW TEST:152.489 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":190,"skipped":2832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:29:59.824: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7935
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:30:06.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7935" for this suite.

â€¢ [SLOW TEST:7.178 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":191,"skipped":2861,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:30:07.003: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-9dbf742f-0eab-4c0a-a270-311e2d5ded82
STEP: Creating a pod to test consume configMaps
Sep 25 03:30:07.150: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a468041-fe52-48d4-a6d9-3705c20dbc0a" in namespace "configmap-8446" to be "success or failure"
Sep 25 03:30:07.153: INFO: Pod "pod-configmaps-0a468041-fe52-48d4-a6d9-3705c20dbc0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169837ms
Sep 25 03:30:09.157: INFO: Pod "pod-configmaps-0a468041-fe52-48d4-a6d9-3705c20dbc0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006729082s
Sep 25 03:30:11.162: INFO: Pod "pod-configmaps-0a468041-fe52-48d4-a6d9-3705c20dbc0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011090976s
STEP: Saw pod success
Sep 25 03:30:11.162: INFO: Pod "pod-configmaps-0a468041-fe52-48d4-a6d9-3705c20dbc0a" satisfied condition "success or failure"
Sep 25 03:30:11.165: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-configmaps-0a468041-fe52-48d4-a6d9-3705c20dbc0a container configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:30:11.191: INFO: Waiting for pod pod-configmaps-0a468041-fe52-48d4-a6d9-3705c20dbc0a to disappear
Sep 25 03:30:11.193: INFO: Pod pod-configmaps-0a468041-fe52-48d4-a6d9-3705c20dbc0a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:30:11.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8446" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":192,"skipped":2862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:30:11.202: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 25 03:30:15.880: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d06a0bed-aa9f-483b-bc1a-4116861cf34b"
Sep 25 03:30:15.880: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d06a0bed-aa9f-483b-bc1a-4116861cf34b" in namespace "pods-4555" to be "terminated due to deadline exceeded"
Sep 25 03:30:15.883: INFO: Pod "pod-update-activedeadlineseconds-d06a0bed-aa9f-483b-bc1a-4116861cf34b": Phase="Running", Reason="", readiness=true. Elapsed: 3.521912ms
Sep 25 03:30:17.888: INFO: Pod "pod-update-activedeadlineseconds-d06a0bed-aa9f-483b-bc1a-4116861cf34b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.008225406s
Sep 25 03:30:17.888: INFO: Pod "pod-update-activedeadlineseconds-d06a0bed-aa9f-483b-bc1a-4116861cf34b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:30:17.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4555" for this suite.

â€¢ [SLOW TEST:6.697 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":193,"skipped":2903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:30:17.900: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-29a53986-0b12-459d-8d11-34563d088192
STEP: Creating a pod to test consume secrets
Sep 25 03:30:18.049: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-708c57cb-c1ea-409b-a78e-079a176feb57" in namespace "projected-7698" to be "success or failure"
Sep 25 03:30:18.051: INFO: Pod "pod-projected-secrets-708c57cb-c1ea-409b-a78e-079a176feb57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.467063ms
Sep 25 03:30:20.062: INFO: Pod "pod-projected-secrets-708c57cb-c1ea-409b-a78e-079a176feb57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01343298s
STEP: Saw pod success
Sep 25 03:30:20.062: INFO: Pod "pod-projected-secrets-708c57cb-c1ea-409b-a78e-079a176feb57" satisfied condition "success or failure"
Sep 25 03:30:20.065: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-projected-secrets-708c57cb-c1ea-409b-a78e-079a176feb57 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 25 03:30:20.082: INFO: Waiting for pod pod-projected-secrets-708c57cb-c1ea-409b-a78e-079a176feb57 to disappear
Sep 25 03:30:20.083: INFO: Pod pod-projected-secrets-708c57cb-c1ea-409b-a78e-079a176feb57 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:30:20.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7698" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":2932,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:30:20.089: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Sep 25 03:30:20.218: INFO: PodSpec: initContainers in spec.initContainers
Sep 25 03:31:13.581: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f9da4979-4ea3-4654-a6f8-244717b3fdb5", GenerateName:"", Namespace:"init-container-7871", SelfLink:"/api/v1/namespaces/init-container-7871/pods/pod-init-f9da4979-4ea3-4654-a6f8-244717b3fdb5", UID:"f5782fb9-b332-4b85-92f7-15eaefb196ce", ResourceVersion:"431456", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63736601420, loc:(*time.Location)(0x7931640)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"218962176"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.100.96/32", "cni.projectcalico.org/podIPs":"192.168.100.96/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-pb7cq", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002e1aec0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pb7cq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pb7cq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-pb7cq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001b97030), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"biz-k8s-node-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0023626c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001b970d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001b970f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001b970f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001b970fc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601420, loc:(*time.Location)(0x7931640)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601420, loc:(*time.Location)(0x7931640)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601420, loc:(*time.Location)(0x7931640)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601420, loc:(*time.Location)(0x7931640)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.31", PodIP:"192.168.100.96", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.100.96"}}, StartTime:(*v1.Time)(0xc001b78b60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002f2c380)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002f2c3f0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://52853927cdf735749f3db77a1ce5aa2c41ba7e2c664ca7db30a6fd0231d43d8b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b78c00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b78ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc001b9718f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:31:13.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7871" for this suite.

â€¢ [SLOW TEST:53.502 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":195,"skipped":2953,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:31:13.592: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:31:13.733: INFO: (0) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 10.241917ms)
Sep 25 03:31:13.738: INFO: (1) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.799766ms)
Sep 25 03:31:13.742: INFO: (2) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.18566ms)
Sep 25 03:31:13.747: INFO: (3) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.549264ms)
Sep 25 03:31:13.751: INFO: (4) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.278325ms)
Sep 25 03:31:13.756: INFO: (5) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 5.006472ms)
Sep 25 03:31:13.761: INFO: (6) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.433561ms)
Sep 25 03:31:13.765: INFO: (7) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.814681ms)
Sep 25 03:31:13.768: INFO: (8) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.606967ms)
Sep 25 03:31:13.773: INFO: (9) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.309782ms)
Sep 25 03:31:13.777: INFO: (10) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.254026ms)
Sep 25 03:31:13.781: INFO: (11) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.503292ms)
Sep 25 03:31:13.785: INFO: (12) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.52106ms)
Sep 25 03:31:13.789: INFO: (13) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.735955ms)
Sep 25 03:31:13.792: INFO: (14) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.405783ms)
Sep 25 03:31:13.796: INFO: (15) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.656687ms)
Sep 25 03:31:13.799: INFO: (16) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.296224ms)
Sep 25 03:31:13.803: INFO: (17) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.973958ms)
Sep 25 03:31:13.807: INFO: (18) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.423194ms)
Sep 25 03:31:13.811: INFO: (19) /api/v1/nodes/biz-k8s-node-3/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.822002ms)
[AfterEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:31:13.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9454" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":196,"skipped":2966,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:31:13.817: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Sep 25 03:31:14.002: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 25 03:31:14.017: INFO: Waiting for terminating namespaces to be deleted...
Sep 25 03:31:14.026: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-1 before test
Sep 25 03:31:14.056: INFO: common-api-notify-5595dbd9b5-rb8vd from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container common-api-notify ready: true, restart count 0
Sep 25 03:31:14.056: INFO: traefik-kubeaddons-68c579bbbd-2qmn4 from kubeaddons started at 2020-09-24 03:26:59 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Sep 25 03:31:14.056: INFO: store-api-appmanage-6b8b464c4c-m528t from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container store-api-appmanage ready: true, restart count 0
Sep 25 03:31:14.056: INFO: kube-proxy-ff4h2 from kube-system started at 2020-09-24 03:18:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:31:14.056: INFO: dex-kubeaddons-58884f456-xtvhz from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container main ready: true, restart count 0
Sep 25 03:31:14.056: INFO: kubernetes-dashboard-95959d56b-wmxhc from kubeaddons started at 2020-09-24 03:25:22 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Sep 25 03:31:14.056: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 25 03:31:14.056: INFO: minio-3 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:31:14.056: INFO: local-volume-provisioner-rmgc9 from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:31:14.056: INFO: builder-daemon-main-fcb6b989c-hfwnc from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container builder-daemon-main ready: true, restart count 0
Sep 25 03:31:14.056: INFO: portal-cdn-download-5cd45dfd87-fqvrl from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container portal-cdn-download ready: false, restart count 19
Sep 25 03:31:14.056: INFO: sonobuoy from sonobuoy started at 2020-09-25 02:35:47 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 25 03:31:14.056: INFO: calico-node-k6ztw from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:31:14.056: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:31:14.056: INFO: calico-kube-controllers-77c79f7594-5qtpj from kube-system started at 2020-09-24 03:18:35 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 25 03:31:14.056: INFO: traefik-forward-auth-kubeaddons-69bbb98fb6-fdn8n from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container traefik-forward-auth ready: true, restart count 1
Sep 25 03:31:14.056: INFO: tiller-deploy-6cdf7f9d6f-5mjft from kube-system started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container tiller ready: true, restart count 0
Sep 25 03:31:14.056: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-rw4pq from cert-manager started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container cert-manager ready: true, restart count 0
Sep 25 03:31:14.056: INFO: common-api-account-6b7c79d795-d846m from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container common-api-account ready: true, restart count 0
Sep 25 03:31:14.056: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-q9x6d from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
Sep 25 03:31:14.056: INFO: store-api-mypage-5d6d59c499-dvczz from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container store-api-mypage ready: true, restart count 0
Sep 25 03:31:14.056: INFO: dex-k8s-authenticator-kubeaddons-74966666f5-gvf92 from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container dex-k8s-authenticator ready: true, restart count 1
Sep 25 03:31:14.056: INFO: minio-0 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:31:14.056: INFO: daemon-notify-74b7d7c4b5-zklwv from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container daemon-notify ready: true, restart count 0
Sep 25 03:31:14.056: INFO: builder-api-main-54dfc4474c-gnvlr from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container builder-api-main ready: true, restart count 0
Sep 25 03:31:14.056: INFO: mdcs-api-common-84c884c5c7-r4kt9 from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container mdcs-api-common ready: true, restart count 0
Sep 25 03:31:14.056: INFO: sonobuoy-e2e-job-a16116856b774ea6 from sonobuoy started at 2020-09-25 02:35:55 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container e2e ready: true, restart count 0
Sep 25 03:31:14.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:31:14.056: INFO: admin-api-common-596bf5fdb7-6pw8d from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container admin-api-common ready: true, restart count 0
Sep 25 03:31:14.056: INFO: portal-api-dock-5c5497b47f-8qjn6 from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container portal-api-dock ready: true, restart count 0
Sep 25 03:31:14.056: INFO: coredns-849d6f84b4-cc8tj from kube-system started at 2020-09-24 03:18:43 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container coredns ready: true, restart count 0
Sep 25 03:31:14.056: INFO: metallb-kubeaddons-speaker-cf5wv from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:31:14.056: INFO: velero-kubeaddons-5d85fcdcb9-6lt97 from velero started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container velero ready: true, restart count 0
Sep 25 03:31:14.056: INFO: common-api-auth-c49dcbcc9-4xb9s from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container common-api-auth ready: true, restart count 0
Sep 25 03:31:14.056: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-pqlj2 from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.056: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:31:14.056: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:31:14.056: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-2 before test
Sep 25 03:31:14.091: INFO: daemon-task-6c8f8f5ffc-pbmqt from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container daemon-task ready: true, restart count 0
Sep 25 03:31:14.091: INFO: coredns-849d6f84b4-qdfhl from kube-system started at 2020-09-24 03:18:43 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container coredns ready: true, restart count 0
Sep 25 03:31:14.091: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-g6ss8 from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:31:14.091: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:31:14.091: INFO: cert-manager-kubeaddons-7d7f98fbc6-fxzfx from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container cert-manager ready: true, restart count 0
Sep 25 03:31:14.091: INFO: dstorageclass-controller-manager-5c966c767f-ngg5l from kubeaddons started at 2020-09-24 03:26:27 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:31:14.091: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:31:14.091: INFO: portal-cdn-upload-567d9969fc-9b6dv from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container portal-cdn-upload ready: false, restart count 11
Sep 25 03:31:14.091: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-qc42h from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:31:14.091: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:31:14.091: INFO: daemon-account-866d44497f-5q7dl from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container daemon-account ready: true, restart count 0
Sep 25 03:31:14.091: INFO: dev-biz-portal-front-687d79db75-q4rwx from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container dev-biz-portal-front ready: false, restart count 19
Sep 25 03:31:14.091: INFO: minio-2 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:31:14.091: INFO: core-sockjs-79b764df9f-24fs4 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container core-sockjs ready: true, restart count 0
Sep 25 03:31:14.091: INFO: workflow-api-draft-5899bb55c9-d4dhd from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container workflow-api-draft ready: true, restart count 0
Sep 25 03:31:14.091: INFO: calico-node-mj6vw from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:31:14.091: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:31:14.091: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-92k4x from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container cainjector ready: true, restart count 0
Sep 25 03:31:14.091: INFO: opsportal-kubeaddons-kommander-ui-6d64cc5d54-c8pct from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
Sep 25 03:31:14.091: INFO: kubeaddons-controller-manager-659bd9c576-jfg77 from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:31:14.091: INFO: local-volume-provisioner-n7q6g from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.091: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:31:14.092: INFO: common-api-oauth-6cd64bdc5-nkwk5 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container common-api-oauth ready: true, restart count 0
Sep 25 03:31:14.092: INFO: core-router-57d6544c9-j52x2 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container core-router ready: true, restart count 0
Sep 25 03:31:14.092: INFO: workflow-api-common-76dd6fb889-gbn6z from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container workflow-api-common ready: true, restart count 0
Sep 25 03:31:14.092: INFO: workflow-daemon-draft-5955b46c89-t7p8l from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container workflow-daemon-draft ready: true, restart count 0
Sep 25 03:31:14.092: INFO: metallb-kubeaddons-speaker-pvqct from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:31:14.092: INFO: metallb-kubeaddons-controller-f84b74d86-8hz2m from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container controller ready: true, restart count 0
Sep 25 03:31:14.092: INFO: store-api-bizgroup-695cc488b5-4pmrh from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container store-api-bizgroup ready: true, restart count 0
Sep 25 03:31:14.092: INFO: opsportal-landing-6f6865b688-v7dhz from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container opsportal-landing ready: true, restart count 0
Sep 25 03:31:14.092: INFO: gatekeeper-kubeaddons-fdc87db85-9q9kx from kubeaddons started at 2020-09-24 03:26:34 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:31:14.092: INFO: portal-api-page-766cc7bf79-vcwbs from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container portal-api-page ready: true, restart count 0
Sep 25 03:31:14.092: INFO: traefik-kubeaddons-68c579bbbd-zwq9n from kubeaddons started at 2020-09-24 03:26:37 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Sep 25 03:31:14.092: INFO: store-api-common-bfd5d66c5-2wkcr from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container store-api-common ready: true, restart count 0
Sep 25 03:31:14.092: INFO: builder-api-custom-676877997f-5vft8 from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.092: INFO: 	Container builder-api-custom ready: true, restart count 0
Sep 25 03:31:14.092: INFO: kube-proxy-5jx4d from kube-system started at 2020-09-24 03:18:13 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.093: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:31:14.093: INFO: reloader-kubeaddons-reloader-7c97f877cf-hpgvn from kubeaddons started at 2020-09-24 03:25:24 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.093: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
Sep 25 03:31:14.093: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-3 before test
Sep 25 03:31:14.110: INFO: portal-api-dock-5c5497b47f-6tctr from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container portal-api-dock ready: true, restart count 0
Sep 25 03:31:14.110: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-2cctd from kubeaddons started at 2020-09-24 03:27:26 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:31:14.110: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:31:14.110: INFO: minio-1 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container minio ready: false, restart count 0
Sep 25 03:31:14.110: INFO: common-api-notify-5595dbd9b5-l4l6r from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container common-api-notify ready: true, restart count 0
Sep 25 03:31:14.110: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-vbvdn from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:31:14.110: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:31:14.110: INFO: portal-cdn-upload-567d9969fc-d4sq2 from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container portal-cdn-upload ready: false, restart count 12
Sep 25 03:31:14.110: INFO: dex-kubeaddons-58884f456-gtkb7 from kubeaddons started at 2020-09-24 03:27:50 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container main ready: true, restart count 1
Sep 25 03:31:14.110: INFO: traefik-forward-auth-kubeaddons-69bbb98fb6-g4fpr from kubeaddons started at 2020-09-24 03:28:06 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container traefik-forward-auth ready: false, restart count 0
Sep 25 03:31:14.110: INFO: builder-api-custom-676877997f-5dj9d from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container builder-api-custom ready: true, restart count 0
Sep 25 03:31:14.110: INFO: velero-kubeaddons-5d85fcdcb9-gwwkq from velero started at 2020-09-24 03:28:07 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container velero ready: true, restart count 3
Sep 25 03:31:14.110: INFO: dex-k8s-authenticator-kubeaddons-74966666f5-9qtxz from kubeaddons started at 2020-09-24 03:28:01 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container dex-k8s-authenticator ready: false, restart count 0
Sep 25 03:31:14.110: INFO: tiller-deploy-6cdf7f9d6f-qqfpj from kube-system started at 2020-09-24 03:25:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container tiller ready: false, restart count 0
Sep 25 03:31:14.110: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-pzrmp from kubeaddons started at 2020-09-24 03:27:52 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container kube-oidc-proxy ready: false, restart count 0
Sep 25 03:31:14.110: INFO: local-volume-provisioner-972q8 from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:31:14.110: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-jvmxg from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container cert-manager ready: true, restart count 1
Sep 25 03:31:14.110: INFO: opsportal-landing-6f6865b688-8sfw4 from kubeaddons started at 2020-09-24 03:25:44 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container opsportal-landing ready: true, restart count 0
Sep 25 03:31:14.110: INFO: admin-api-common-596bf5fdb7-dkf5t from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container admin-api-common ready: true, restart count 0
Sep 25 03:31:14.110: INFO: daemon-task-6c8f8f5ffc-wfsxt from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container daemon-task ready: true, restart count 0
Sep 25 03:31:14.110: INFO: store-api-common-bfd5d66c5-wcnh9 from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container store-api-common ready: true, restart count 0
Sep 25 03:31:14.110: INFO: kube-proxy-lpjq2 from kube-system started at 2020-09-24 03:18:13 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:31:14.110: INFO: opsportal-kubeaddons-kommander-ui-6d64cc5d54-gxbm8 from kubeaddons started at 2020-09-24 03:25:44 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: false, restart count 0
Sep 25 03:31:14.110: INFO: workflow-api-draft-5899bb55c9-j5dgh from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container workflow-api-draft ready: true, restart count 0
Sep 25 03:31:14.110: INFO: pod-init-f9da4979-4ea3-4654-a6f8-244717b3fdb5 from init-container-7871 started at 2020-09-25 03:30:20 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container run1 ready: false, restart count 0
Sep 25 03:31:14.110: INFO: calico-node-bzvbt from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:31:14.110: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:31:14.110: INFO: metallb-kubeaddons-speaker-lbzvt from kubeaddons started at 2020-09-25 03:16:37 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:31:14.110: INFO: kubeaddons-controller-manager-659bd9c576-g2wjv from kubeaddons started at 2020-09-24 03:25:02 +0000 UTC (1 container statuses recorded)
Sep 25 03:31:14.110: INFO: 	Container manager ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f46d2426-ea2a-4e46-93f6-05a38aee7951 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-f46d2426-ea2a-4e46-93f6-05a38aee7951 off the node biz-k8s-node-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f46d2426-ea2a-4e46-93f6-05a38aee7951
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:31:18.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1552" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":197,"skipped":2970,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:31:18.199: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7563
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:31:18.337: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e219c725-7820-4853-976c-bc919673ad6f" in namespace "security-context-test-7563" to be "success or failure"
Sep 25 03:31:18.342: INFO: Pod "alpine-nnp-false-e219c725-7820-4853-976c-bc919673ad6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.641393ms
Sep 25 03:31:20.346: INFO: Pod "alpine-nnp-false-e219c725-7820-4853-976c-bc919673ad6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009154401s
Sep 25 03:31:22.350: INFO: Pod "alpine-nnp-false-e219c725-7820-4853-976c-bc919673ad6f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013408849s
Sep 25 03:31:24.353: INFO: Pod "alpine-nnp-false-e219c725-7820-4853-976c-bc919673ad6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015758204s
Sep 25 03:31:24.353: INFO: Pod "alpine-nnp-false-e219c725-7820-4853-976c-bc919673ad6f" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:31:24.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7563" for this suite.

â€¢ [SLOW TEST:6.166 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":198,"skipped":2981,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:31:24.365: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:31:24.502: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 25 03:31:25.532: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:31:26.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4820" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":199,"skipped":2994,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:31:26.551: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 25 03:31:32.737: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 25 03:31:32.740: INFO: Pod pod-with-prestop-http-hook still exists
Sep 25 03:31:34.740: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 25 03:31:34.744: INFO: Pod pod-with-prestop-http-hook still exists
Sep 25 03:31:36.740: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 25 03:31:36.744: INFO: Pod pod-with-prestop-http-hook still exists
Sep 25 03:31:38.740: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 25 03:31:38.744: INFO: Pod pod-with-prestop-http-hook still exists
Sep 25 03:31:40.740: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 25 03:31:40.745: INFO: Pod pod-with-prestop-http-hook still exists
Sep 25 03:31:42.740: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 25 03:31:42.744: INFO: Pod pod-with-prestop-http-hook still exists
Sep 25 03:31:44.740: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 25 03:31:44.744: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:31:44.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7734" for this suite.

â€¢ [SLOW TEST:18.210 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":200,"skipped":3006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:31:44.761: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7728
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:31:44.888: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:31:46.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7728" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":201,"skipped":3030,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:31:46.040: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:31:46.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-397e537e-8b05-4a46-8bce-0d5ee28a136c" in namespace "projected-7222" to be "success or failure"
Sep 25 03:31:46.185: INFO: Pod "downwardapi-volume-397e537e-8b05-4a46-8bce-0d5ee28a136c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.42642ms
Sep 25 03:31:48.189: INFO: Pod "downwardapi-volume-397e537e-8b05-4a46-8bce-0d5ee28a136c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009383335s
STEP: Saw pod success
Sep 25 03:31:48.189: INFO: Pod "downwardapi-volume-397e537e-8b05-4a46-8bce-0d5ee28a136c" satisfied condition "success or failure"
Sep 25 03:31:48.193: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-397e537e-8b05-4a46-8bce-0d5ee28a136c container client-container: <nil>
STEP: delete the pod
Sep 25 03:31:48.209: INFO: Waiting for pod downwardapi-volume-397e537e-8b05-4a46-8bce-0d5ee28a136c to disappear
Sep 25 03:31:48.210: INFO: Pod downwardapi-volume-397e537e-8b05-4a46-8bce-0d5ee28a136c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:31:48.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7222" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":202,"skipped":3048,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:31:48.217: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Sep 25 03:31:52.896: INFO: Successfully updated pod "annotationupdate020eb22c-4ac2-4db5-9ad1-f452c7e7c8cb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:31:54.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8754" for this suite.

â€¢ [SLOW TEST:6.704 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":203,"skipped":3053,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:31:54.922: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7922
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:31:55.059: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0aaf2ee5-3f78-4a9c-a42f-ceca94894bb2" in namespace "projected-7922" to be "success or failure"
Sep 25 03:31:55.061: INFO: Pod "downwardapi-volume-0aaf2ee5-3f78-4a9c-a42f-ceca94894bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.687434ms
Sep 25 03:31:57.066: INFO: Pod "downwardapi-volume-0aaf2ee5-3f78-4a9c-a42f-ceca94894bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006578133s
Sep 25 03:31:59.071: INFO: Pod "downwardapi-volume-0aaf2ee5-3f78-4a9c-a42f-ceca94894bb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011351878s
STEP: Saw pod success
Sep 25 03:31:59.071: INFO: Pod "downwardapi-volume-0aaf2ee5-3f78-4a9c-a42f-ceca94894bb2" satisfied condition "success or failure"
Sep 25 03:31:59.074: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-0aaf2ee5-3f78-4a9c-a42f-ceca94894bb2 container client-container: <nil>
STEP: delete the pod
Sep 25 03:31:59.092: INFO: Waiting for pod downwardapi-volume-0aaf2ee5-3f78-4a9c-a42f-ceca94894bb2 to disappear
Sep 25 03:31:59.094: INFO: Pod downwardapi-volume-0aaf2ee5-3f78-4a9c-a42f-ceca94894bb2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:31:59.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7922" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":204,"skipped":3054,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:31:59.101: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-2349
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:31:59.233: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Creating first CR 
Sep 25 03:31:59.809: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-25T03:31:59Z generation:1 name:name1 resourceVersion:432079 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4631d5cc-66ea-4f0f-91c3-e31c9039fa06] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 25 03:32:09.816: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-25T03:32:09Z generation:1 name:name2 resourceVersion:432151 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:544e140b-d87e-489d-af09-b0fc3aec7b95] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 25 03:32:19.823: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-25T03:31:59Z generation:2 name:name1 resourceVersion:432202 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4631d5cc-66ea-4f0f-91c3-e31c9039fa06] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 25 03:32:29.829: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-25T03:32:09Z generation:2 name:name2 resourceVersion:432253 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:544e140b-d87e-489d-af09-b0fc3aec7b95] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 25 03:32:39.846: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-25T03:31:59Z generation:2 name:name1 resourceVersion:432302 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4631d5cc-66ea-4f0f-91c3-e31c9039fa06] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 25 03:32:49.854: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-09-25T03:32:09Z generation:2 name:name2 resourceVersion:432348 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:544e140b-d87e-489d-af09-b0fc3aec7b95] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:33:00.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2349" for this suite.

â€¢ [SLOW TEST:61.273 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":205,"skipped":3057,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:33:00.374: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Sep 25 03:33:00.499: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 25 03:33:00.511: INFO: Waiting for terminating namespaces to be deleted...
Sep 25 03:33:00.513: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-1 before test
Sep 25 03:33:00.540: INFO: traefik-kubeaddons-68c579bbbd-2qmn4 from kubeaddons started at 2020-09-24 03:26:59 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Sep 25 03:33:00.540: INFO: store-api-appmanage-6b8b464c4c-m528t from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container store-api-appmanage ready: true, restart count 0
Sep 25 03:33:00.540: INFO: common-api-notify-5595dbd9b5-rb8vd from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container common-api-notify ready: true, restart count 0
Sep 25 03:33:00.540: INFO: kube-proxy-ff4h2 from kube-system started at 2020-09-24 03:18:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:33:00.540: INFO: dex-kubeaddons-58884f456-xtvhz from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container main ready: true, restart count 0
Sep 25 03:33:00.540: INFO: local-volume-provisioner-rmgc9 from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:33:00.540: INFO: builder-daemon-main-fcb6b989c-hfwnc from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container builder-daemon-main ready: true, restart count 0
Sep 25 03:33:00.540: INFO: portal-cdn-download-5cd45dfd87-fqvrl from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container portal-cdn-download ready: false, restart count 19
Sep 25 03:33:00.540: INFO: sonobuoy from sonobuoy started at 2020-09-25 02:35:47 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 25 03:33:00.540: INFO: calico-node-k6ztw from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:33:00.540: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:33:00.540: INFO: calico-kube-controllers-77c79f7594-5qtpj from kube-system started at 2020-09-24 03:18:35 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 25 03:33:00.540: INFO: kubernetes-dashboard-95959d56b-wmxhc from kubeaddons started at 2020-09-24 03:25:22 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Sep 25 03:33:00.540: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 25 03:33:00.540: INFO: minio-3 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:33:00.540: INFO: traefik-forward-auth-kubeaddons-69bbb98fb6-fdn8n from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container traefik-forward-auth ready: true, restart count 1
Sep 25 03:33:00.540: INFO: tiller-deploy-6cdf7f9d6f-5mjft from kube-system started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container tiller ready: true, restart count 0
Sep 25 03:33:00.540: INFO: common-api-account-6b7c79d795-d846m from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container common-api-account ready: true, restart count 0
Sep 25 03:33:00.540: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-q9x6d from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
Sep 25 03:33:00.540: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-rw4pq from cert-manager started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container cert-manager ready: true, restart count 0
Sep 25 03:33:00.540: INFO: minio-0 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:33:00.540: INFO: daemon-notify-74b7d7c4b5-zklwv from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container daemon-notify ready: true, restart count 0
Sep 25 03:33:00.540: INFO: store-api-mypage-5d6d59c499-dvczz from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container store-api-mypage ready: true, restart count 0
Sep 25 03:33:00.540: INFO: dex-k8s-authenticator-kubeaddons-74966666f5-gvf92 from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container dex-k8s-authenticator ready: true, restart count 1
Sep 25 03:33:00.540: INFO: sonobuoy-e2e-job-a16116856b774ea6 from sonobuoy started at 2020-09-25 02:35:55 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container e2e ready: true, restart count 0
Sep 25 03:33:00.540: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:33:00.540: INFO: admin-api-common-596bf5fdb7-6pw8d from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container admin-api-common ready: true, restart count 0
Sep 25 03:33:00.540: INFO: portal-api-dock-5c5497b47f-8qjn6 from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container portal-api-dock ready: true, restart count 0
Sep 25 03:33:00.540: INFO: coredns-849d6f84b4-cc8tj from kube-system started at 2020-09-24 03:18:43 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container coredns ready: true, restart count 0
Sep 25 03:33:00.540: INFO: metallb-kubeaddons-speaker-cf5wv from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:33:00.540: INFO: builder-api-main-54dfc4474c-gnvlr from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container builder-api-main ready: true, restart count 0
Sep 25 03:33:00.540: INFO: mdcs-api-common-84c884c5c7-r4kt9 from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container mdcs-api-common ready: true, restart count 0
Sep 25 03:33:00.540: INFO: common-api-auth-c49dcbcc9-4xb9s from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container common-api-auth ready: true, restart count 0
Sep 25 03:33:00.540: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-pqlj2 from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:33:00.540: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:33:00.540: INFO: velero-kubeaddons-5d85fcdcb9-6lt97 from velero started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.540: INFO: 	Container velero ready: true, restart count 0
Sep 25 03:33:00.540: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-2 before test
Sep 25 03:33:00.573: INFO: calico-node-mj6vw from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:33:00.573: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:33:00.573: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-92k4x from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container cainjector ready: true, restart count 0
Sep 25 03:33:00.573: INFO: minio-2 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:33:00.573: INFO: core-sockjs-79b764df9f-24fs4 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container core-sockjs ready: true, restart count 0
Sep 25 03:33:00.573: INFO: workflow-api-draft-5899bb55c9-d4dhd from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container workflow-api-draft ready: true, restart count 0
Sep 25 03:33:00.573: INFO: opsportal-kubeaddons-kommander-ui-6d64cc5d54-c8pct from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
Sep 25 03:33:00.573: INFO: kubeaddons-controller-manager-659bd9c576-jfg77 from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:33:00.573: INFO: workflow-api-common-76dd6fb889-gbn6z from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container workflow-api-common ready: true, restart count 0
Sep 25 03:33:00.573: INFO: workflow-daemon-draft-5955b46c89-t7p8l from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container workflow-daemon-draft ready: true, restart count 0
Sep 25 03:33:00.573: INFO: metallb-kubeaddons-speaker-pvqct from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:33:00.573: INFO: metallb-kubeaddons-controller-f84b74d86-8hz2m from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container controller ready: true, restart count 0
Sep 25 03:33:00.573: INFO: local-volume-provisioner-n7q6g from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:33:00.573: INFO: common-api-oauth-6cd64bdc5-nkwk5 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container common-api-oauth ready: true, restart count 0
Sep 25 03:33:00.573: INFO: core-router-57d6544c9-j52x2 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container core-router ready: true, restart count 0
Sep 25 03:33:00.573: INFO: gatekeeper-kubeaddons-fdc87db85-9q9kx from kubeaddons started at 2020-09-24 03:26:34 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:33:00.573: INFO: portal-api-page-766cc7bf79-vcwbs from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container portal-api-page ready: true, restart count 0
Sep 25 03:33:00.573: INFO: store-api-bizgroup-695cc488b5-4pmrh from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container store-api-bizgroup ready: true, restart count 0
Sep 25 03:33:00.573: INFO: opsportal-landing-6f6865b688-v7dhz from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container opsportal-landing ready: true, restart count 0
Sep 25 03:33:00.573: INFO: kube-proxy-5jx4d from kube-system started at 2020-09-24 03:18:13 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:33:00.573: INFO: reloader-kubeaddons-reloader-7c97f877cf-hpgvn from kubeaddons started at 2020-09-24 03:25:24 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
Sep 25 03:33:00.573: INFO: traefik-kubeaddons-68c579bbbd-zwq9n from kubeaddons started at 2020-09-24 03:26:37 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Sep 25 03:33:00.573: INFO: store-api-common-bfd5d66c5-2wkcr from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container store-api-common ready: true, restart count 0
Sep 25 03:33:00.573: INFO: builder-api-custom-676877997f-5vft8 from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container builder-api-custom ready: true, restart count 0
Sep 25 03:33:00.573: INFO: coredns-849d6f84b4-qdfhl from kube-system started at 2020-09-24 03:18:43 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.573: INFO: 	Container coredns ready: true, restart count 0
Sep 25 03:33:00.573: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-g6ss8 from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.574: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:33:00.574: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:33:00.574: INFO: daemon-task-6c8f8f5ffc-pbmqt from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.574: INFO: 	Container daemon-task ready: true, restart count 0
Sep 25 03:33:00.574: INFO: cert-manager-kubeaddons-7d7f98fbc6-fxzfx from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.574: INFO: 	Container cert-manager ready: true, restart count 0
Sep 25 03:33:00.574: INFO: dstorageclass-controller-manager-5c966c767f-ngg5l from kubeaddons started at 2020-09-24 03:26:27 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.574: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:33:00.574: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:33:00.574: INFO: daemon-account-866d44497f-5q7dl from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.574: INFO: 	Container daemon-account ready: true, restart count 0
Sep 25 03:33:00.574: INFO: dev-biz-portal-front-687d79db75-q4rwx from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.574: INFO: 	Container dev-biz-portal-front ready: false, restart count 19
Sep 25 03:33:00.574: INFO: portal-cdn-upload-567d9969fc-9b6dv from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.574: INFO: 	Container portal-cdn-upload ready: false, restart count 11
Sep 25 03:33:00.574: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-qc42h from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.574: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:33:00.574: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:33:00.574: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-3 before test
Sep 25 03:33:00.594: INFO: minio-1 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container minio ready: false, restart count 0
Sep 25 03:33:00.594: INFO: common-api-notify-5595dbd9b5-l4l6r from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container common-api-notify ready: true, restart count 0
Sep 25 03:33:00.594: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-2cctd from kubeaddons started at 2020-09-24 03:27:26 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:33:00.594: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:33:00.594: INFO: portal-cdn-upload-567d9969fc-d4sq2 from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container portal-cdn-upload ready: false, restart count 12
Sep 25 03:33:00.594: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-vbvdn from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:33:00.594: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:33:00.594: INFO: dex-kubeaddons-58884f456-gtkb7 from kubeaddons started at 2020-09-24 03:27:50 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container main ready: true, restart count 1
Sep 25 03:33:00.594: INFO: traefik-forward-auth-kubeaddons-69bbb98fb6-g4fpr from kubeaddons started at 2020-09-24 03:28:06 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container traefik-forward-auth ready: false, restart count 0
Sep 25 03:33:00.594: INFO: builder-api-custom-676877997f-5dj9d from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container builder-api-custom ready: true, restart count 0
Sep 25 03:33:00.594: INFO: velero-kubeaddons-5d85fcdcb9-gwwkq from velero started at 2020-09-24 03:28:07 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container velero ready: true, restart count 3
Sep 25 03:33:00.594: INFO: dex-k8s-authenticator-kubeaddons-74966666f5-9qtxz from kubeaddons started at 2020-09-24 03:28:01 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container dex-k8s-authenticator ready: false, restart count 0
Sep 25 03:33:00.594: INFO: tiller-deploy-6cdf7f9d6f-qqfpj from kube-system started at 2020-09-24 03:25:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container tiller ready: false, restart count 0
Sep 25 03:33:00.594: INFO: local-volume-provisioner-972q8 from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:33:00.594: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-pzrmp from kubeaddons started at 2020-09-24 03:27:52 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container kube-oidc-proxy ready: false, restart count 0
Sep 25 03:33:00.594: INFO: opsportal-landing-6f6865b688-8sfw4 from kubeaddons started at 2020-09-24 03:25:44 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container opsportal-landing ready: true, restart count 0
Sep 25 03:33:00.594: INFO: admin-api-common-596bf5fdb7-dkf5t from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container admin-api-common ready: true, restart count 0
Sep 25 03:33:00.594: INFO: daemon-task-6c8f8f5ffc-wfsxt from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container daemon-task ready: true, restart count 0
Sep 25 03:33:00.594: INFO: store-api-common-bfd5d66c5-wcnh9 from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container store-api-common ready: true, restart count 0
Sep 25 03:33:00.594: INFO: kube-proxy-lpjq2 from kube-system started at 2020-09-24 03:18:13 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:33:00.594: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-jvmxg from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container cert-manager ready: true, restart count 1
Sep 25 03:33:00.594: INFO: workflow-api-draft-5899bb55c9-j5dgh from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container workflow-api-draft ready: true, restart count 0
Sep 25 03:33:00.594: INFO: calico-node-bzvbt from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:33:00.594: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:33:00.594: INFO: opsportal-kubeaddons-kommander-ui-6d64cc5d54-gxbm8 from kubeaddons started at 2020-09-24 03:25:44 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: false, restart count 0
Sep 25 03:33:00.594: INFO: kubeaddons-controller-manager-659bd9c576-g2wjv from kubeaddons started at 2020-09-24 03:25:02 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:33:00.594: INFO: metallb-kubeaddons-speaker-lbzvt from kubeaddons started at 2020-09-25 03:16:37 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:33:00.594: INFO: portal-api-dock-5c5497b47f-6tctr from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:33:00.594: INFO: 	Container portal-api-dock ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-38df5c54-3d04-472f-9376-000a4bbbcede 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-38df5c54-3d04-472f-9376-000a4bbbcede off the node biz-k8s-node-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-38df5c54-3d04-472f-9376-000a4bbbcede
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:33:08.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1438" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:8.354 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":206,"skipped":3072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:33:08.729: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Sep 25 03:33:11.400: INFO: Successfully updated pod "labelsupdate0fc911e9-6a76-4783-b003-1f48587def37"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:33:13.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-869" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":207,"skipped":3121,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:33:13.423: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3942
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-tlnbp in namespace proxy-3942
I0925 03:33:13.567531      23 runners.go:189] Created replication controller with name: proxy-service-tlnbp, namespace: proxy-3942, replica count: 1
I0925 03:33:14.618129      23 runners.go:189] proxy-service-tlnbp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 03:33:15.618399      23 runners.go:189] proxy-service-tlnbp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0925 03:33:16.618800      23 runners.go:189] proxy-service-tlnbp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0925 03:33:17.619122      23 runners.go:189] proxy-service-tlnbp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0925 03:33:18.619403      23 runners.go:189] proxy-service-tlnbp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0925 03:33:19.619851      23 runners.go:189] proxy-service-tlnbp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0925 03:33:20.620180      23 runners.go:189] proxy-service-tlnbp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0925 03:33:21.620475      23 runners.go:189] proxy-service-tlnbp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 25 03:33:21.624: INFO: setup took 8.072758946s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 25 03:33:21.634: INFO: (0) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 10.248227ms)
Sep 25 03:33:21.635: INFO: (0) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 10.324335ms)
Sep 25 03:33:21.635: INFO: (0) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 10.614411ms)
Sep 25 03:33:21.635: INFO: (0) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 11.298652ms)
Sep 25 03:33:21.636: INFO: (0) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 11.199143ms)
Sep 25 03:33:21.637: INFO: (0) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 12.540996ms)
Sep 25 03:33:21.640: INFO: (0) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 15.666757ms)
Sep 25 03:33:21.640: INFO: (0) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 16.016276ms)
Sep 25 03:33:21.640: INFO: (0) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 15.827518ms)
Sep 25 03:33:21.642: INFO: (0) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 17.986464ms)
Sep 25 03:33:21.642: INFO: (0) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 17.90159ms)
Sep 25 03:33:21.648: INFO: (0) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 24.080336ms)
Sep 25 03:33:21.648: INFO: (0) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 24.415664ms)
Sep 25 03:33:21.648: INFO: (0) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 24.284314ms)
Sep 25 03:33:21.649: INFO: (0) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 24.498516ms)
Sep 25 03:33:21.649: INFO: (0) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 24.431714ms)
Sep 25 03:33:21.657: INFO: (1) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 8.347503ms)
Sep 25 03:33:21.657: INFO: (1) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 8.006425ms)
Sep 25 03:33:21.657: INFO: (1) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 7.783105ms)
Sep 25 03:33:21.658: INFO: (1) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 8.468972ms)
Sep 25 03:33:21.658: INFO: (1) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 8.16605ms)
Sep 25 03:33:21.658: INFO: (1) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 8.40212ms)
Sep 25 03:33:21.658: INFO: (1) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 8.278162ms)
Sep 25 03:33:21.658: INFO: (1) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 8.432759ms)
Sep 25 03:33:21.658: INFO: (1) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 8.971742ms)
Sep 25 03:33:21.659: INFO: (1) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 9.674273ms)
Sep 25 03:33:21.659: INFO: (1) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 9.492339ms)
Sep 25 03:33:21.659: INFO: (1) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 9.797657ms)
Sep 25 03:33:21.659: INFO: (1) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 9.571351ms)
Sep 25 03:33:21.659: INFO: (1) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 10.365369ms)
Sep 25 03:33:21.660: INFO: (1) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 10.400142ms)
Sep 25 03:33:21.661: INFO: (1) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 11.216955ms)
Sep 25 03:33:21.664: INFO: (2) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 2.662767ms)
Sep 25 03:33:21.665: INFO: (2) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 3.117606ms)
Sep 25 03:33:21.665: INFO: (2) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 4.112023ms)
Sep 25 03:33:21.666: INFO: (2) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 5.115177ms)
Sep 25 03:33:21.666: INFO: (2) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 4.587351ms)
Sep 25 03:33:21.667: INFO: (2) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 5.685423ms)
Sep 25 03:33:21.667: INFO: (2) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 5.314971ms)
Sep 25 03:33:21.667: INFO: (2) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 5.306335ms)
Sep 25 03:33:21.668: INFO: (2) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 5.548805ms)
Sep 25 03:33:21.668: INFO: (2) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 5.820352ms)
Sep 25 03:33:21.668: INFO: (2) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 5.98569ms)
Sep 25 03:33:21.669: INFO: (2) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 7.382237ms)
Sep 25 03:33:21.669: INFO: (2) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 7.717295ms)
Sep 25 03:33:21.670: INFO: (2) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 7.653047ms)
Sep 25 03:33:21.670: INFO: (2) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 7.997564ms)
Sep 25 03:33:21.670: INFO: (2) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 8.286523ms)
Sep 25 03:33:21.673: INFO: (3) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 2.450779ms)
Sep 25 03:33:21.676: INFO: (3) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 5.189597ms)
Sep 25 03:33:21.676: INFO: (3) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 6.107138ms)
Sep 25 03:33:21.676: INFO: (3) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 5.322305ms)
Sep 25 03:33:21.676: INFO: (3) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 5.933924ms)
Sep 25 03:33:21.677: INFO: (3) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 6.644125ms)
Sep 25 03:33:21.677: INFO: (3) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 6.144366ms)
Sep 25 03:33:21.678: INFO: (3) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 6.684573ms)
Sep 25 03:33:21.678: INFO: (3) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 7.428425ms)
Sep 25 03:33:21.678: INFO: (3) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 7.431712ms)
Sep 25 03:33:21.678: INFO: (3) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 6.866727ms)
Sep 25 03:33:21.679: INFO: (3) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 7.595404ms)
Sep 25 03:33:21.679: INFO: (3) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 7.371991ms)
Sep 25 03:33:21.679: INFO: (3) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 7.225847ms)
Sep 25 03:33:21.679: INFO: (3) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 7.553514ms)
Sep 25 03:33:21.679: INFO: (3) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 8.46918ms)
Sep 25 03:33:21.682: INFO: (4) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 3.279037ms)
Sep 25 03:33:21.689: INFO: (4) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 9.255186ms)
Sep 25 03:33:21.689: INFO: (4) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 9.081092ms)
Sep 25 03:33:21.693: INFO: (4) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 13.148452ms)
Sep 25 03:33:21.693: INFO: (4) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 13.752522ms)
Sep 25 03:33:21.694: INFO: (4) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 13.364637ms)
Sep 25 03:33:21.694: INFO: (4) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 13.945492ms)
Sep 25 03:33:21.694: INFO: (4) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 14.412147ms)
Sep 25 03:33:21.694: INFO: (4) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 14.470888ms)
Sep 25 03:33:21.695: INFO: (4) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 14.815885ms)
Sep 25 03:33:21.695: INFO: (4) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 14.902969ms)
Sep 25 03:33:21.696: INFO: (4) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 15.265487ms)
Sep 25 03:33:21.697: INFO: (4) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 16.610573ms)
Sep 25 03:33:21.697: INFO: (4) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 16.439209ms)
Sep 25 03:33:21.697: INFO: (4) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 17.017988ms)
Sep 25 03:33:21.697: INFO: (4) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 16.619753ms)
Sep 25 03:33:21.701: INFO: (5) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 3.944604ms)
Sep 25 03:33:21.701: INFO: (5) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 3.813983ms)
Sep 25 03:33:21.701: INFO: (5) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 3.592313ms)
Sep 25 03:33:21.705: INFO: (5) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 6.198047ms)
Sep 25 03:33:21.705: INFO: (5) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 6.026803ms)
Sep 25 03:33:21.705: INFO: (5) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 6.686229ms)
Sep 25 03:33:21.705: INFO: (5) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 6.260286ms)
Sep 25 03:33:21.705: INFO: (5) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 6.542387ms)
Sep 25 03:33:21.705: INFO: (5) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 6.404167ms)
Sep 25 03:33:21.705: INFO: (5) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 6.14168ms)
Sep 25 03:33:21.705: INFO: (5) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 7.024141ms)
Sep 25 03:33:21.705: INFO: (5) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 6.68846ms)
Sep 25 03:33:21.706: INFO: (5) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 6.906175ms)
Sep 25 03:33:21.706: INFO: (5) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 7.050715ms)
Sep 25 03:33:21.706: INFO: (5) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 7.222849ms)
Sep 25 03:33:21.706: INFO: (5) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 7.102476ms)
Sep 25 03:33:21.708: INFO: (6) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 2.755438ms)
Sep 25 03:33:21.710: INFO: (6) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 4.572016ms)
Sep 25 03:33:21.711: INFO: (6) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 4.49259ms)
Sep 25 03:33:21.712: INFO: (6) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 5.159721ms)
Sep 25 03:33:21.712: INFO: (6) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 5.332078ms)
Sep 25 03:33:21.712: INFO: (6) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 5.182952ms)
Sep 25 03:33:21.712: INFO: (6) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 5.272236ms)
Sep 25 03:33:21.712: INFO: (6) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 5.537228ms)
Sep 25 03:33:21.712: INFO: (6) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 5.7703ms)
Sep 25 03:33:21.712: INFO: (6) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 5.559897ms)
Sep 25 03:33:21.712: INFO: (6) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 5.613624ms)
Sep 25 03:33:21.713: INFO: (6) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 6.543538ms)
Sep 25 03:33:21.713: INFO: (6) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 6.599425ms)
Sep 25 03:33:21.713: INFO: (6) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 6.599458ms)
Sep 25 03:33:21.713: INFO: (6) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 6.805879ms)
Sep 25 03:33:21.713: INFO: (6) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 6.715833ms)
Sep 25 03:33:21.718: INFO: (7) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 4.882015ms)
Sep 25 03:33:21.718: INFO: (7) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 4.028696ms)
Sep 25 03:33:21.718: INFO: (7) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 3.918546ms)
Sep 25 03:33:21.718: INFO: (7) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 4.302915ms)
Sep 25 03:33:21.718: INFO: (7) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 5.206036ms)
Sep 25 03:33:21.718: INFO: (7) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 4.181843ms)
Sep 25 03:33:21.719: INFO: (7) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 5.149409ms)
Sep 25 03:33:21.719: INFO: (7) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 4.811186ms)
Sep 25 03:33:21.719: INFO: (7) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 5.115212ms)
Sep 25 03:33:21.719: INFO: (7) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 5.01525ms)
Sep 25 03:33:21.719: INFO: (7) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 4.370306ms)
Sep 25 03:33:21.719: INFO: (7) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 5.666719ms)
Sep 25 03:33:21.719: INFO: (7) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 5.138921ms)
Sep 25 03:33:21.719: INFO: (7) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 5.560354ms)
Sep 25 03:33:21.719: INFO: (7) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 4.822318ms)
Sep 25 03:33:21.722: INFO: (7) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 7.694421ms)
Sep 25 03:33:21.727: INFO: (8) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 5.249678ms)
Sep 25 03:33:21.727: INFO: (8) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 5.209115ms)
Sep 25 03:33:21.727: INFO: (8) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 4.917438ms)
Sep 25 03:33:21.727: INFO: (8) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 5.150979ms)
Sep 25 03:33:21.728: INFO: (8) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 5.440451ms)
Sep 25 03:33:21.728: INFO: (8) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 5.418797ms)
Sep 25 03:33:21.728: INFO: (8) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 5.57851ms)
Sep 25 03:33:21.728: INFO: (8) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 5.852249ms)
Sep 25 03:33:21.728: INFO: (8) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 6.009749ms)
Sep 25 03:33:21.728: INFO: (8) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 5.890425ms)
Sep 25 03:33:21.730: INFO: (8) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 8.308334ms)
Sep 25 03:33:21.730: INFO: (8) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 8.063018ms)
Sep 25 03:33:21.730: INFO: (8) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 7.996705ms)
Sep 25 03:33:21.730: INFO: (8) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 8.130444ms)
Sep 25 03:33:21.731: INFO: (8) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 8.246559ms)
Sep 25 03:33:21.731: INFO: (8) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 8.414034ms)
Sep 25 03:33:21.735: INFO: (9) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 3.581722ms)
Sep 25 03:33:21.735: INFO: (9) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 4.527982ms)
Sep 25 03:33:21.735: INFO: (9) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 4.40724ms)
Sep 25 03:33:21.735: INFO: (9) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 4.460825ms)
Sep 25 03:33:21.735: INFO: (9) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 4.727622ms)
Sep 25 03:33:21.735: INFO: (9) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 4.428242ms)
Sep 25 03:33:21.736: INFO: (9) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 4.757944ms)
Sep 25 03:33:21.736: INFO: (9) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 4.392804ms)
Sep 25 03:33:21.736: INFO: (9) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 4.426865ms)
Sep 25 03:33:21.737: INFO: (9) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 6.175187ms)
Sep 25 03:33:21.737: INFO: (9) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 6.006264ms)
Sep 25 03:33:21.738: INFO: (9) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 6.96255ms)
Sep 25 03:33:21.738: INFO: (9) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 7.185612ms)
Sep 25 03:33:21.738: INFO: (9) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 7.313813ms)
Sep 25 03:33:21.738: INFO: (9) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 7.242965ms)
Sep 25 03:33:21.738: INFO: (9) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 7.47365ms)
Sep 25 03:33:21.744: INFO: (10) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 5.634878ms)
Sep 25 03:33:21.745: INFO: (10) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 5.6712ms)
Sep 25 03:33:21.745: INFO: (10) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 5.790972ms)
Sep 25 03:33:21.745: INFO: (10) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 6.052129ms)
Sep 25 03:33:21.745: INFO: (10) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 6.647436ms)
Sep 25 03:33:21.745: INFO: (10) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 6.674334ms)
Sep 25 03:33:21.745: INFO: (10) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 6.852523ms)
Sep 25 03:33:21.745: INFO: (10) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 6.871846ms)
Sep 25 03:33:21.746: INFO: (10) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 6.743601ms)
Sep 25 03:33:21.746: INFO: (10) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 6.73126ms)
Sep 25 03:33:21.748: INFO: (10) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 8.760641ms)
Sep 25 03:33:21.748: INFO: (10) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 9.314418ms)
Sep 25 03:33:21.748: INFO: (10) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 9.309781ms)
Sep 25 03:33:21.748: INFO: (10) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 9.364402ms)
Sep 25 03:33:21.748: INFO: (10) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 9.844133ms)
Sep 25 03:33:21.748: INFO: (10) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 9.727716ms)
Sep 25 03:33:21.753: INFO: (11) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 3.224699ms)
Sep 25 03:33:21.753: INFO: (11) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 3.18112ms)
Sep 25 03:33:21.753: INFO: (11) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 4.370163ms)
Sep 25 03:33:21.753: INFO: (11) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 3.548071ms)
Sep 25 03:33:21.753: INFO: (11) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 4.452008ms)
Sep 25 03:33:21.753: INFO: (11) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 3.820691ms)
Sep 25 03:33:21.753: INFO: (11) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 3.876977ms)
Sep 25 03:33:21.753: INFO: (11) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 3.928656ms)
Sep 25 03:33:21.753: INFO: (11) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 3.999858ms)
Sep 25 03:33:21.753: INFO: (11) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 4.061895ms)
Sep 25 03:33:21.756: INFO: (11) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 6.918269ms)
Sep 25 03:33:21.756: INFO: (11) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 6.384602ms)
Sep 25 03:33:21.756: INFO: (11) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 6.655745ms)
Sep 25 03:33:21.756: INFO: (11) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 6.412967ms)
Sep 25 03:33:21.756: INFO: (11) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 6.705486ms)
Sep 25 03:33:21.756: INFO: (11) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 7.047851ms)
Sep 25 03:33:21.758: INFO: (12) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 2.212883ms)
Sep 25 03:33:21.759: INFO: (12) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 2.738274ms)
Sep 25 03:33:21.759: INFO: (12) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 3.057048ms)
Sep 25 03:33:21.760: INFO: (12) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 3.765032ms)
Sep 25 03:33:21.760: INFO: (12) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 3.404545ms)
Sep 25 03:33:21.760: INFO: (12) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 3.582007ms)
Sep 25 03:33:21.760: INFO: (12) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 3.524981ms)
Sep 25 03:33:21.760: INFO: (12) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 3.566761ms)
Sep 25 03:33:21.761: INFO: (12) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 4.097901ms)
Sep 25 03:33:21.761: INFO: (12) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 4.52468ms)
Sep 25 03:33:21.763: INFO: (12) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 5.627091ms)
Sep 25 03:33:21.763: INFO: (12) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 5.976783ms)
Sep 25 03:33:21.763: INFO: (12) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 5.87843ms)
Sep 25 03:33:21.763: INFO: (12) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 5.739101ms)
Sep 25 03:33:21.763: INFO: (12) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 5.944392ms)
Sep 25 03:33:21.763: INFO: (12) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 5.862013ms)
Sep 25 03:33:21.766: INFO: (13) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 2.778918ms)
Sep 25 03:33:21.766: INFO: (13) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 2.581959ms)
Sep 25 03:33:21.766: INFO: (13) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 3.153403ms)
Sep 25 03:33:21.767: INFO: (13) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 3.627964ms)
Sep 25 03:33:21.767: INFO: (13) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 3.287994ms)
Sep 25 03:33:21.767: INFO: (13) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 3.62539ms)
Sep 25 03:33:21.767: INFO: (13) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 3.279506ms)
Sep 25 03:33:21.769: INFO: (13) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 5.582057ms)
Sep 25 03:33:21.769: INFO: (13) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 5.255123ms)
Sep 25 03:33:21.769: INFO: (13) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 6.124283ms)
Sep 25 03:33:21.769: INFO: (13) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 5.666378ms)
Sep 25 03:33:21.769: INFO: (13) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 5.377942ms)
Sep 25 03:33:21.770: INFO: (13) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 5.744227ms)
Sep 25 03:33:21.770: INFO: (13) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 6.291763ms)
Sep 25 03:33:21.770: INFO: (13) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 6.504639ms)
Sep 25 03:33:21.770: INFO: (13) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 6.354797ms)
Sep 25 03:33:21.774: INFO: (14) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 3.738246ms)
Sep 25 03:33:21.775: INFO: (14) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 4.492149ms)
Sep 25 03:33:21.775: INFO: (14) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 4.56884ms)
Sep 25 03:33:21.775: INFO: (14) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 4.623587ms)
Sep 25 03:33:21.775: INFO: (14) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 4.731261ms)
Sep 25 03:33:21.775: INFO: (14) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 4.545974ms)
Sep 25 03:33:21.775: INFO: (14) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 4.522381ms)
Sep 25 03:33:21.775: INFO: (14) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 4.689138ms)
Sep 25 03:33:21.775: INFO: (14) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 4.805781ms)
Sep 25 03:33:21.775: INFO: (14) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 4.65003ms)
Sep 25 03:33:21.776: INFO: (14) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 5.40857ms)
Sep 25 03:33:21.776: INFO: (14) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 5.507003ms)
Sep 25 03:33:21.776: INFO: (14) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 5.62063ms)
Sep 25 03:33:21.776: INFO: (14) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 5.724745ms)
Sep 25 03:33:21.776: INFO: (14) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 5.650031ms)
Sep 25 03:33:21.776: INFO: (14) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 5.637095ms)
Sep 25 03:33:21.779: INFO: (15) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 3.214288ms)
Sep 25 03:33:21.781: INFO: (15) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 4.43218ms)
Sep 25 03:33:21.781: INFO: (15) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 4.936291ms)
Sep 25 03:33:21.783: INFO: (15) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 6.030455ms)
Sep 25 03:33:21.783: INFO: (15) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 6.469117ms)
Sep 25 03:33:21.783: INFO: (15) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 6.507149ms)
Sep 25 03:33:21.783: INFO: (15) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 6.553892ms)
Sep 25 03:33:21.783: INFO: (15) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 6.590629ms)
Sep 25 03:33:21.783: INFO: (15) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 6.924486ms)
Sep 25 03:33:21.783: INFO: (15) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 6.827358ms)
Sep 25 03:33:21.784: INFO: (15) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 7.0493ms)
Sep 25 03:33:21.784: INFO: (15) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 7.047561ms)
Sep 25 03:33:21.784: INFO: (15) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 7.202319ms)
Sep 25 03:33:21.784: INFO: (15) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 7.366685ms)
Sep 25 03:33:21.784: INFO: (15) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 7.294347ms)
Sep 25 03:33:21.784: INFO: (15) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 7.25216ms)
Sep 25 03:33:21.787: INFO: (16) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 2.902846ms)
Sep 25 03:33:21.787: INFO: (16) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 2.616425ms)
Sep 25 03:33:21.790: INFO: (16) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 5.866055ms)
Sep 25 03:33:21.790: INFO: (16) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 6.226761ms)
Sep 25 03:33:21.790: INFO: (16) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 6.045003ms)
Sep 25 03:33:21.791: INFO: (16) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 6.356938ms)
Sep 25 03:33:21.791: INFO: (16) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 6.541525ms)
Sep 25 03:33:21.791: INFO: (16) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 6.248832ms)
Sep 25 03:33:21.791: INFO: (16) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 6.212632ms)
Sep 25 03:33:21.791: INFO: (16) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 6.650972ms)
Sep 25 03:33:21.791: INFO: (16) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 6.568014ms)
Sep 25 03:33:21.791: INFO: (16) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 6.813872ms)
Sep 25 03:33:21.791: INFO: (16) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 7.302805ms)
Sep 25 03:33:21.791: INFO: (16) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 6.847693ms)
Sep 25 03:33:21.791: INFO: (16) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 7.6486ms)
Sep 25 03:33:21.792: INFO: (16) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 7.213927ms)
Sep 25 03:33:21.795: INFO: (17) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 3.003855ms)
Sep 25 03:33:21.798: INFO: (17) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 5.400285ms)
Sep 25 03:33:21.798: INFO: (17) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 6.003381ms)
Sep 25 03:33:21.798: INFO: (17) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 6.530894ms)
Sep 25 03:33:21.798: INFO: (17) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 6.106116ms)
Sep 25 03:33:21.798: INFO: (17) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 6.166421ms)
Sep 25 03:33:21.798: INFO: (17) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 6.313754ms)
Sep 25 03:33:21.798: INFO: (17) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 5.99419ms)
Sep 25 03:33:21.798: INFO: (17) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 6.359542ms)
Sep 25 03:33:21.798: INFO: (17) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 6.201488ms)
Sep 25 03:33:21.798: INFO: (17) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 6.16535ms)
Sep 25 03:33:21.799: INFO: (17) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 6.366538ms)
Sep 25 03:33:21.800: INFO: (17) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 7.73859ms)
Sep 25 03:33:21.800: INFO: (17) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 8.098395ms)
Sep 25 03:33:21.801: INFO: (17) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 8.238763ms)
Sep 25 03:33:21.801: INFO: (17) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 8.204138ms)
Sep 25 03:33:21.803: INFO: (18) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 2.052766ms)
Sep 25 03:33:21.816: INFO: (18) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 15.285243ms)
Sep 25 03:33:21.816: INFO: (18) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 15.29973ms)
Sep 25 03:33:21.816: INFO: (18) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 15.359788ms)
Sep 25 03:33:21.816: INFO: (18) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 15.320924ms)
Sep 25 03:33:21.816: INFO: (18) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 15.382879ms)
Sep 25 03:33:21.816: INFO: (18) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 15.497781ms)
Sep 25 03:33:21.817: INFO: (18) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 15.960141ms)
Sep 25 03:33:21.817: INFO: (18) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 16.097332ms)
Sep 25 03:33:21.817: INFO: (18) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 16.127497ms)
Sep 25 03:33:21.817: INFO: (18) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 16.062388ms)
Sep 25 03:33:21.817: INFO: (18) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 16.119885ms)
Sep 25 03:33:21.817: INFO: (18) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 16.086602ms)
Sep 25 03:33:21.817: INFO: (18) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 16.256901ms)
Sep 25 03:33:21.817: INFO: (18) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 16.207448ms)
Sep 25 03:33:21.817: INFO: (18) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 16.720035ms)
Sep 25 03:33:21.821: INFO: (19) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">test<... (200; 2.882439ms)
Sep 25 03:33:21.821: INFO: (19) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:460/proxy/: tls baz (200; 2.518835ms)
Sep 25 03:33:21.821: INFO: (19) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 2.950904ms)
Sep 25 03:33:21.821: INFO: (19) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:462/proxy/: tls qux (200; 3.570261ms)
Sep 25 03:33:21.821: INFO: (19) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname1/proxy/: tls baz (200; 3.787006ms)
Sep 25 03:33:21.822: INFO: (19) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:160/proxy/: foo (200; 3.321834ms)
Sep 25 03:33:21.822: INFO: (19) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:1080/proxy/rewriteme">... (200; 3.716925ms)
Sep 25 03:33:21.822: INFO: (19) /api/v1/namespaces/proxy-3942/pods/http:proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 3.557134ms)
Sep 25 03:33:21.823: INFO: (19) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd:162/proxy/: bar (200; 4.285235ms)
Sep 25 03:33:21.823: INFO: (19) /api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/proxy-service-tlnbp-zmtdd/proxy/rewriteme">test</a> (200; 4.439488ms)
Sep 25 03:33:21.823: INFO: (19) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname2/proxy/: bar (200; 4.794813ms)
Sep 25 03:33:21.823: INFO: (19) /api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/: <a href="/api/v1/namespaces/proxy-3942/pods/https:proxy-service-tlnbp-zmtdd:443/proxy/tlsrewritem... (200; 4.735695ms)
Sep 25 03:33:21.823: INFO: (19) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname1/proxy/: foo (200; 4.719027ms)
Sep 25 03:33:21.823: INFO: (19) /api/v1/namespaces/proxy-3942/services/proxy-service-tlnbp:portname2/proxy/: bar (200; 4.949943ms)
Sep 25 03:33:21.823: INFO: (19) /api/v1/namespaces/proxy-3942/services/http:proxy-service-tlnbp:portname1/proxy/: foo (200; 4.979628ms)
Sep 25 03:33:21.824: INFO: (19) /api/v1/namespaces/proxy-3942/services/https:proxy-service-tlnbp:tlsportname2/proxy/: tls qux (200; 4.953891ms)
STEP: deleting ReplicationController proxy-service-tlnbp in namespace proxy-3942, will wait for the garbage collector to delete the pods
Sep 25 03:33:21.883: INFO: Deleting ReplicationController proxy-service-tlnbp took: 7.114879ms
Sep 25 03:33:22.784: INFO: Terminating ReplicationController proxy-service-tlnbp pods took: 901.374759ms
[AfterEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:33:32.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3942" for this suite.

â€¢ [SLOW TEST:19.570 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":208,"skipped":3129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:33:32.994: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:33:33.129: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 25 03:33:38.134: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 25 03:33:38.134: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Sep 25 03:33:38.149: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4667 /apis/apps/v1/namespaces/deployment-4667/deployments/test-cleanup-deployment 840de865-a78e-4602-86c8-2a9c5dbd04b1 432813 1 2020-09-25 03:33:38 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0009491c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep 25 03:33:38.154: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-4667 /apis/apps/v1/namespaces/deployment-4667/replicasets/test-cleanup-deployment-55ffc6b7b6 08c76f9d-5588-407b-ab47-d38b30d18ce1 432817 1 2020-09-25 03:33:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 840de865-a78e-4602-86c8-2a9c5dbd04b1 0xc000949b27 0xc000949b28}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000949b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 25 03:33:38.154: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 25 03:33:38.154: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4667 /apis/apps/v1/namespaces/deployment-4667/replicasets/test-cleanup-controller 91b683a5-f193-49e3-9585-9e525be0600c 432816 1 2020-09-25 03:33:33 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 840de865-a78e-4602-86c8-2a9c5dbd04b1 0xc000949a27 0xc000949a28}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000949ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 25 03:33:38.157: INFO: Pod "test-cleanup-controller-w4l8v" is available:
&Pod{ObjectMeta:{test-cleanup-controller-w4l8v test-cleanup-controller- deployment-4667 /api/v1/namespaces/deployment-4667/pods/test-cleanup-controller-w4l8v 317a7933-e865-4351-ab4a-58ef0f0df3ea 432794 0 2020-09-25 03:33:33 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:192.168.100.119/32 cni.projectcalico.org/podIPs:192.168.100.119/32] [{apps/v1 ReplicaSet test-cleanup-controller 91b683a5-f193-49e3-9585-9e525be0600c 0xc001f8f2c7 0xc001f8f2c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-knjr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-knjr4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-knjr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:biz-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:33:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:33:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:33:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-09-25 03:33:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.31,PodIP:192.168.100.119,StartTime:2020-09-25 03:33:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-09-25 03:33:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://9a6cb7cddafbf5317fffae9d3ea8f9646aa2769c6a8656ea70147a33239cde6f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.100.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:33:38.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4667" for this suite.

â€¢ [SLOW TEST:5.181 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":209,"skipped":3185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:33:38.176: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Sep 25 03:33:38.302: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:33:46.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7416" for this suite.

â€¢ [SLOW TEST:7.847 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":210,"skipped":3215,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:33:46.025: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7007
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:33:46.154: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:33:51.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7007" for this suite.

â€¢ [SLOW TEST:5.932 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":211,"skipped":3222,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:33:51.957: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 25 03:33:52.344: INFO: Pod name wrapped-volume-race-597af262-e3b3-4600-a6df-0e6688cfea46: Found 3 pods out of 5
Sep 25 03:33:57.352: INFO: Pod name wrapped-volume-race-597af262-e3b3-4600-a6df-0e6688cfea46: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-597af262-e3b3-4600-a6df-0e6688cfea46 in namespace emptydir-wrapper-690, will wait for the garbage collector to delete the pods
Sep 25 03:33:59.435: INFO: Deleting ReplicationController wrapped-volume-race-597af262-e3b3-4600-a6df-0e6688cfea46 took: 7.609429ms
Sep 25 03:34:00.336: INFO: Terminating ReplicationController wrapped-volume-race-597af262-e3b3-4600-a6df-0e6688cfea46 pods took: 900.365508ms
STEP: Creating RC which spawns configmap-volume pods
Sep 25 03:34:13.147: INFO: Pod name wrapped-volume-race-42671528-1dcb-4afc-81c8-80f5399dc5a4: Found 0 pods out of 5
Sep 25 03:34:18.154: INFO: Pod name wrapped-volume-race-42671528-1dcb-4afc-81c8-80f5399dc5a4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-42671528-1dcb-4afc-81c8-80f5399dc5a4 in namespace emptydir-wrapper-690, will wait for the garbage collector to delete the pods
Sep 25 03:34:20.242: INFO: Deleting ReplicationController wrapped-volume-race-42671528-1dcb-4afc-81c8-80f5399dc5a4 took: 6.915859ms
Sep 25 03:34:20.342: INFO: Terminating ReplicationController wrapped-volume-race-42671528-1dcb-4afc-81c8-80f5399dc5a4 pods took: 100.254608ms
STEP: Creating RC which spawns configmap-volume pods
Sep 25 03:34:33.253: INFO: Pod name wrapped-volume-race-b5a11214-55c8-415a-9bcb-fe5624e36912: Found 0 pods out of 5
Sep 25 03:34:38.260: INFO: Pod name wrapped-volume-race-b5a11214-55c8-415a-9bcb-fe5624e36912: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b5a11214-55c8-415a-9bcb-fe5624e36912 in namespace emptydir-wrapper-690, will wait for the garbage collector to delete the pods
Sep 25 03:34:40.345: INFO: Deleting ReplicationController wrapped-volume-race-b5a11214-55c8-415a-9bcb-fe5624e36912 took: 9.496694ms
Sep 25 03:34:41.248: INFO: Terminating ReplicationController wrapped-volume-race-b5a11214-55c8-415a-9bcb-fe5624e36912 pods took: 903.156164ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:34:53.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-690" for this suite.

â€¢ [SLOW TEST:61.244 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":212,"skipped":3232,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:34:53.201: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1629
[It] should create a deployment from an image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 25 03:34:53.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-7358'
Sep 25 03:34:53.463: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 25 03:34:53.464: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1634
Sep 25 03:34:55.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7358'
Sep 25 03:34:55.581: INFO: stderr: ""
Sep 25 03:34:55.581: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:34:55.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7358" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image [Deprecated] [Conformance]","total":280,"completed":213,"skipped":3252,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:34:55.589: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-67530ba4-3fda-4850-b29f-2d7d83718b4c
STEP: Creating a pod to test consume configMaps
Sep 25 03:34:55.730: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b82b76cb-ad3e-45f4-a719-c3f44a714902" in namespace "projected-7369" to be "success or failure"
Sep 25 03:34:55.733: INFO: Pod "pod-projected-configmaps-b82b76cb-ad3e-45f4-a719-c3f44a714902": Phase="Pending", Reason="", readiness=false. Elapsed: 3.430982ms
Sep 25 03:34:57.738: INFO: Pod "pod-projected-configmaps-b82b76cb-ad3e-45f4-a719-c3f44a714902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008332637s
Sep 25 03:34:59.741: INFO: Pod "pod-projected-configmaps-b82b76cb-ad3e-45f4-a719-c3f44a714902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011412708s
STEP: Saw pod success
Sep 25 03:34:59.741: INFO: Pod "pod-projected-configmaps-b82b76cb-ad3e-45f4-a719-c3f44a714902" satisfied condition "success or failure"
Sep 25 03:34:59.744: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-projected-configmaps-b82b76cb-ad3e-45f4-a719-c3f44a714902 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:34:59.763: INFO: Waiting for pod pod-projected-configmaps-b82b76cb-ad3e-45f4-a719-c3f44a714902 to disappear
Sep 25 03:34:59.766: INFO: Pod pod-projected-configmaps-b82b76cb-ad3e-45f4-a719-c3f44a714902 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:34:59.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7369" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":214,"skipped":3256,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:34:59.773: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6852
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Sep 25 03:34:59.899: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:35:17.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6852" for this suite.

â€¢ [SLOW TEST:17.448 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":215,"skipped":3261,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:35:17.221: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7677
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Sep 25 03:35:17.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-7677'
Sep 25 03:35:17.651: INFO: stderr: ""
Sep 25 03:35:17.651: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Sep 25 03:35:18.656: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 03:35:18.656: INFO: Found 0 / 1
Sep 25 03:35:19.656: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 03:35:19.656: INFO: Found 1 / 1
Sep 25 03:35:19.656: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 25 03:35:19.660: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 03:35:19.660: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 25 03:35:19.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 patch pod agnhost-master-nmc87 --namespace=kubectl-7677 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 25 03:35:19.771: INFO: stderr: ""
Sep 25 03:35:19.771: INFO: stdout: "pod/agnhost-master-nmc87 patched\n"
STEP: checking annotations
Sep 25 03:35:19.774: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 25 03:35:19.774: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:35:19.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7677" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":216,"skipped":3277,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:35:19.784: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3516
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 25 03:35:19.935: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:35:19.938: INFO: Number of nodes with available pods: 0
Sep 25 03:35:19.938: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:35:20.944: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:35:20.948: INFO: Number of nodes with available pods: 0
Sep 25 03:35:20.948: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:35:21.944: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:35:21.948: INFO: Number of nodes with available pods: 0
Sep 25 03:35:21.948: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:35:22.944: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:35:22.948: INFO: Number of nodes with available pods: 0
Sep 25 03:35:22.948: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:35:23.943: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:35:23.947: INFO: Number of nodes with available pods: 3
Sep 25 03:35:23.947: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 25 03:35:23.961: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:35:23.965: INFO: Number of nodes with available pods: 2
Sep 25 03:35:23.965: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:35:24.968: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:35:24.970: INFO: Number of nodes with available pods: 2
Sep 25 03:35:24.970: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:35:25.971: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:35:25.975: INFO: Number of nodes with available pods: 2
Sep 25 03:35:25.976: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:35:26.971: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:35:26.975: INFO: Number of nodes with available pods: 2
Sep 25 03:35:26.975: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:35:27.971: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:35:27.975: INFO: Number of nodes with available pods: 3
Sep 25 03:35:27.975: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3516, will wait for the garbage collector to delete the pods
Sep 25 03:35:28.041: INFO: Deleting DaemonSet.extensions daemon-set took: 7.750324ms
Sep 25 03:35:28.942: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.408838ms
Sep 25 03:35:33.046: INFO: Number of nodes with available pods: 0
Sep 25 03:35:33.046: INFO: Number of running nodes: 0, number of available pods: 0
Sep 25 03:35:33.049: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3516/daemonsets","resourceVersion":"434445"},"items":null}

Sep 25 03:35:33.052: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3516/pods","resourceVersion":"434445"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:35:33.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3516" for this suite.

â€¢ [SLOW TEST:13.291 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":217,"skipped":3288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:35:33.075: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5574
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5574.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5574.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5574.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5574.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5574.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5574.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 25 03:35:37.261: INFO: DNS probes using dns-5574/dns-test-134e3ca8-c46d-4779-b716-c005e3e5654e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:35:37.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5574" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":218,"skipped":3324,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:35:37.292: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Sep 25 03:35:37.420: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-888863303 proxy --unix-socket=/tmp/kubectl-proxy-unix404673618/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:35:37.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9683" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":219,"skipped":3328,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:35:37.503: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6877
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Sep 25 03:35:42.189: INFO: Successfully updated pod "annotationupdate3843b44f-2295-4a77-94c0-777c04e49a73"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:35:44.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6877" for this suite.

â€¢ [SLOW TEST:6.711 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":220,"skipped":3330,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:35:44.215: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:35:49.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7471" for this suite.

â€¢ [SLOW TEST:5.206 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":221,"skipped":3342,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:35:49.421: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-a032f817-4fb3-4f97-8ac0-8160f8decc3f
STEP: Creating a pod to test consume configMaps
Sep 25 03:35:49.561: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b9a4605c-0e49-4b6e-b980-410cdfcefcf7" in namespace "projected-3168" to be "success or failure"
Sep 25 03:35:49.565: INFO: Pod "pod-projected-configmaps-b9a4605c-0e49-4b6e-b980-410cdfcefcf7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.133008ms
Sep 25 03:35:51.569: INFO: Pod "pod-projected-configmaps-b9a4605c-0e49-4b6e-b980-410cdfcefcf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007649981s
STEP: Saw pod success
Sep 25 03:35:51.569: INFO: Pod "pod-projected-configmaps-b9a4605c-0e49-4b6e-b980-410cdfcefcf7" satisfied condition "success or failure"
Sep 25 03:35:51.572: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-projected-configmaps-b9a4605c-0e49-4b6e-b980-410cdfcefcf7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:35:51.586: INFO: Waiting for pod pod-projected-configmaps-b9a4605c-0e49-4b6e-b980-410cdfcefcf7 to disappear
Sep 25 03:35:51.587: INFO: Pod pod-projected-configmaps-b9a4605c-0e49-4b6e-b980-410cdfcefcf7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:35:51.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3168" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":222,"skipped":3358,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:35:51.594: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:35:51.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1902" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":223,"skipped":3365,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:35:51.746: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:35:58.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7153" for this suite.

â€¢ [SLOW TEST:7.145 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":224,"skipped":3367,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:35:58.892: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:36:03.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3962" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":225,"skipped":3372,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:36:03.066: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:36:03.221: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 25 03:36:03.230: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:03.232: INFO: Number of nodes with available pods: 0
Sep 25 03:36:03.232: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:36:04.237: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:04.241: INFO: Number of nodes with available pods: 0
Sep 25 03:36:04.241: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:36:05.238: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:05.243: INFO: Number of nodes with available pods: 0
Sep 25 03:36:05.243: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:36:06.237: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:06.242: INFO: Number of nodes with available pods: 2
Sep 25 03:36:06.242: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:07.238: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:07.242: INFO: Number of nodes with available pods: 3
Sep 25 03:36:07.242: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 25 03:36:07.270: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:07.270: INFO: Wrong image for pod: daemon-set-z5drx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:07.270: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:07.276: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:08.279: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:08.279: INFO: Wrong image for pod: daemon-set-z5drx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:08.279: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:08.282: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:09.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:09.281: INFO: Wrong image for pod: daemon-set-z5drx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:09.281: INFO: Pod daemon-set-z5drx is not available
Sep 25 03:36:09.281: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:09.286: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:10.280: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:10.280: INFO: Wrong image for pod: daemon-set-z5drx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:10.280: INFO: Pod daemon-set-z5drx is not available
Sep 25 03:36:10.280: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:10.285: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:11.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:11.281: INFO: Wrong image for pod: daemon-set-z5drx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:11.281: INFO: Pod daemon-set-z5drx is not available
Sep 25 03:36:11.281: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:11.288: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:12.282: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:12.282: INFO: Wrong image for pod: daemon-set-z5drx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:12.282: INFO: Pod daemon-set-z5drx is not available
Sep 25 03:36:12.282: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:12.287: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:13.281: INFO: Pod daemon-set-4w92z is not available
Sep 25 03:36:13.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:13.281: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:13.287: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:14.282: INFO: Pod daemon-set-4w92z is not available
Sep 25 03:36:14.282: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:14.282: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:14.286: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:15.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:15.281: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:15.286: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:16.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:16.281: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:16.286: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:17.280: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:17.280: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:17.280: INFO: Pod daemon-set-z65mn is not available
Sep 25 03:36:17.283: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:18.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:18.282: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:18.282: INFO: Pod daemon-set-z65mn is not available
Sep 25 03:36:18.287: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:19.282: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:19.282: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:19.282: INFO: Pod daemon-set-z65mn is not available
Sep 25 03:36:19.288: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:20.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:20.281: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:20.281: INFO: Pod daemon-set-z65mn is not available
Sep 25 03:36:20.287: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:21.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:21.281: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:21.281: INFO: Pod daemon-set-z65mn is not available
Sep 25 03:36:21.287: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:22.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:22.281: INFO: Wrong image for pod: daemon-set-z65mn. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:22.281: INFO: Pod daemon-set-z65mn is not available
Sep 25 03:36:22.286: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:23.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:23.281: INFO: Pod daemon-set-pv9ht is not available
Sep 25 03:36:23.287: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:24.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:24.281: INFO: Pod daemon-set-pv9ht is not available
Sep 25 03:36:24.285: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:25.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:25.281: INFO: Pod daemon-set-pv9ht is not available
Sep 25 03:36:25.285: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:26.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:26.285: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:27.281: INFO: Wrong image for pod: daemon-set-g27kq. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Sep 25 03:36:27.281: INFO: Pod daemon-set-g27kq is not available
Sep 25 03:36:27.286: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:28.280: INFO: Pod daemon-set-srgxq is not available
Sep 25 03:36:28.283: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 25 03:36:28.287: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:28.290: INFO: Number of nodes with available pods: 2
Sep 25 03:36:28.290: INFO: Node biz-k8s-node-3 is running more than one daemon pod
Sep 25 03:36:29.296: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:29.299: INFO: Number of nodes with available pods: 2
Sep 25 03:36:29.299: INFO: Node biz-k8s-node-3 is running more than one daemon pod
Sep 25 03:36:30.296: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:30.299: INFO: Number of nodes with available pods: 3
Sep 25 03:36:30.299: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6541, will wait for the garbage collector to delete the pods
Sep 25 03:36:30.372: INFO: Deleting DaemonSet.extensions daemon-set took: 7.047735ms
Sep 25 03:36:31.272: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.350096ms
Sep 25 03:36:43.176: INFO: Number of nodes with available pods: 0
Sep 25 03:36:43.176: INFO: Number of running nodes: 0, number of available pods: 0
Sep 25 03:36:43.178: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6541/daemonsets","resourceVersion":"435372"},"items":null}

Sep 25 03:36:43.179: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6541/pods","resourceVersion":"435372"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:36:43.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6541" for this suite.

â€¢ [SLOW TEST:40.130 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":226,"skipped":3391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:36:43.197: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 25 03:36:43.353: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:43.356: INFO: Number of nodes with available pods: 0
Sep 25 03:36:43.356: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:36:44.362: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:44.366: INFO: Number of nodes with available pods: 0
Sep 25 03:36:44.366: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:36:45.362: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:45.365: INFO: Number of nodes with available pods: 0
Sep 25 03:36:45.365: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:36:46.364: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:46.366: INFO: Number of nodes with available pods: 1
Sep 25 03:36:46.366: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:47.362: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:47.365: INFO: Number of nodes with available pods: 3
Sep 25 03:36:47.365: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 25 03:36:47.378: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:47.380: INFO: Number of nodes with available pods: 2
Sep 25 03:36:47.380: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:48.383: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:48.385: INFO: Number of nodes with available pods: 2
Sep 25 03:36:48.385: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:49.386: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:49.390: INFO: Number of nodes with available pods: 2
Sep 25 03:36:49.390: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:50.385: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:50.387: INFO: Number of nodes with available pods: 2
Sep 25 03:36:50.388: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:51.385: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:51.389: INFO: Number of nodes with available pods: 2
Sep 25 03:36:51.389: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:52.384: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:52.386: INFO: Number of nodes with available pods: 2
Sep 25 03:36:52.386: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:53.385: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:53.389: INFO: Number of nodes with available pods: 2
Sep 25 03:36:53.389: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:54.384: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:54.389: INFO: Number of nodes with available pods: 2
Sep 25 03:36:54.389: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:55.384: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:55.386: INFO: Number of nodes with available pods: 2
Sep 25 03:36:55.387: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:56.386: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:56.390: INFO: Number of nodes with available pods: 2
Sep 25 03:36:56.390: INFO: Node biz-k8s-node-2 is running more than one daemon pod
Sep 25 03:36:57.386: INFO: DaemonSet pods can't tolerate node biz-k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 25 03:36:57.389: INFO: Number of nodes with available pods: 3
Sep 25 03:36:57.389: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4952, will wait for the garbage collector to delete the pods
Sep 25 03:36:57.451: INFO: Deleting DaemonSet.extensions daemon-set took: 6.581498ms
Sep 25 03:36:58.351: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.29918ms
Sep 25 03:37:03.153: INFO: Number of nodes with available pods: 0
Sep 25 03:37:03.153: INFO: Number of running nodes: 0, number of available pods: 0
Sep 25 03:37:03.154: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4952/daemonsets","resourceVersion":"435620"},"items":null}

Sep 25 03:37:03.156: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4952/pods","resourceVersion":"435620"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:37:03.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4952" for this suite.

â€¢ [SLOW TEST:19.973 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":227,"skipped":3414,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:37:03.170: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 25 03:37:03.309: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 25 03:37:12.365: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:37:12.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6450" for this suite.

â€¢ [SLOW TEST:9.206 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":228,"skipped":3430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:37:12.377: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5904
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Sep 25 03:37:12.507: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Sep 25 03:37:12.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-5904'
Sep 25 03:37:12.700: INFO: stderr: ""
Sep 25 03:37:12.700: INFO: stdout: "service/agnhost-slave created\n"
Sep 25 03:37:12.701: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Sep 25 03:37:12.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-5904'
Sep 25 03:37:12.885: INFO: stderr: ""
Sep 25 03:37:12.885: INFO: stdout: "service/agnhost-master created\n"
Sep 25 03:37:12.885: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 25 03:37:12.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-5904'
Sep 25 03:37:13.055: INFO: stderr: ""
Sep 25 03:37:13.055: INFO: stdout: "service/frontend created\n"
Sep 25 03:37:13.055: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep 25 03:37:13.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-5904'
Sep 25 03:37:13.225: INFO: stderr: ""
Sep 25 03:37:13.225: INFO: stdout: "deployment.apps/frontend created\n"
Sep 25 03:37:13.226: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 25 03:37:13.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-5904'
Sep 25 03:37:13.398: INFO: stderr: ""
Sep 25 03:37:13.398: INFO: stdout: "deployment.apps/agnhost-master created\n"
Sep 25 03:37:13.398: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 25 03:37:13.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-5904'
Sep 25 03:37:13.572: INFO: stderr: ""
Sep 25 03:37:13.572: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Sep 25 03:37:13.572: INFO: Waiting for all frontend pods to be Running.
Sep 25 03:37:18.623: INFO: Waiting for frontend to serve content.
Sep 25 03:37:18.638: INFO: Trying to add a new entry to the guestbook.
Sep 25 03:37:18.647: INFO: Verifying that added entry can be retrieved.
Sep 25 03:37:18.659: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Sep 25 03:37:23.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete --grace-period=0 --force -f - --namespace=kubectl-5904'
Sep 25 03:37:23.847: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 25 03:37:23.847: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep 25 03:37:23.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete --grace-period=0 --force -f - --namespace=kubectl-5904'
Sep 25 03:37:23.943: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 25 03:37:23.943: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 25 03:37:23.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete --grace-period=0 --force -f - --namespace=kubectl-5904'
Sep 25 03:37:24.038: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 25 03:37:24.038: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 25 03:37:24.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete --grace-period=0 --force -f - --namespace=kubectl-5904'
Sep 25 03:37:24.128: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 25 03:37:24.128: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 25 03:37:24.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete --grace-period=0 --force -f - --namespace=kubectl-5904'
Sep 25 03:37:24.208: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 25 03:37:24.208: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 25 03:37:24.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete --grace-period=0 --force -f - --namespace=kubectl-5904'
Sep 25 03:37:24.292: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 25 03:37:24.292: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:37:24.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5904" for this suite.

â€¢ [SLOW TEST:11.922 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:381
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":229,"skipped":3454,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:37:24.299: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-f2d01fd8-6a63-4ef0-8d78-00b7d6c861c5
STEP: Creating a pod to test consume configMaps
Sep 25 03:37:24.439: INFO: Waiting up to 5m0s for pod "pod-configmaps-6aebe68f-6b27-4b6c-bed9-fbf2a5719637" in namespace "configmap-3139" to be "success or failure"
Sep 25 03:37:24.443: INFO: Pod "pod-configmaps-6aebe68f-6b27-4b6c-bed9-fbf2a5719637": Phase="Pending", Reason="", readiness=false. Elapsed: 3.043332ms
Sep 25 03:37:26.447: INFO: Pod "pod-configmaps-6aebe68f-6b27-4b6c-bed9-fbf2a5719637": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007272042s
Sep 25 03:37:28.452: INFO: Pod "pod-configmaps-6aebe68f-6b27-4b6c-bed9-fbf2a5719637": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012126503s
STEP: Saw pod success
Sep 25 03:37:28.452: INFO: Pod "pod-configmaps-6aebe68f-6b27-4b6c-bed9-fbf2a5719637" satisfied condition "success or failure"
Sep 25 03:37:28.454: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-configmaps-6aebe68f-6b27-4b6c-bed9-fbf2a5719637 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:37:28.469: INFO: Waiting for pod pod-configmaps-6aebe68f-6b27-4b6c-bed9-fbf2a5719637 to disappear
Sep 25 03:37:28.471: INFO: Pod pod-configmaps-6aebe68f-6b27-4b6c-bed9-fbf2a5719637 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:37:28.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3139" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":230,"skipped":3455,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:37:28.478: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5054
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create a job from an image, then delete the job [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Sep 25 03:37:28.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 --namespace=kubectl-5054 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep 25 03:37:32.166: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep 25 03:37:32.166: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:37:34.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5054" for this suite.

â€¢ [SLOW TEST:5.707 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1843
    should create a job from an image, then delete the job [Deprecated] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job [Deprecated] [Conformance]","total":280,"completed":231,"skipped":3463,"failed":0}
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:37:34.185: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7290
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:37:34.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7290" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":232,"skipped":3463,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:37:34.327: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-5566
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 25 03:37:34.453: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 25 03:37:58.548: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.100.123:8080/dial?request=hostname&protocol=udp&host=192.168.100.184&port=8081&tries=1'] Namespace:pod-network-test-5566 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 03:37:58.548: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 03:37:58.644: INFO: Waiting for responses: map[]
Sep 25 03:37:58.647: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.100.123:8080/dial?request=hostname&protocol=udp&host=192.168.100.55&port=8081&tries=1'] Namespace:pod-network-test-5566 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 03:37:58.647: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 03:37:58.743: INFO: Waiting for responses: map[]
Sep 25 03:37:58.745: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.100.123:8080/dial?request=hostname&protocol=udp&host=192.168.100.111&port=8081&tries=1'] Namespace:pod-network-test-5566 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 25 03:37:58.745: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 03:37:58.855: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:37:58.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5566" for this suite.

â€¢ [SLOW TEST:24.536 seconds]
[sig-network] Networking
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":233,"skipped":3464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:37:58.864: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:37:59.267: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:38:01.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601879, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601879, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601879, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601879, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:38:04.292: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:38:04.296: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7914-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:05.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2773" for this suite.
STEP: Destroying namespace "webhook-2773-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.661 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":234,"skipped":3494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:05.527: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:38:05.985: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:38:07.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601885, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601885, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601885, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601885, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:38:11.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:11.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1872" for this suite.
STEP: Destroying namespace "webhook-1872-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.578 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":235,"skipped":3619,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:11.106: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 25 03:38:11.257: INFO: Waiting up to 5m0s for pod "pod-0e5d4915-cf86-4358-9551-4aef073c319c" in namespace "emptydir-2514" to be "success or failure"
Sep 25 03:38:11.265: INFO: Pod "pod-0e5d4915-cf86-4358-9551-4aef073c319c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.220383ms
Sep 25 03:38:13.270: INFO: Pod "pod-0e5d4915-cf86-4358-9551-4aef073c319c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012588295s
Sep 25 03:38:15.275: INFO: Pod "pod-0e5d4915-cf86-4358-9551-4aef073c319c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017754561s
Sep 25 03:38:17.279: INFO: Pod "pod-0e5d4915-cf86-4358-9551-4aef073c319c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021695534s
STEP: Saw pod success
Sep 25 03:38:17.279: INFO: Pod "pod-0e5d4915-cf86-4358-9551-4aef073c319c" satisfied condition "success or failure"
Sep 25 03:38:17.282: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-0e5d4915-cf86-4358-9551-4aef073c319c container test-container: <nil>
STEP: delete the pod
Sep 25 03:38:17.296: INFO: Waiting for pod pod-0e5d4915-cf86-4358-9551-4aef073c319c to disappear
Sep 25 03:38:17.301: INFO: Pod pod-0e5d4915-cf86-4358-9551-4aef073c319c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:17.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2514" for this suite.

â€¢ [SLOW TEST:6.202 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":236,"skipped":3626,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:17.309: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1358
STEP: creating an pod
Sep 25 03:38:17.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-9945 -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 25 03:38:17.554: INFO: stderr: ""
Sep 25 03:38:17.554: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Sep 25 03:38:17.554: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 25 03:38:17.554: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9945" to be "running and ready, or succeeded"
Sep 25 03:38:17.557: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.392146ms
Sep 25 03:38:19.559: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004778243s
Sep 25 03:38:21.563: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.008180717s
Sep 25 03:38:21.563: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 25 03:38:21.563: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 25 03:38:21.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 logs logs-generator logs-generator --namespace=kubectl-9945'
Sep 25 03:38:21.690: INFO: stderr: ""
Sep 25 03:38:21.690: INFO: stdout: "I0925 03:38:18.950180       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/vkn 418\nI0925 03:38:19.150561       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/2tft 416\nI0925 03:38:19.350544       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/8b6 360\nI0925 03:38:19.550357       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/4srp 433\nI0925 03:38:19.750643       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/xlml 514\nI0925 03:38:19.950436       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/7t29 400\nI0925 03:38:20.150359       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/p2f 350\nI0925 03:38:20.350371       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/zc5s 391\nI0925 03:38:20.550456       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/jmb 445\nI0925 03:38:20.750369       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/q4m7 484\nI0925 03:38:20.950362       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/m7xp 276\nI0925 03:38:21.150368       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/8rv 204\nI0925 03:38:21.350379       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/9zp4 441\nI0925 03:38:21.550478       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/dgg 507\n"
STEP: limiting log lines
Sep 25 03:38:21.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 logs logs-generator logs-generator --namespace=kubectl-9945 --tail=1'
Sep 25 03:38:21.815: INFO: stderr: ""
Sep 25 03:38:21.815: INFO: stdout: "I0925 03:38:21.750483       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/t6d 391\n"
Sep 25 03:38:21.815: INFO: got output "I0925 03:38:21.750483       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/t6d 391\n"
STEP: limiting log bytes
Sep 25 03:38:21.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 logs logs-generator logs-generator --namespace=kubectl-9945 --limit-bytes=1'
Sep 25 03:38:21.937: INFO: stderr: ""
Sep 25 03:38:21.937: INFO: stdout: "I"
Sep 25 03:38:21.937: INFO: got output "I"
STEP: exposing timestamps
Sep 25 03:38:21.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 logs logs-generator logs-generator --namespace=kubectl-9945 --tail=1 --timestamps'
Sep 25 03:38:22.051: INFO: stderr: ""
Sep 25 03:38:22.052: INFO: stdout: "2020-09-25T12:38:21.950683753+09:00 I0925 03:38:21.950397       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/f2f 276\n"
Sep 25 03:38:22.052: INFO: got output "2020-09-25T12:38:21.950683753+09:00 I0925 03:38:21.950397       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/f2f 276\n"
STEP: restricting to a time range
Sep 25 03:38:24.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 logs logs-generator logs-generator --namespace=kubectl-9945 --since=1s'
Sep 25 03:38:24.678: INFO: stderr: ""
Sep 25 03:38:24.678: INFO: stdout: "I0925 03:38:23.750447       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/5mb 223\nI0925 03:38:23.950371       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/2pbj 369\nI0925 03:38:24.150500       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/4h8r 385\nI0925 03:38:24.350522       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/wvw 281\nI0925 03:38:24.550424       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/rptx 404\n"
Sep 25 03:38:24.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 logs logs-generator logs-generator --namespace=kubectl-9945 --since=24h'
Sep 25 03:38:24.798: INFO: stderr: ""
Sep 25 03:38:24.798: INFO: stdout: "I0925 03:38:18.950180       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/vkn 418\nI0925 03:38:19.150561       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/2tft 416\nI0925 03:38:19.350544       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/8b6 360\nI0925 03:38:19.550357       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/4srp 433\nI0925 03:38:19.750643       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/xlml 514\nI0925 03:38:19.950436       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/7t29 400\nI0925 03:38:20.150359       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/p2f 350\nI0925 03:38:20.350371       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/zc5s 391\nI0925 03:38:20.550456       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/jmb 445\nI0925 03:38:20.750369       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/q4m7 484\nI0925 03:38:20.950362       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/m7xp 276\nI0925 03:38:21.150368       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/8rv 204\nI0925 03:38:21.350379       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/9zp4 441\nI0925 03:38:21.550478       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/dgg 507\nI0925 03:38:21.750483       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/t6d 391\nI0925 03:38:21.950397       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/f2f 276\nI0925 03:38:22.150418       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/rl5j 465\nI0925 03:38:22.350432       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/rrg 507\nI0925 03:38:22.550399       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/9zzt 599\nI0925 03:38:22.750597       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/bfnx 410\nI0925 03:38:22.950337       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/22pf 582\nI0925 03:38:23.150480       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/8hdm 585\nI0925 03:38:23.350497       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/8wxd 279\nI0925 03:38:23.550465       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/xdqh 468\nI0925 03:38:23.750447       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/5mb 223\nI0925 03:38:23.950371       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/2pbj 369\nI0925 03:38:24.150500       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/4h8r 385\nI0925 03:38:24.350522       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/wvw 281\nI0925 03:38:24.550424       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/rptx 404\nI0925 03:38:24.750503       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/gszv 443\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1364
Sep 25 03:38:24.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete pod logs-generator --namespace=kubectl-9945'
Sep 25 03:38:27.106: INFO: stderr: ""
Sep 25 03:38:27.106: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:27.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9945" for this suite.

â€¢ [SLOW TEST:9.804 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":237,"skipped":3660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:27.114: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 25 03:38:27.259: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5793 /api/v1/namespaces/watch-5793/configmaps/e2e-watch-test-watch-closed 5141e8c8-8736-49e9-9a94-42cc067a923e 436787 0 2020-09-25 03:38:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 25 03:38:27.259: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5793 /api/v1/namespaces/watch-5793/configmaps/e2e-watch-test-watch-closed 5141e8c8-8736-49e9-9a94-42cc067a923e 436788 0 2020-09-25 03:38:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 25 03:38:27.270: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5793 /api/v1/namespaces/watch-5793/configmaps/e2e-watch-test-watch-closed 5141e8c8-8736-49e9-9a94-42cc067a923e 436789 0 2020-09-25 03:38:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 25 03:38:27.270: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5793 /api/v1/namespaces/watch-5793/configmaps/e2e-watch-test-watch-closed 5141e8c8-8736-49e9-9a94-42cc067a923e 436790 0 2020-09-25 03:38:27 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:27.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5793" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":238,"skipped":3743,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:27.276: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9649
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-7a7cc2df-8d9c-4011-ba9a-0827bce628b8
STEP: Creating secret with name s-test-opt-upd-f387dc28-2097-4e80-a93c-f6ecefea30fc
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7a7cc2df-8d9c-4011-ba9a-0827bce628b8
STEP: Updating secret s-test-opt-upd-f387dc28-2097-4e80-a93c-f6ecefea30fc
STEP: Creating secret with name s-test-opt-create-51a1f2ff-325e-4fe3-b55c-419884111369
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:35.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9649" for this suite.

â€¢ [SLOW TEST:8.247 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":239,"skipped":3763,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:35.523: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1469
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:38:37.702: INFO: Waiting up to 5m0s for pod "client-envvars-fb3b7d37-3866-422b-8131-c04b9560c3ee" in namespace "pods-1469" to be "success or failure"
Sep 25 03:38:37.705: INFO: Pod "client-envvars-fb3b7d37-3866-422b-8131-c04b9560c3ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.24963ms
Sep 25 03:38:39.709: INFO: Pod "client-envvars-fb3b7d37-3866-422b-8131-c04b9560c3ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007587426s
Sep 25 03:38:41.713: INFO: Pod "client-envvars-fb3b7d37-3866-422b-8131-c04b9560c3ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011041988s
STEP: Saw pod success
Sep 25 03:38:41.713: INFO: Pod "client-envvars-fb3b7d37-3866-422b-8131-c04b9560c3ee" satisfied condition "success or failure"
Sep 25 03:38:41.716: INFO: Trying to get logs from node biz-k8s-node-3 pod client-envvars-fb3b7d37-3866-422b-8131-c04b9560c3ee container env3cont: <nil>
STEP: delete the pod
Sep 25 03:38:41.731: INFO: Waiting for pod client-envvars-fb3b7d37-3866-422b-8131-c04b9560c3ee to disappear
Sep 25 03:38:41.734: INFO: Pod client-envvars-fb3b7d37-3866-422b-8131-c04b9560c3ee no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:41.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1469" for this suite.

â€¢ [SLOW TEST:6.215 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":240,"skipped":3808,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:41.739: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:38:41.874: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-4cf273e6-21d9-4f33-8f57-15e33f1442ec" in namespace "security-context-test-6500" to be "success or failure"
Sep 25 03:38:41.877: INFO: Pod "busybox-readonly-false-4cf273e6-21d9-4f33-8f57-15e33f1442ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.790547ms
Sep 25 03:38:43.881: INFO: Pod "busybox-readonly-false-4cf273e6-21d9-4f33-8f57-15e33f1442ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007008952s
Sep 25 03:38:45.886: INFO: Pod "busybox-readonly-false-4cf273e6-21d9-4f33-8f57-15e33f1442ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011541759s
Sep 25 03:38:45.886: INFO: Pod "busybox-readonly-false-4cf273e6-21d9-4f33-8f57-15e33f1442ec" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:45.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6500" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":241,"skipped":3824,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:45.892: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:46.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9288" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":242,"skipped":3840,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:46.081: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Sep 25 03:38:46.204: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 25 03:38:46.214: INFO: Waiting for terminating namespaces to be deleted...
Sep 25 03:38:46.217: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-1 before test
Sep 25 03:38:46.245: INFO: minio-0 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:38:46.245: INFO: daemon-notify-74b7d7c4b5-zklwv from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container daemon-notify ready: true, restart count 0
Sep 25 03:38:46.245: INFO: store-api-mypage-5d6d59c499-dvczz from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container store-api-mypage ready: true, restart count 0
Sep 25 03:38:46.245: INFO: dex-k8s-authenticator-kubeaddons-74966666f5-gvf92 from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container dex-k8s-authenticator ready: true, restart count 1
Sep 25 03:38:46.245: INFO: coredns-849d6f84b4-cc8tj from kube-system started at 2020-09-24 03:18:43 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container coredns ready: true, restart count 0
Sep 25 03:38:46.245: INFO: metallb-kubeaddons-speaker-cf5wv from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:38:46.245: INFO: builder-api-main-54dfc4474c-gnvlr from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container builder-api-main ready: true, restart count 0
Sep 25 03:38:46.245: INFO: mdcs-api-common-84c884c5c7-r4kt9 from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container mdcs-api-common ready: true, restart count 0
Sep 25 03:38:46.245: INFO: sonobuoy-e2e-job-a16116856b774ea6 from sonobuoy started at 2020-09-25 02:35:55 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container e2e ready: true, restart count 0
Sep 25 03:38:46.245: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 25 03:38:46.245: INFO: admin-api-common-596bf5fdb7-6pw8d from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container admin-api-common ready: true, restart count 0
Sep 25 03:38:46.245: INFO: portal-api-dock-5c5497b47f-8qjn6 from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container portal-api-dock ready: true, restart count 0
Sep 25 03:38:46.245: INFO: common-api-auth-c49dcbcc9-4xb9s from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container common-api-auth ready: true, restart count 0
Sep 25 03:38:46.245: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-pqlj2 from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 25 03:38:46.245: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:38:46.245: INFO: velero-kubeaddons-5d85fcdcb9-6lt97 from velero started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container velero ready: true, restart count 0
Sep 25 03:38:46.245: INFO: traefik-kubeaddons-68c579bbbd-2qmn4 from kubeaddons started at 2020-09-24 03:26:59 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Sep 25 03:38:46.245: INFO: store-api-appmanage-6b8b464c4c-m528t from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container store-api-appmanage ready: true, restart count 0
Sep 25 03:38:46.245: INFO: common-api-notify-5595dbd9b5-rb8vd from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container common-api-notify ready: true, restart count 0
Sep 25 03:38:46.245: INFO: kube-proxy-ff4h2 from kube-system started at 2020-09-24 03:18:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:38:46.245: INFO: dex-kubeaddons-58884f456-xtvhz from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container main ready: true, restart count 0
Sep 25 03:38:46.245: INFO: calico-node-k6ztw from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:38:46.245: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:38:46.245: INFO: calico-kube-controllers-77c79f7594-5qtpj from kube-system started at 2020-09-24 03:18:35 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep 25 03:38:46.245: INFO: kubernetes-dashboard-95959d56b-wmxhc from kubeaddons started at 2020-09-24 03:25:22 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Sep 25 03:38:46.245: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep 25 03:38:46.245: INFO: minio-3 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:38:46.245: INFO: local-volume-provisioner-rmgc9 from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:38:46.245: INFO: builder-daemon-main-fcb6b989c-hfwnc from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container builder-daemon-main ready: true, restart count 0
Sep 25 03:38:46.245: INFO: portal-cdn-download-5cd45dfd87-fqvrl from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container portal-cdn-download ready: false, restart count 20
Sep 25 03:38:46.245: INFO: sonobuoy from sonobuoy started at 2020-09-25 02:35:47 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 25 03:38:46.245: INFO: traefik-forward-auth-kubeaddons-69bbb98fb6-fdn8n from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container traefik-forward-auth ready: true, restart count 1
Sep 25 03:38:46.245: INFO: tiller-deploy-6cdf7f9d6f-5mjft from kube-system started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container tiller ready: true, restart count 0
Sep 25 03:38:46.245: INFO: common-api-account-6b7c79d795-d846m from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container common-api-account ready: true, restart count 0
Sep 25 03:38:46.245: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-q9x6d from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container kube-oidc-proxy ready: true, restart count 0
Sep 25 03:38:46.245: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-rw4pq from cert-manager started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.245: INFO: 	Container cert-manager ready: true, restart count 0
Sep 25 03:38:46.245: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-2 before test
Sep 25 03:38:46.272: INFO: workflow-api-draft-5899bb55c9-d4dhd from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container workflow-api-draft ready: true, restart count 0
Sep 25 03:38:46.272: INFO: calico-node-mj6vw from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:38:46.272: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:38:46.272: INFO: cert-manager-kubeaddons-cainjector-6dcd94769b-92k4x from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container cainjector ready: true, restart count 0
Sep 25 03:38:46.272: INFO: minio-2 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container minio ready: true, restart count 0
Sep 25 03:38:46.272: INFO: core-sockjs-79b764df9f-24fs4 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container core-sockjs ready: true, restart count 0
Sep 25 03:38:46.272: INFO: opsportal-kubeaddons-kommander-ui-6d64cc5d54-c8pct from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: true, restart count 0
Sep 25 03:38:46.272: INFO: kubeaddons-controller-manager-659bd9c576-jfg77 from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:38:46.272: INFO: core-router-57d6544c9-j52x2 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container core-router ready: true, restart count 0
Sep 25 03:38:46.272: INFO: workflow-api-common-76dd6fb889-gbn6z from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container workflow-api-common ready: true, restart count 0
Sep 25 03:38:46.272: INFO: workflow-daemon-draft-5955b46c89-t7p8l from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container workflow-daemon-draft ready: true, restart count 0
Sep 25 03:38:46.272: INFO: metallb-kubeaddons-speaker-pvqct from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:38:46.272: INFO: metallb-kubeaddons-controller-f84b74d86-8hz2m from kubeaddons started at 2020-09-24 03:25:21 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container controller ready: true, restart count 0
Sep 25 03:38:46.272: INFO: local-volume-provisioner-n7q6g from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:38:46.272: INFO: common-api-oauth-6cd64bdc5-nkwk5 from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container common-api-oauth ready: true, restart count 0
Sep 25 03:38:46.272: INFO: gatekeeper-kubeaddons-fdc87db85-9q9kx from kubeaddons started at 2020-09-24 03:26:34 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:38:46.272: INFO: portal-api-page-766cc7bf79-vcwbs from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container portal-api-page ready: true, restart count 0
Sep 25 03:38:46.272: INFO: store-api-bizgroup-695cc488b5-4pmrh from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container store-api-bizgroup ready: true, restart count 0
Sep 25 03:38:46.272: INFO: opsportal-landing-6f6865b688-v7dhz from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container opsportal-landing ready: true, restart count 0
Sep 25 03:38:46.272: INFO: builder-api-custom-676877997f-5vft8 from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.272: INFO: 	Container builder-api-custom ready: true, restart count 0
Sep 25 03:38:46.273: INFO: kube-proxy-5jx4d from kube-system started at 2020-09-24 03:18:13 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:38:46.273: INFO: reloader-kubeaddons-reloader-7c97f877cf-hpgvn from kubeaddons started at 2020-09-24 03:25:24 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container reloader-kubeaddons-reloader ready: true, restart count 0
Sep 25 03:38:46.273: INFO: traefik-kubeaddons-68c579bbbd-zwq9n from kubeaddons started at 2020-09-24 03:26:37 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container traefik-kubeaddons ready: true, restart count 0
Sep 25 03:38:46.273: INFO: store-api-common-bfd5d66c5-2wkcr from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container store-api-common ready: true, restart count 0
Sep 25 03:38:46.273: INFO: coredns-849d6f84b4-qdfhl from kube-system started at 2020-09-24 03:18:43 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container coredns ready: true, restart count 0
Sep 25 03:38:46.273: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-g6ss8 from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 25 03:38:46.273: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:38:46.273: INFO: daemon-task-6c8f8f5ffc-pbmqt from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container daemon-task ready: true, restart count 0
Sep 25 03:38:46.273: INFO: cert-manager-kubeaddons-7d7f98fbc6-fxzfx from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container cert-manager ready: true, restart count 0
Sep 25 03:38:46.273: INFO: dstorageclass-controller-manager-5c966c767f-ngg5l from kubeaddons started at 2020-09-24 03:26:27 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:38:46.273: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:38:46.273: INFO: daemon-account-866d44497f-5q7dl from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container daemon-account ready: true, restart count 0
Sep 25 03:38:46.273: INFO: dev-biz-portal-front-687d79db75-q4rwx from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container dev-biz-portal-front ready: false, restart count 20
Sep 25 03:38:46.273: INFO: portal-cdn-upload-567d9969fc-9b6dv from bizmicro started at 2020-09-25 02:56:23 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container portal-cdn-upload ready: false, restart count 12
Sep 25 03:38:46.273: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-qc42h from kubeaddons started at 2020-09-25 02:56:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.273: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:38:46.273: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:38:46.273: INFO: 
Logging pods the kubelet thinks is on node biz-k8s-node-3 before test
Sep 25 03:38:46.296: INFO: portal-cdn-upload-567d9969fc-d4sq2 from bizmicro started at 2020-09-25 02:13:11 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container portal-cdn-upload ready: false, restart count 12
Sep 25 03:38:46.296: INFO: sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-vbvdn from sonobuoy started at 2020-09-25 02:35:56 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 25 03:38:46.296: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 25 03:38:46.296: INFO: velero-kubeaddons-5d85fcdcb9-gwwkq from velero started at 2020-09-24 03:28:07 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container velero ready: true, restart count 3
Sep 25 03:38:46.296: INFO: dex-k8s-authenticator-kubeaddons-74966666f5-9qtxz from kubeaddons started at 2020-09-24 03:28:01 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container dex-k8s-authenticator ready: false, restart count 0
Sep 25 03:38:46.296: INFO: tiller-deploy-6cdf7f9d6f-qqfpj from kube-system started at 2020-09-24 03:25:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container tiller ready: false, restart count 0
Sep 25 03:38:46.296: INFO: dex-kubeaddons-58884f456-gtkb7 from kubeaddons started at 2020-09-24 03:27:50 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container main ready: true, restart count 1
Sep 25 03:38:46.296: INFO: traefik-forward-auth-kubeaddons-69bbb98fb6-g4fpr from kubeaddons started at 2020-09-24 03:28:06 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container traefik-forward-auth ready: false, restart count 0
Sep 25 03:38:46.296: INFO: builder-api-custom-676877997f-5dj9d from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container builder-api-custom ready: true, restart count 0
Sep 25 03:38:46.296: INFO: local-volume-provisioner-972q8 from kube-system started at 2020-09-24 03:26:42 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container local-volume-provisioner ready: true, restart count 0
Sep 25 03:38:46.296: INFO: kube-oidc-proxy-kubeaddons-6fbd5c8fc4-pzrmp from kubeaddons started at 2020-09-24 03:27:52 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container kube-oidc-proxy ready: false, restart count 0
Sep 25 03:38:46.296: INFO: pod-secrets-c04ed2b8-13ff-40aa-ac6c-ce4481640266 from secrets-9649 started at 2020-09-25 03:38:27 +0000 UTC (3 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container creates-volume-test ready: false, restart count 0
Sep 25 03:38:46.296: INFO: 	Container dels-volume-test ready: false, restart count 0
Sep 25 03:38:46.296: INFO: 	Container upds-volume-test ready: false, restart count 0
Sep 25 03:38:46.296: INFO: store-api-common-bfd5d66c5-wcnh9 from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container store-api-common ready: true, restart count 0
Sep 25 03:38:46.296: INFO: server-envvars-bcccf559-aebd-4464-8951-f9fecd34ac54 from pods-1469 started at 2020-09-25 03:38:35 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container srv ready: true, restart count 0
Sep 25 03:38:46.296: INFO: kube-proxy-lpjq2 from kube-system started at 2020-09-24 03:18:13 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 25 03:38:46.296: INFO: cert-manager-kubeaddons-webhook-77fbc6d59b-jvmxg from cert-manager started at 2020-09-24 03:25:53 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container cert-manager ready: true, restart count 1
Sep 25 03:38:46.296: INFO: opsportal-landing-6f6865b688-8sfw4 from kubeaddons started at 2020-09-24 03:25:44 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container opsportal-landing ready: true, restart count 0
Sep 25 03:38:46.296: INFO: admin-api-common-596bf5fdb7-dkf5t from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container admin-api-common ready: true, restart count 0
Sep 25 03:38:46.296: INFO: daemon-task-6c8f8f5ffc-wfsxt from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container daemon-task ready: true, restart count 0
Sep 25 03:38:46.296: INFO: calico-node-bzvbt from kube-system started at 2020-09-24 03:18:23 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container bird-metrics ready: true, restart count 0
Sep 25 03:38:46.296: INFO: 	Container calico-node ready: true, restart count 0
Sep 25 03:38:46.296: INFO: opsportal-kubeaddons-kommander-ui-6d64cc5d54-gxbm8 from kubeaddons started at 2020-09-24 03:25:44 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.296: INFO: 	Container opsportal-kubeaddons-kommander-ui ready: false, restart count 0
Sep 25 03:38:46.296: INFO: workflow-api-draft-5899bb55c9-j5dgh from bizmicro started at 2020-09-25 02:13:12 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.297: INFO: 	Container workflow-api-draft ready: true, restart count 0
Sep 25 03:38:46.297: INFO: busybox-readonly-false-4cf273e6-21d9-4f33-8f57-15e33f1442ec from security-context-test-6500 started at 2020-09-25 03:38:41 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.297: INFO: 	Container busybox-readonly-false-4cf273e6-21d9-4f33-8f57-15e33f1442ec ready: false, restart count 0
Sep 25 03:38:46.297: INFO: kubeaddons-controller-manager-659bd9c576-g2wjv from kubeaddons started at 2020-09-24 03:25:02 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.297: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:38:46.297: INFO: metallb-kubeaddons-speaker-lbzvt from kubeaddons started at 2020-09-25 03:16:37 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.297: INFO: 	Container speaker ready: true, restart count 0
Sep 25 03:38:46.297: INFO: portal-api-dock-5c5497b47f-6tctr from bizmicro started at 2020-09-25 02:13:10 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.297: INFO: 	Container portal-api-dock ready: true, restart count 0
Sep 25 03:38:46.297: INFO: dex-kubeaddons-dex-controller-7bd5fc575c-2cctd from kubeaddons started at 2020-09-24 03:27:26 +0000 UTC (2 container statuses recorded)
Sep 25 03:38:46.297: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 25 03:38:46.297: INFO: 	Container manager ready: true, restart count 0
Sep 25 03:38:46.297: INFO: minio-1 from velero started at 2020-09-24 03:28:08 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.297: INFO: 	Container minio ready: false, restart count 0
Sep 25 03:38:46.297: INFO: common-api-notify-5595dbd9b5-l4l6r from bizmicro started at 2020-09-25 02:13:09 +0000 UTC (1 container statuses recorded)
Sep 25 03:38:46.297: INFO: 	Container common-api-notify ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node biz-k8s-node-1
STEP: verifying the node has the label node biz-k8s-node-2
STEP: verifying the node has the label node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod admin-api-common-596bf5fdb7-6pw8d requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod admin-api-common-596bf5fdb7-dkf5t requesting resource cpu=1m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod builder-api-custom-676877997f-5dj9d requesting resource cpu=1m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod builder-api-custom-676877997f-5vft8 requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod builder-api-main-54dfc4474c-gnvlr requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod builder-daemon-main-fcb6b989c-hfwnc requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod common-api-account-6b7c79d795-d846m requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod common-api-auth-c49dcbcc9-4xb9s requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod common-api-notify-5595dbd9b5-l4l6r requesting resource cpu=1m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod common-api-notify-5595dbd9b5-rb8vd requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod common-api-oauth-6cd64bdc5-nkwk5 requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod core-router-57d6544c9-j52x2 requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod core-sockjs-79b764df9f-24fs4 requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod daemon-account-866d44497f-5q7dl requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod daemon-notify-74b7d7c4b5-zklwv requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod daemon-task-6c8f8f5ffc-pbmqt requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod daemon-task-6c8f8f5ffc-wfsxt requesting resource cpu=1m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod dev-biz-portal-front-687d79db75-q4rwx requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod mdcs-api-common-84c884c5c7-r4kt9 requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod portal-api-dock-5c5497b47f-6tctr requesting resource cpu=1m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod portal-api-dock-5c5497b47f-8qjn6 requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod portal-api-page-766cc7bf79-vcwbs requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod portal-cdn-download-5cd45dfd87-fqvrl requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod portal-cdn-upload-567d9969fc-9b6dv requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod portal-cdn-upload-567d9969fc-d4sq2 requesting resource cpu=1m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod store-api-appmanage-6b8b464c4c-m528t requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod store-api-bizgroup-695cc488b5-4pmrh requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod store-api-common-bfd5d66c5-2wkcr requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod store-api-common-bfd5d66c5-wcnh9 requesting resource cpu=1m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod store-api-mypage-5d6d59c499-dvczz requesting resource cpu=1m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod workflow-api-common-76dd6fb889-gbn6z requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod workflow-api-draft-5899bb55c9-d4dhd requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod workflow-api-draft-5899bb55c9-j5dgh requesting resource cpu=1m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod workflow-daemon-draft-5955b46c89-t7p8l requesting resource cpu=1m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod cert-manager-kubeaddons-7d7f98fbc6-fxzfx requesting resource cpu=0m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod cert-manager-kubeaddons-cainjector-6dcd94769b-92k4x requesting resource cpu=0m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod cert-manager-kubeaddons-webhook-77fbc6d59b-jvmxg requesting resource cpu=0m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod cert-manager-kubeaddons-webhook-77fbc6d59b-rw4pq requesting resource cpu=0m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod calico-kube-controllers-77c79f7594-5qtpj requesting resource cpu=30m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod calico-node-bzvbt requesting resource cpu=300m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod calico-node-k6ztw requesting resource cpu=300m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod calico-node-mj6vw requesting resource cpu=300m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod coredns-849d6f84b4-cc8tj requesting resource cpu=100m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod coredns-849d6f84b4-qdfhl requesting resource cpu=100m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod kube-proxy-5jx4d requesting resource cpu=0m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod kube-proxy-ff4h2 requesting resource cpu=0m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod kube-proxy-lpjq2 requesting resource cpu=0m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod local-volume-provisioner-972q8 requesting resource cpu=0m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod local-volume-provisioner-n7q6g requesting resource cpu=0m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod local-volume-provisioner-rmgc9 requesting resource cpu=0m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod tiller-deploy-6cdf7f9d6f-5mjft requesting resource cpu=0m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod tiller-deploy-6cdf7f9d6f-qqfpj requesting resource cpu=0m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod dex-k8s-authenticator-kubeaddons-74966666f5-9qtxz requesting resource cpu=100m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod dex-k8s-authenticator-kubeaddons-74966666f5-gvf92 requesting resource cpu=100m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod dex-kubeaddons-58884f456-gtkb7 requesting resource cpu=100m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod dex-kubeaddons-58884f456-xtvhz requesting resource cpu=100m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod dex-kubeaddons-dex-controller-7bd5fc575c-2cctd requesting resource cpu=100m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod dex-kubeaddons-dex-controller-7bd5fc575c-qc42h requesting resource cpu=100m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod dstorageclass-controller-manager-5c966c767f-ngg5l requesting resource cpu=0m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod gatekeeper-kubeaddons-fdc87db85-9q9kx requesting resource cpu=200m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod kube-oidc-proxy-kubeaddons-6fbd5c8fc4-pzrmp requesting resource cpu=0m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod kube-oidc-proxy-kubeaddons-6fbd5c8fc4-q9x6d requesting resource cpu=0m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod kubeaddons-controller-manager-659bd9c576-g2wjv requesting resource cpu=100m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod kubeaddons-controller-manager-659bd9c576-jfg77 requesting resource cpu=100m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod kubernetes-dashboard-95959d56b-wmxhc requesting resource cpu=250m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod metallb-kubeaddons-controller-f84b74d86-8hz2m requesting resource cpu=0m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod metallb-kubeaddons-speaker-cf5wv requesting resource cpu=0m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod metallb-kubeaddons-speaker-lbzvt requesting resource cpu=0m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod metallb-kubeaddons-speaker-pvqct requesting resource cpu=0m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod opsportal-kubeaddons-kommander-ui-6d64cc5d54-c8pct requesting resource cpu=100m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod opsportal-kubeaddons-kommander-ui-6d64cc5d54-gxbm8 requesting resource cpu=100m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod opsportal-landing-6f6865b688-8sfw4 requesting resource cpu=100m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod opsportal-landing-6f6865b688-v7dhz requesting resource cpu=100m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod reloader-kubeaddons-reloader-7c97f877cf-hpgvn requesting resource cpu=100m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod traefik-forward-auth-kubeaddons-69bbb98fb6-fdn8n requesting resource cpu=100m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod traefik-forward-auth-kubeaddons-69bbb98fb6-g4fpr requesting resource cpu=100m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod traefik-kubeaddons-68c579bbbd-2qmn4 requesting resource cpu=500m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod traefik-kubeaddons-68c579bbbd-zwq9n requesting resource cpu=500m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod server-envvars-bcccf559-aebd-4464-8951-f9fecd34ac54 requesting resource cpu=0m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod sonobuoy requesting resource cpu=0m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod sonobuoy-e2e-job-a16116856b774ea6 requesting resource cpu=0m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-g6ss8 requesting resource cpu=0m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-pqlj2 requesting resource cpu=0m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod sonobuoy-systemd-logs-daemon-set-0f6311edd7f949ab-vbvdn requesting resource cpu=0m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod minio-0 requesting resource cpu=250m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod minio-1 requesting resource cpu=250m on Node biz-k8s-node-3
Sep 25 03:38:46.354: INFO: Pod minio-2 requesting resource cpu=250m on Node biz-k8s-node-2
Sep 25 03:38:46.354: INFO: Pod minio-3 requesting resource cpu=250m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod velero-kubeaddons-5d85fcdcb9-6lt97 requesting resource cpu=0m on Node biz-k8s-node-1
Sep 25 03:38:46.354: INFO: Pod velero-kubeaddons-5d85fcdcb9-gwwkq requesting resource cpu=0m on Node biz-k8s-node-3
STEP: Starting Pods to consume most of the cluster CPU.
Sep 25 03:38:46.354: INFO: Creating a pod which consumes cpu=4205m on Node biz-k8s-node-1
Sep 25 03:38:46.362: INFO: Creating a pod which consumes cpu=4295m on Node biz-k8s-node-2
Sep 25 03:38:46.368: INFO: Creating a pod which consumes cpu=4719m on Node biz-k8s-node-3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24576e14-cd37-445f-bd96-db6cef2f7e44.1637e9ae10a35e4d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9333/filler-pod-24576e14-cd37-445f-bd96-db6cef2f7e44 to biz-k8s-node-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24576e14-cd37-445f-bd96-db6cef2f7e44.1637e9ae35edefba], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24576e14-cd37-445f-bd96-db6cef2f7e44.1637e9ae52123e27], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24576e14-cd37-445f-bd96-db6cef2f7e44.1637e9ae542b4adb], Reason = [Created], Message = [Created container filler-pod-24576e14-cd37-445f-bd96-db6cef2f7e44]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24576e14-cd37-445f-bd96-db6cef2f7e44.1637e9ae5cdbe7b9], Reason = [Started], Message = [Started container filler-pod-24576e14-cd37-445f-bd96-db6cef2f7e44]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7b34485-cc55-4e5c-bd31-d56a9f1af12a.1637e9ae104efc36], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9333/filler-pod-a7b34485-cc55-4e5c-bd31-d56a9f1af12a to biz-k8s-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7b34485-cc55-4e5c-bd31-d56a9f1af12a.1637e9ae341535a5], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7b34485-cc55-4e5c-bd31-d56a9f1af12a.1637e9ae52540232], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7b34485-cc55-4e5c-bd31-d56a9f1af12a.1637e9ae53ef6651], Reason = [Created], Message = [Created container filler-pod-a7b34485-cc55-4e5c-bd31-d56a9f1af12a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a7b34485-cc55-4e5c-bd31-d56a9f1af12a.1637e9ae5b270b43], Reason = [Started], Message = [Started container filler-pod-a7b34485-cc55-4e5c-bd31-d56a9f1af12a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6111e93-64c5-472c-844f-d913339a26ff.1637e9ae110a3032], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9333/filler-pod-f6111e93-64c5-472c-844f-d913339a26ff to biz-k8s-node-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6111e93-64c5-472c-844f-d913339a26ff.1637e9ae344d2e60], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6111e93-64c5-472c-844f-d913339a26ff.1637e9ae52376fd8], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6111e93-64c5-472c-844f-d913339a26ff.1637e9ae54712e04], Reason = [Created], Message = [Created container filler-pod-f6111e93-64c5-472c-844f-d913339a26ff]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6111e93-64c5-472c-844f-d913339a26ff.1637e9ae5c57102f], Reason = [Started], Message = [Started container filler-pod-f6111e93-64c5-472c-844f-d913339a26ff]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1637e9ae89e84ac5], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node biz-k8s-node-3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node biz-k8s-node-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node biz-k8s-node-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:49.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9333" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":243,"skipped":3853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:49.449: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1276
STEP: creating the pod
Sep 25 03:38:49.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-3503'
Sep 25 03:38:49.835: INFO: stderr: ""
Sep 25 03:38:49.835: INFO: stdout: "pod/pause created\n"
Sep 25 03:38:49.835: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 25 03:38:49.835: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3503" to be "running and ready"
Sep 25 03:38:49.838: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980821ms
Sep 25 03:38:51.842: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006911597s
Sep 25 03:38:51.842: INFO: Pod "pause" satisfied condition "running and ready"
Sep 25 03:38:51.842: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 25 03:38:51.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 label pods pause testing-label=testing-label-value --namespace=kubectl-3503'
Sep 25 03:38:51.959: INFO: stderr: ""
Sep 25 03:38:51.959: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 25 03:38:51.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pod pause -L testing-label --namespace=kubectl-3503'
Sep 25 03:38:52.043: INFO: stderr: ""
Sep 25 03:38:52.043: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 25 03:38:52.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 label pods pause testing-label- --namespace=kubectl-3503'
Sep 25 03:38:52.137: INFO: stderr: ""
Sep 25 03:38:52.137: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 25 03:38:52.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pod pause -L testing-label --namespace=kubectl-3503'
Sep 25 03:38:52.227: INFO: stderr: ""
Sep 25 03:38:52.227: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1283
STEP: using delete to clean up resources
Sep 25 03:38:52.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete --grace-period=0 --force -f - --namespace=kubectl-3503'
Sep 25 03:38:52.328: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 25 03:38:52.328: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 25 03:38:52.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get rc,svc -l name=pause --no-headers --namespace=kubectl-3503'
Sep 25 03:38:52.423: INFO: stderr: "No resources found in kubectl-3503 namespace.\n"
Sep 25 03:38:52.423: INFO: stdout: ""
Sep 25 03:38:52.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -l name=pause --namespace=kubectl-3503 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 25 03:38:52.525: INFO: stderr: ""
Sep 25 03:38:52.525: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:52.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3503" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":244,"skipped":3914,"failed":0}
SS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:52.532: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:38:52.671: INFO: Waiting up to 5m0s for pod "busybox-user-65534-80b68d56-5d88-4e7b-8900-f5162d137ff3" in namespace "security-context-test-3078" to be "success or failure"
Sep 25 03:38:52.675: INFO: Pod "busybox-user-65534-80b68d56-5d88-4e7b-8900-f5162d137ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.261517ms
Sep 25 03:38:54.680: INFO: Pod "busybox-user-65534-80b68d56-5d88-4e7b-8900-f5162d137ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008556835s
Sep 25 03:38:56.683: INFO: Pod "busybox-user-65534-80b68d56-5d88-4e7b-8900-f5162d137ff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011419039s
Sep 25 03:38:56.683: INFO: Pod "busybox-user-65534-80b68d56-5d88-4e7b-8900-f5162d137ff3" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:38:56.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3078" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":3916,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:38:56.690: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7250
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Sep 25 03:39:01.375: INFO: Successfully updated pod "labelsupdate4d1347eb-689f-4f7d-b0b9-c94f31f7f84e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:39:03.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7250" for this suite.

â€¢ [SLOW TEST:6.709 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":246,"skipped":3939,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:39:03.400: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5845
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-034f9c01-d40c-4bdb-973c-0a2061c6f21b
STEP: Creating a pod to test consume configMaps
Sep 25 03:39:03.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-f2179f04-4ae6-4b41-8123-4aae3f7eaf72" in namespace "configmap-5845" to be "success or failure"
Sep 25 03:39:03.552: INFO: Pod "pod-configmaps-f2179f04-4ae6-4b41-8123-4aae3f7eaf72": Phase="Pending", Reason="", readiness=false. Elapsed: 3.031997ms
Sep 25 03:39:05.562: INFO: Pod "pod-configmaps-f2179f04-4ae6-4b41-8123-4aae3f7eaf72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012978254s
STEP: Saw pod success
Sep 25 03:39:05.562: INFO: Pod "pod-configmaps-f2179f04-4ae6-4b41-8123-4aae3f7eaf72" satisfied condition "success or failure"
Sep 25 03:39:05.565: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-configmaps-f2179f04-4ae6-4b41-8123-4aae3f7eaf72 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:39:05.576: INFO: Waiting for pod pod-configmaps-f2179f04-4ae6-4b41-8123-4aae3f7eaf72 to disappear
Sep 25 03:39:05.580: INFO: Pod pod-configmaps-f2179f04-4ae6-4b41-8123-4aae3f7eaf72 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:39:05.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5845" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":247,"skipped":3956,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:39:05.594: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8100
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1526
[It] should create an rc from an image [Deprecated] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 25 03:39:05.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8100'
Sep 25 03:39:05.824: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 25 03:39:05.824: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Sep 25 03:39:05.832: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-gcb2m]
Sep 25 03:39:05.832: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-gcb2m" in namespace "kubectl-8100" to be "running and ready"
Sep 25 03:39:05.833: INFO: Pod "e2e-test-httpd-rc-gcb2m": Phase="Pending", Reason="", readiness=false. Elapsed: 1.785112ms
Sep 25 03:39:07.838: INFO: Pod "e2e-test-httpd-rc-gcb2m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006339255s
Sep 25 03:39:09.842: INFO: Pod "e2e-test-httpd-rc-gcb2m": Phase="Running", Reason="", readiness=true. Elapsed: 4.010184929s
Sep 25 03:39:09.842: INFO: Pod "e2e-test-httpd-rc-gcb2m" satisfied condition "running and ready"
Sep 25 03:39:09.842: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-gcb2m]
Sep 25 03:39:09.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 logs rc/e2e-test-httpd-rc --namespace=kubectl-8100'
Sep 25 03:39:10.000: INFO: stderr: ""
Sep 25 03:39:10.000: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.100.80. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.100.80. Set the 'ServerName' directive globally to suppress this message\n[Fri Sep 25 03:39:08.526303 2020] [mpm_event:notice] [pid 1:tid 140567072082792] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Fri Sep 25 03:39:08.526360 2020] [core:notice] [pid 1:tid 140567072082792] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1531
Sep 25 03:39:10.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete rc e2e-test-httpd-rc --namespace=kubectl-8100'
Sep 25 03:39:10.114: INFO: stderr: ""
Sep 25 03:39:10.114: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:39:10.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8100" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image [Deprecated] [Conformance]","total":280,"completed":248,"skipped":3974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:39:10.122: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-996c8e44-f9fc-4a38-9f99-169c96d16c95
STEP: Creating a pod to test consume configMaps
Sep 25 03:39:10.267: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-626ac0d8-c979-407b-9836-9ed023f7b22f" in namespace "projected-8062" to be "success or failure"
Sep 25 03:39:10.269: INFO: Pod "pod-projected-configmaps-626ac0d8-c979-407b-9836-9ed023f7b22f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.917835ms
Sep 25 03:39:12.273: INFO: Pod "pod-projected-configmaps-626ac0d8-c979-407b-9836-9ed023f7b22f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006145591s
STEP: Saw pod success
Sep 25 03:39:12.274: INFO: Pod "pod-projected-configmaps-626ac0d8-c979-407b-9836-9ed023f7b22f" satisfied condition "success or failure"
Sep 25 03:39:12.277: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-projected-configmaps-626ac0d8-c979-407b-9836-9ed023f7b22f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:39:12.293: INFO: Waiting for pod pod-projected-configmaps-626ac0d8-c979-407b-9836-9ed023f7b22f to disappear
Sep 25 03:39:12.295: INFO: Pod pod-projected-configmaps-626ac0d8-c979-407b-9836-9ed023f7b22f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:39:12.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8062" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":249,"skipped":4052,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:39:12.302: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2918
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:39:12.448: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 25 03:39:12.454: INFO: Number of nodes with available pods: 0
Sep 25 03:39:12.454: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 25 03:39:12.466: INFO: Number of nodes with available pods: 0
Sep 25 03:39:12.466: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:13.471: INFO: Number of nodes with available pods: 0
Sep 25 03:39:13.471: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:14.471: INFO: Number of nodes with available pods: 0
Sep 25 03:39:14.471: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:15.474: INFO: Number of nodes with available pods: 0
Sep 25 03:39:15.474: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:16.471: INFO: Number of nodes with available pods: 1
Sep 25 03:39:16.471: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 25 03:39:16.490: INFO: Number of nodes with available pods: 1
Sep 25 03:39:16.490: INFO: Number of running nodes: 0, number of available pods: 1
Sep 25 03:39:17.493: INFO: Number of nodes with available pods: 0
Sep 25 03:39:17.493: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 25 03:39:17.499: INFO: Number of nodes with available pods: 0
Sep 25 03:39:17.500: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:18.504: INFO: Number of nodes with available pods: 0
Sep 25 03:39:18.504: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:19.503: INFO: Number of nodes with available pods: 0
Sep 25 03:39:19.503: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:20.503: INFO: Number of nodes with available pods: 0
Sep 25 03:39:20.503: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:21.502: INFO: Number of nodes with available pods: 0
Sep 25 03:39:21.503: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:22.503: INFO: Number of nodes with available pods: 0
Sep 25 03:39:22.503: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:23.503: INFO: Number of nodes with available pods: 0
Sep 25 03:39:23.503: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:24.504: INFO: Number of nodes with available pods: 0
Sep 25 03:39:24.504: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:25.504: INFO: Number of nodes with available pods: 0
Sep 25 03:39:25.504: INFO: Node biz-k8s-node-1 is running more than one daemon pod
Sep 25 03:39:26.505: INFO: Number of nodes with available pods: 1
Sep 25 03:39:26.505: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2918, will wait for the garbage collector to delete the pods
Sep 25 03:39:26.570: INFO: Deleting DaemonSet.extensions daemon-set took: 6.087054ms
Sep 25 03:39:27.471: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.363578ms
Sep 25 03:39:32.974: INFO: Number of nodes with available pods: 0
Sep 25 03:39:32.974: INFO: Number of running nodes: 0, number of available pods: 0
Sep 25 03:39:32.977: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2918/daemonsets","resourceVersion":"437681"},"items":null}

Sep 25 03:39:32.980: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2918/pods","resourceVersion":"437681"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:39:32.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2918" for this suite.

â€¢ [SLOW TEST:20.702 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":250,"skipped":4080,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:39:33.004: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-f9156954-5df4-45cc-bf72-da930b7bc2b3
STEP: Creating a pod to test consume secrets
Sep 25 03:39:33.154: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b77f1bcc-6f8e-4ff0-9eb3-310dc3d1af73" in namespace "projected-8626" to be "success or failure"
Sep 25 03:39:33.158: INFO: Pod "pod-projected-secrets-b77f1bcc-6f8e-4ff0-9eb3-310dc3d1af73": Phase="Pending", Reason="", readiness=false. Elapsed: 3.773118ms
Sep 25 03:39:35.162: INFO: Pod "pod-projected-secrets-b77f1bcc-6f8e-4ff0-9eb3-310dc3d1af73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007813278s
Sep 25 03:39:37.166: INFO: Pod "pod-projected-secrets-b77f1bcc-6f8e-4ff0-9eb3-310dc3d1af73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012129725s
STEP: Saw pod success
Sep 25 03:39:37.167: INFO: Pod "pod-projected-secrets-b77f1bcc-6f8e-4ff0-9eb3-310dc3d1af73" satisfied condition "success or failure"
Sep 25 03:39:37.169: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-projected-secrets-b77f1bcc-6f8e-4ff0-9eb3-310dc3d1af73 container secret-volume-test: <nil>
STEP: delete the pod
Sep 25 03:39:37.186: INFO: Waiting for pod pod-projected-secrets-b77f1bcc-6f8e-4ff0-9eb3-310dc3d1af73 to disappear
Sep 25 03:39:37.188: INFO: Pod pod-projected-secrets-b77f1bcc-6f8e-4ff0-9eb3-310dc3d1af73 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:39:37.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8626" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":251,"skipped":4082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:39:37.196: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:39:37.847: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:39:39.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601977, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601977, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601977, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736601977, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:39:42.870: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:39:42.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8500" for this suite.
STEP: Destroying namespace "webhook-8500-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.778 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":252,"skipped":4124,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:39:42.974: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4048
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-4048/configmap-test-f88761a0-203a-42c6-93a1-3a218ab80e10
STEP: Creating a pod to test consume configMaps
Sep 25 03:39:43.118: INFO: Waiting up to 5m0s for pod "pod-configmaps-15ab77a4-bd04-4d95-a39e-f3fce6c5645e" in namespace "configmap-4048" to be "success or failure"
Sep 25 03:39:43.123: INFO: Pod "pod-configmaps-15ab77a4-bd04-4d95-a39e-f3fce6c5645e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.804285ms
Sep 25 03:39:45.127: INFO: Pod "pod-configmaps-15ab77a4-bd04-4d95-a39e-f3fce6c5645e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008764712s
Sep 25 03:39:47.131: INFO: Pod "pod-configmaps-15ab77a4-bd04-4d95-a39e-f3fce6c5645e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012560338s
STEP: Saw pod success
Sep 25 03:39:47.131: INFO: Pod "pod-configmaps-15ab77a4-bd04-4d95-a39e-f3fce6c5645e" satisfied condition "success or failure"
Sep 25 03:39:47.134: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-configmaps-15ab77a4-bd04-4d95-a39e-f3fce6c5645e container env-test: <nil>
STEP: delete the pod
Sep 25 03:39:47.153: INFO: Waiting for pod pod-configmaps-15ab77a4-bd04-4d95-a39e-f3fce6c5645e to disappear
Sep 25 03:39:47.154: INFO: Pod pod-configmaps-15ab77a4-bd04-4d95-a39e-f3fce6c5645e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:39:47.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4048" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":253,"skipped":4125,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:39:47.161: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4877
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4877
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-4877
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4877
Sep 25 03:39:47.299: INFO: Found 0 stateful pods, waiting for 1
Sep 25 03:39:57.304: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 25 03:39:57.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-4877 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 25 03:39:57.539: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 25 03:39:57.539: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 25 03:39:57.539: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 25 03:39:57.543: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 25 03:40:07.548: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 25 03:40:07.548: INFO: Waiting for statefulset status.replicas updated to 0
Sep 25 03:40:07.562: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 25 03:40:07.562: INFO: ss-0  biz-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  }]
Sep 25 03:40:07.562: INFO: 
Sep 25 03:40:07.562: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 25 03:40:08.566: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997211217s
Sep 25 03:40:09.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992705641s
Sep 25 03:40:10.575: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988046462s
Sep 25 03:40:11.579: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983739606s
Sep 25 03:40:12.583: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979731591s
Sep 25 03:40:13.587: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975876746s
Sep 25 03:40:14.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971818742s
Sep 25 03:40:15.595: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967224549s
Sep 25 03:40:16.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.860533ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4877
Sep 25 03:40:17.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-4877 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 25 03:40:17.834: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 25 03:40:17.834: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 25 03:40:17.834: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 25 03:40:17.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-4877 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 25 03:40:18.062: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 25 03:40:18.062: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 25 03:40:18.062: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 25 03:40:18.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-4877 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 25 03:40:18.245: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 25 03:40:18.245: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 25 03:40:18.245: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 25 03:40:18.249: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep 25 03:40:28.253: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:40:28.253: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:40:28.253: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 25 03:40:28.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-4877 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 25 03:40:28.448: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 25 03:40:28.448: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 25 03:40:28.449: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 25 03:40:28.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-4877 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 25 03:40:28.652: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 25 03:40:28.652: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 25 03:40:28.652: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 25 03:40:28.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 exec --namespace=statefulset-4877 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 25 03:40:28.846: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 25 03:40:28.846: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 25 03:40:28.846: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 25 03:40:28.846: INFO: Waiting for statefulset status.replicas updated to 0
Sep 25 03:40:28.849: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 25 03:40:38.859: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 25 03:40:38.859: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 25 03:40:38.859: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 25 03:40:38.869: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 25 03:40:38.869: INFO: ss-0  biz-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  }]
Sep 25 03:40:38.870: INFO: ss-1  biz-k8s-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  }]
Sep 25 03:40:38.870: INFO: ss-2  biz-k8s-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  }]
Sep 25 03:40:38.870: INFO: 
Sep 25 03:40:38.870: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 25 03:40:39.873: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 25 03:40:39.873: INFO: ss-0  biz-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  }]
Sep 25 03:40:39.873: INFO: ss-1  biz-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  }]
Sep 25 03:40:39.873: INFO: ss-2  biz-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  }]
Sep 25 03:40:39.873: INFO: 
Sep 25 03:40:39.873: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 25 03:40:40.876: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 25 03:40:40.876: INFO: ss-0  biz-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  }]
Sep 25 03:40:40.876: INFO: ss-1  biz-k8s-node-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  }]
Sep 25 03:40:40.876: INFO: ss-2  biz-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  }]
Sep 25 03:40:40.876: INFO: 
Sep 25 03:40:40.876: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 25 03:40:41.880: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 25 03:40:41.880: INFO: ss-0  biz-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  }]
Sep 25 03:40:41.880: INFO: ss-1  biz-k8s-node-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  }]
Sep 25 03:40:41.880: INFO: ss-2  biz-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  }]
Sep 25 03:40:41.880: INFO: 
Sep 25 03:40:41.880: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 25 03:40:42.885: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Sep 25 03:40:42.885: INFO: ss-0  biz-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:39:47 +0000 UTC  }]
Sep 25 03:40:42.885: INFO: ss-1  biz-k8s-node-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  }]
Sep 25 03:40:42.885: INFO: ss-2  biz-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-09-25 03:40:07 +0000 UTC  }]
Sep 25 03:40:42.885: INFO: 
Sep 25 03:40:42.885: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 25 03:40:43.890: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.980732814s
Sep 25 03:40:44.894: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.976061547s
Sep 25 03:40:45.899: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.971711185s
Sep 25 03:40:46.912: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.967230808s
Sep 25 03:40:47.916: INFO: Verifying statefulset ss doesn't scale past 0 for another 954.501799ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4877
Sep 25 03:40:48.920: INFO: Scaling statefulset ss to 0
Sep 25 03:40:48.930: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Sep 25 03:40:48.932: INFO: Deleting all statefulset in ns statefulset-4877
Sep 25 03:40:48.935: INFO: Scaling statefulset ss to 0
Sep 25 03:40:48.942: INFO: Waiting for statefulset status.replicas updated to 0
Sep 25 03:40:48.945: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:40:48.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4877" for this suite.

â€¢ [SLOW TEST:61.802 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":254,"skipped":4153,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:40:48.963: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-188
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-188
STEP: Deleting pre-stop pod
Sep 25 03:41:02.145: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:41:02.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-188" for this suite.

â€¢ [SLOW TEST:13.199 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":255,"skipped":4157,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:41:02.162: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6367
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-d98e68b0-dcf1-4aa2-a62e-58b351803ccb in namespace container-probe-6367
Sep 25 03:41:06.313: INFO: Started pod busybox-d98e68b0-dcf1-4aa2-a62e-58b351803ccb in namespace container-probe-6367
STEP: checking the pod's current state and verifying that restartCount is present
Sep 25 03:41:06.316: INFO: Initial restart count of pod busybox-d98e68b0-dcf1-4aa2-a62e-58b351803ccb is 0
Sep 25 03:41:54.424: INFO: Restart count of pod container-probe-6367/busybox-d98e68b0-dcf1-4aa2-a62e-58b351803ccb is now 1 (48.107352831s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:41:54.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6367" for this suite.

â€¢ [SLOW TEST:52.280 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":256,"skipped":4164,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:41:54.443: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2908
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-mlnv
STEP: Creating a pod to test atomic-volume-subpath
Sep 25 03:41:54.590: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mlnv" in namespace "subpath-2908" to be "success or failure"
Sep 25 03:41:54.594: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.083587ms
Sep 25 03:41:56.598: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007027551s
Sep 25 03:41:58.601: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Running", Reason="", readiness=true. Elapsed: 4.010478297s
Sep 25 03:42:00.605: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Running", Reason="", readiness=true. Elapsed: 6.014492639s
Sep 25 03:42:02.609: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Running", Reason="", readiness=true. Elapsed: 8.018642093s
Sep 25 03:42:04.613: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Running", Reason="", readiness=true. Elapsed: 10.022989659s
Sep 25 03:42:06.617: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Running", Reason="", readiness=true. Elapsed: 12.026250378s
Sep 25 03:42:08.621: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Running", Reason="", readiness=true. Elapsed: 14.030317118s
Sep 25 03:42:10.625: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Running", Reason="", readiness=true. Elapsed: 16.034702436s
Sep 25 03:42:12.629: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Running", Reason="", readiness=true. Elapsed: 18.038989999s
Sep 25 03:42:14.635: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Running", Reason="", readiness=true. Elapsed: 20.044125021s
Sep 25 03:42:16.642: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Running", Reason="", readiness=true. Elapsed: 22.051295701s
Sep 25 03:42:18.646: INFO: Pod "pod-subpath-test-configmap-mlnv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0555395s
STEP: Saw pod success
Sep 25 03:42:18.646: INFO: Pod "pod-subpath-test-configmap-mlnv" satisfied condition "success or failure"
Sep 25 03:42:18.649: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-subpath-test-configmap-mlnv container test-container-subpath-configmap-mlnv: <nil>
STEP: delete the pod
Sep 25 03:42:18.675: INFO: Waiting for pod pod-subpath-test-configmap-mlnv to disappear
Sep 25 03:42:18.679: INFO: Pod pod-subpath-test-configmap-mlnv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mlnv
Sep 25 03:42:18.680: INFO: Deleting pod "pod-subpath-test-configmap-mlnv" in namespace "subpath-2908"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:42:18.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2908" for this suite.

â€¢ [SLOW TEST:24.246 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":257,"skipped":4178,"failed":0}
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:42:18.689: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Sep 25 03:42:18.830: INFO: Waiting up to 5m0s for pod "var-expansion-0902680a-1f4c-45ac-bbc3-82f2b180a313" in namespace "var-expansion-5811" to be "success or failure"
Sep 25 03:42:18.834: INFO: Pod "var-expansion-0902680a-1f4c-45ac-bbc3-82f2b180a313": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090619ms
Sep 25 03:42:20.838: INFO: Pod "var-expansion-0902680a-1f4c-45ac-bbc3-82f2b180a313": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008391782s
Sep 25 03:42:22.843: INFO: Pod "var-expansion-0902680a-1f4c-45ac-bbc3-82f2b180a313": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013049191s
STEP: Saw pod success
Sep 25 03:42:22.843: INFO: Pod "var-expansion-0902680a-1f4c-45ac-bbc3-82f2b180a313" satisfied condition "success or failure"
Sep 25 03:42:22.846: INFO: Trying to get logs from node biz-k8s-node-3 pod var-expansion-0902680a-1f4c-45ac-bbc3-82f2b180a313 container dapi-container: <nil>
STEP: delete the pod
Sep 25 03:42:22.864: INFO: Waiting for pod var-expansion-0902680a-1f4c-45ac-bbc3-82f2b180a313 to disappear
Sep 25 03:42:22.866: INFO: Pod var-expansion-0902680a-1f4c-45ac-bbc3-82f2b180a313 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:42:22.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5811" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":258,"skipped":4178,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:42:22.874: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5561.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5561.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5561.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5561.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5561.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5561.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5561.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5561.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5561.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5561.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 25 03:42:27.040: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local from pod dns-5561/dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9: the server could not find the requested resource (get pods dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9)
Sep 25 03:42:27.044: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local from pod dns-5561/dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9: the server could not find the requested resource (get pods dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9)
Sep 25 03:42:27.047: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5561.svc.cluster.local from pod dns-5561/dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9: the server could not find the requested resource (get pods dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9)
Sep 25 03:42:27.051: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5561.svc.cluster.local from pod dns-5561/dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9: the server could not find the requested resource (get pods dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9)
Sep 25 03:42:27.059: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local from pod dns-5561/dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9: the server could not find the requested resource (get pods dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9)
Sep 25 03:42:27.062: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local from pod dns-5561/dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9: the server could not find the requested resource (get pods dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9)
Sep 25 03:42:27.064: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5561.svc.cluster.local from pod dns-5561/dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9: the server could not find the requested resource (get pods dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9)
Sep 25 03:42:27.067: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5561.svc.cluster.local from pod dns-5561/dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9: the server could not find the requested resource (get pods dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9)
Sep 25 03:42:27.072: INFO: Lookups using dns-5561/dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5561.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5561.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5561.svc.cluster.local jessie_udp@dns-test-service-2.dns-5561.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5561.svc.cluster.local]

Sep 25 03:42:32.121: INFO: DNS probes using dns-5561/dns-test-f19a6d4f-c3c4-4991-9c94-c3a2e04124d9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:42:32.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5561" for this suite.

â€¢ [SLOW TEST:9.277 seconds]
[sig-network] DNS
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":259,"skipped":4185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:42:32.151: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8312
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e518f880-9027-4fbe-899b-3abd6c416227
STEP: Creating a pod to test consume secrets
Sep 25 03:42:32.294: INFO: Waiting up to 5m0s for pod "pod-secrets-3410843f-4c04-4dbc-a46c-c3fdf2914652" in namespace "secrets-8312" to be "success or failure"
Sep 25 03:42:32.297: INFO: Pod "pod-secrets-3410843f-4c04-4dbc-a46c-c3fdf2914652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546179ms
Sep 25 03:42:34.302: INFO: Pod "pod-secrets-3410843f-4c04-4dbc-a46c-c3fdf2914652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007130396s
Sep 25 03:42:36.306: INFO: Pod "pod-secrets-3410843f-4c04-4dbc-a46c-c3fdf2914652": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01130482s
STEP: Saw pod success
Sep 25 03:42:36.306: INFO: Pod "pod-secrets-3410843f-4c04-4dbc-a46c-c3fdf2914652" satisfied condition "success or failure"
Sep 25 03:42:36.309: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-secrets-3410843f-4c04-4dbc-a46c-c3fdf2914652 container secret-env-test: <nil>
STEP: delete the pod
Sep 25 03:42:36.326: INFO: Waiting for pod pod-secrets-3410843f-4c04-4dbc-a46c-c3fdf2914652 to disappear
Sep 25 03:42:36.329: INFO: Pod pod-secrets-3410843f-4c04-4dbc-a46c-c3fdf2914652 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:42:36.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8312" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":260,"skipped":4214,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:42:36.337: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-a7ca6ddc-95e6-4c76-9604-fcf0f527f453
STEP: Creating a pod to test consume configMaps
Sep 25 03:42:36.484: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3932f616-b092-4168-b299-b4c8d6f0a4f2" in namespace "projected-463" to be "success or failure"
Sep 25 03:42:36.488: INFO: Pod "pod-projected-configmaps-3932f616-b092-4168-b299-b4c8d6f0a4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.058353ms
Sep 25 03:42:38.492: INFO: Pod "pod-projected-configmaps-3932f616-b092-4168-b299-b4c8d6f0a4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007226964s
Sep 25 03:42:40.496: INFO: Pod "pod-projected-configmaps-3932f616-b092-4168-b299-b4c8d6f0a4f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011541042s
STEP: Saw pod success
Sep 25 03:42:40.496: INFO: Pod "pod-projected-configmaps-3932f616-b092-4168-b299-b4c8d6f0a4f2" satisfied condition "success or failure"
Sep 25 03:42:40.500: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-projected-configmaps-3932f616-b092-4168-b299-b4c8d6f0a4f2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 25 03:42:40.524: INFO: Waiting for pod pod-projected-configmaps-3932f616-b092-4168-b299-b4c8d6f0a4f2 to disappear
Sep 25 03:42:40.527: INFO: Pod pod-projected-configmaps-3932f616-b092-4168-b299-b4c8d6f0a4f2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:42:40.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-463" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":261,"skipped":4216,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:42:40.534: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8807
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:42:51.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8807" for this suite.

â€¢ [SLOW TEST:11.189 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":262,"skipped":4217,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:42:51.724: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-2653/configmap-test-427c7714-42bc-46ed-a208-188b6d3654c7
STEP: Creating a pod to test consume configMaps
Sep 25 03:42:51.869: INFO: Waiting up to 5m0s for pod "pod-configmaps-11765212-1f7e-48c6-a902-e38d3a56ce56" in namespace "configmap-2653" to be "success or failure"
Sep 25 03:42:51.871: INFO: Pod "pod-configmaps-11765212-1f7e-48c6-a902-e38d3a56ce56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067573ms
Sep 25 03:42:53.876: INFO: Pod "pod-configmaps-11765212-1f7e-48c6-a902-e38d3a56ce56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006439096s
Sep 25 03:42:55.880: INFO: Pod "pod-configmaps-11765212-1f7e-48c6-a902-e38d3a56ce56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0105658s
STEP: Saw pod success
Sep 25 03:42:55.880: INFO: Pod "pod-configmaps-11765212-1f7e-48c6-a902-e38d3a56ce56" satisfied condition "success or failure"
Sep 25 03:42:55.883: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-configmaps-11765212-1f7e-48c6-a902-e38d3a56ce56 container env-test: <nil>
STEP: delete the pod
Sep 25 03:42:55.898: INFO: Waiting for pod pod-configmaps-11765212-1f7e-48c6-a902-e38d3a56ce56 to disappear
Sep 25 03:42:55.900: INFO: Pod pod-configmaps-11765212-1f7e-48c6-a902-e38d3a56ce56 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:42:55.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2653" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":263,"skipped":4226,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:42:55.908: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1116
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 25 03:42:56.037: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 03:42:59.046: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:43:11.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1116" for this suite.

â€¢ [SLOW TEST:15.190 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":264,"skipped":4292,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:43:11.098: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-674
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:43:11.525: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 25 03:43:13.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602191, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602191, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602191, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602191, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:43:16.549: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:43:16.553: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2536-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:43:17.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-674" for this suite.
STEP: Destroying namespace "webhook-674-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.595 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":265,"skipped":4293,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:43:17.693: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:43:17.848: INFO: (0) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 19.761632ms)
Sep 25 03:43:17.851: INFO: (1) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.336621ms)
Sep 25 03:43:17.855: INFO: (2) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.177625ms)
Sep 25 03:43:17.858: INFO: (3) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.372638ms)
Sep 25 03:43:17.861: INFO: (4) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.764013ms)
Sep 25 03:43:17.864: INFO: (5) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.530546ms)
Sep 25 03:43:17.869: INFO: (6) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.328353ms)
Sep 25 03:43:17.873: INFO: (7) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.784138ms)
Sep 25 03:43:17.876: INFO: (8) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.974516ms)
Sep 25 03:43:17.879: INFO: (9) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.004352ms)
Sep 25 03:43:17.882: INFO: (10) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.494695ms)
Sep 25 03:43:17.885: INFO: (11) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.206012ms)
Sep 25 03:43:17.889: INFO: (12) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.100612ms)
Sep 25 03:43:17.892: INFO: (13) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.918468ms)
Sep 25 03:43:17.896: INFO: (14) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.253797ms)
Sep 25 03:43:17.900: INFO: (15) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.242877ms)
Sep 25 03:43:17.904: INFO: (16) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.923545ms)
Sep 25 03:43:17.909: INFO: (17) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.678785ms)
Sep 25 03:43:17.913: INFO: (18) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.680638ms)
Sep 25 03:43:17.917: INFO: (19) /api/v1/nodes/biz-k8s-node-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 4.552613ms)
[AfterEach] version v1
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:43:17.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4220" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":266,"skipped":4296,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:43:17.926: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:43:18.308: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:43:20.319: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602198, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602198, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602198, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602198, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:43:23.336: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:43:23.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7030" for this suite.
STEP: Destroying namespace "webhook-7030-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.578 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":267,"skipped":4297,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:43:23.504: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[BeforeEach] Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:325
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Sep 25 03:43:23.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 create -f - --namespace=kubectl-9679'
Sep 25 03:43:23.894: INFO: stderr: ""
Sep 25 03:43:23.894: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 25 03:43:23.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9679'
Sep 25 03:43:23.984: INFO: stderr: ""
Sep 25 03:43:23.984: INFO: stdout: "update-demo-nautilus-2wm6w update-demo-nautilus-jngq5 "
Sep 25 03:43:23.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-2wm6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:24.080: INFO: stderr: ""
Sep 25 03:43:24.080: INFO: stdout: ""
Sep 25 03:43:24.080: INFO: update-demo-nautilus-2wm6w is created but not running
Sep 25 03:43:29.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9679'
Sep 25 03:43:29.190: INFO: stderr: ""
Sep 25 03:43:29.190: INFO: stdout: "update-demo-nautilus-2wm6w update-demo-nautilus-jngq5 "
Sep 25 03:43:29.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-2wm6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:29.289: INFO: stderr: ""
Sep 25 03:43:29.289: INFO: stdout: "true"
Sep 25 03:43:29.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-2wm6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:29.396: INFO: stderr: ""
Sep 25 03:43:29.396: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 25 03:43:29.396: INFO: validating pod update-demo-nautilus-2wm6w
Sep 25 03:43:29.401: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 25 03:43:29.401: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 25 03:43:29.401: INFO: update-demo-nautilus-2wm6w is verified up and running
Sep 25 03:43:29.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-jngq5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:29.510: INFO: stderr: ""
Sep 25 03:43:29.510: INFO: stdout: "true"
Sep 25 03:43:29.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-jngq5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:29.601: INFO: stderr: ""
Sep 25 03:43:29.601: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 25 03:43:29.601: INFO: validating pod update-demo-nautilus-jngq5
Sep 25 03:43:29.606: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 25 03:43:29.606: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 25 03:43:29.607: INFO: update-demo-nautilus-jngq5 is verified up and running
STEP: scaling down the replication controller
Sep 25 03:43:29.610: INFO: scanned /root for discovery docs: <nil>
Sep 25 03:43:29.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9679'
Sep 25 03:43:30.739: INFO: stderr: ""
Sep 25 03:43:30.739: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 25 03:43:30.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9679'
Sep 25 03:43:30.824: INFO: stderr: ""
Sep 25 03:43:30.824: INFO: stdout: "update-demo-nautilus-2wm6w update-demo-nautilus-jngq5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 25 03:43:35.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9679'
Sep 25 03:43:35.932: INFO: stderr: ""
Sep 25 03:43:35.933: INFO: stdout: "update-demo-nautilus-2wm6w "
Sep 25 03:43:35.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-2wm6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:36.038: INFO: stderr: ""
Sep 25 03:43:36.038: INFO: stdout: "true"
Sep 25 03:43:36.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-2wm6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:36.144: INFO: stderr: ""
Sep 25 03:43:36.144: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 25 03:43:36.144: INFO: validating pod update-demo-nautilus-2wm6w
Sep 25 03:43:36.148: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 25 03:43:36.148: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 25 03:43:36.148: INFO: update-demo-nautilus-2wm6w is verified up and running
STEP: scaling up the replication controller
Sep 25 03:43:36.150: INFO: scanned /root for discovery docs: <nil>
Sep 25 03:43:36.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9679'
Sep 25 03:43:36.285: INFO: stderr: ""
Sep 25 03:43:36.285: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 25 03:43:36.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9679'
Sep 25 03:43:36.385: INFO: stderr: ""
Sep 25 03:43:36.385: INFO: stdout: "update-demo-nautilus-2wm6w update-demo-nautilus-pgcpv "
Sep 25 03:43:36.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-2wm6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:36.500: INFO: stderr: ""
Sep 25 03:43:36.500: INFO: stdout: "true"
Sep 25 03:43:36.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-2wm6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:36.602: INFO: stderr: ""
Sep 25 03:43:36.602: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 25 03:43:36.602: INFO: validating pod update-demo-nautilus-2wm6w
Sep 25 03:43:36.608: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 25 03:43:36.608: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 25 03:43:36.608: INFO: update-demo-nautilus-2wm6w is verified up and running
Sep 25 03:43:36.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-pgcpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:36.697: INFO: stderr: ""
Sep 25 03:43:36.697: INFO: stdout: ""
Sep 25 03:43:36.697: INFO: update-demo-nautilus-pgcpv is created but not running
Sep 25 03:43:41.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9679'
Sep 25 03:43:41.799: INFO: stderr: ""
Sep 25 03:43:41.799: INFO: stdout: "update-demo-nautilus-2wm6w update-demo-nautilus-pgcpv "
Sep 25 03:43:41.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-2wm6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:41.890: INFO: stderr: ""
Sep 25 03:43:41.890: INFO: stdout: "true"
Sep 25 03:43:41.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-2wm6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:41.998: INFO: stderr: ""
Sep 25 03:43:41.998: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 25 03:43:41.998: INFO: validating pod update-demo-nautilus-2wm6w
Sep 25 03:43:42.007: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 25 03:43:42.008: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 25 03:43:42.008: INFO: update-demo-nautilus-2wm6w is verified up and running
Sep 25 03:43:42.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-pgcpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:42.106: INFO: stderr: ""
Sep 25 03:43:42.106: INFO: stdout: "true"
Sep 25 03:43:42.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods update-demo-nautilus-pgcpv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9679'
Sep 25 03:43:42.218: INFO: stderr: ""
Sep 25 03:43:42.218: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 25 03:43:42.218: INFO: validating pod update-demo-nautilus-pgcpv
Sep 25 03:43:42.224: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 25 03:43:42.224: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 25 03:43:42.224: INFO: update-demo-nautilus-pgcpv is verified up and running
STEP: using delete to clean up resources
Sep 25 03:43:42.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 delete --grace-period=0 --force -f - --namespace=kubectl-9679'
Sep 25 03:43:42.330: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 25 03:43:42.330: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 25 03:43:42.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9679'
Sep 25 03:43:42.440: INFO: stderr: "No resources found in kubectl-9679 namespace.\n"
Sep 25 03:43:42.440: INFO: stdout: ""
Sep 25 03:43:42.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -l name=update-demo --namespace=kubectl-9679 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 25 03:43:42.538: INFO: stderr: ""
Sep 25 03:43:42.538: INFO: stdout: "update-demo-nautilus-2wm6w\nupdate-demo-nautilus-pgcpv\n"
Sep 25 03:43:43.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9679'
Sep 25 03:43:43.159: INFO: stderr: "No resources found in kubectl-9679 namespace.\n"
Sep 25 03:43:43.159: INFO: stdout: ""
Sep 25 03:43:43.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-888863303 get pods -l name=update-demo --namespace=kubectl-9679 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 25 03:43:43.241: INFO: stderr: ""
Sep 25 03:43:43.241: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:43:43.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9679" for this suite.

â€¢ [SLOW TEST:19.746 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:323
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":268,"skipped":4300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:43:43.252: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-0a3e94c1-5af5-4773-aef7-abc70ad61d81
STEP: Creating a pod to test consume secrets
Sep 25 03:43:43.403: INFO: Waiting up to 5m0s for pod "pod-secrets-d49398b1-289a-41f6-9442-c2700d7b4421" in namespace "secrets-3410" to be "success or failure"
Sep 25 03:43:43.405: INFO: Pod "pod-secrets-d49398b1-289a-41f6-9442-c2700d7b4421": Phase="Pending", Reason="", readiness=false. Elapsed: 1.424553ms
Sep 25 03:43:45.408: INFO: Pod "pod-secrets-d49398b1-289a-41f6-9442-c2700d7b4421": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005109749s
STEP: Saw pod success
Sep 25 03:43:45.408: INFO: Pod "pod-secrets-d49398b1-289a-41f6-9442-c2700d7b4421" satisfied condition "success or failure"
Sep 25 03:43:45.412: INFO: Trying to get logs from node biz-k8s-node-3 pod pod-secrets-d49398b1-289a-41f6-9442-c2700d7b4421 container secret-volume-test: <nil>
STEP: delete the pod
Sep 25 03:43:45.425: INFO: Waiting for pod pod-secrets-d49398b1-289a-41f6-9442-c2700d7b4421 to disappear
Sep 25 03:43:45.427: INFO: Pod pod-secrets-d49398b1-289a-41f6-9442-c2700d7b4421 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:43:45.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3410" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":269,"skipped":4371,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:43:45.433: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3882
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 25 03:43:45.569: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 25 03:43:58.436: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
Sep 25 03:44:01.904: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:44:14.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3882" for this suite.

â€¢ [SLOW TEST:28.621 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":270,"skipped":4376,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:44:14.054: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 25 03:44:14.196: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5625 /api/v1/namespaces/watch-5625/configmaps/e2e-watch-test-label-changed 3079e2af-dde5-467c-945e-e2b38394f17e 440175 0 2020-09-25 03:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 25 03:44:14.197: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5625 /api/v1/namespaces/watch-5625/configmaps/e2e-watch-test-label-changed 3079e2af-dde5-467c-945e-e2b38394f17e 440176 0 2020-09-25 03:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 25 03:44:14.197: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5625 /api/v1/namespaces/watch-5625/configmaps/e2e-watch-test-label-changed 3079e2af-dde5-467c-945e-e2b38394f17e 440177 0 2020-09-25 03:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 25 03:44:24.222: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5625 /api/v1/namespaces/watch-5625/configmaps/e2e-watch-test-label-changed 3079e2af-dde5-467c-945e-e2b38394f17e 440235 0 2020-09-25 03:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 25 03:44:24.222: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5625 /api/v1/namespaces/watch-5625/configmaps/e2e-watch-test-label-changed 3079e2af-dde5-467c-945e-e2b38394f17e 440236 0 2020-09-25 03:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep 25 03:44:24.222: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5625 /api/v1/namespaces/watch-5625/configmaps/e2e-watch-test-label-changed 3079e2af-dde5-467c-945e-e2b38394f17e 440237 0 2020-09-25 03:44:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:44:24.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5625" for this suite.

â€¢ [SLOW TEST:10.177 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":271,"skipped":4379,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:44:24.232: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 25 03:44:24.686: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 25 03:44:26.693: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602264, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602264, loc:(*time.Location)(0x7931640)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602264, loc:(*time.Location)(0x7931640)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63736602264, loc:(*time.Location)(0x7931640)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 25 03:44:29.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Sep 25 03:44:29.708: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7946-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:44:30.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7937" for this suite.
STEP: Destroying namespace "webhook-7937-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.622 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":272,"skipped":4398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:44:30.855: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:44:31.095: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b4931e4-c125-49e0-a797-ebbeafc0e2af" in namespace "projected-1986" to be "success or failure"
Sep 25 03:44:31.103: INFO: Pod "downwardapi-volume-2b4931e4-c125-49e0-a797-ebbeafc0e2af": Phase="Pending", Reason="", readiness=false. Elapsed: 8.121953ms
Sep 25 03:44:33.106: INFO: Pod "downwardapi-volume-2b4931e4-c125-49e0-a797-ebbeafc0e2af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011475121s
STEP: Saw pod success
Sep 25 03:44:33.107: INFO: Pod "downwardapi-volume-2b4931e4-c125-49e0-a797-ebbeafc0e2af" satisfied condition "success or failure"
Sep 25 03:44:33.112: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-2b4931e4-c125-49e0-a797-ebbeafc0e2af container client-container: <nil>
STEP: delete the pod
Sep 25 03:44:33.134: INFO: Waiting for pod downwardapi-volume-2b4931e4-c125-49e0-a797-ebbeafc0e2af to disappear
Sep 25 03:44:33.136: INFO: Pod downwardapi-volume-2b4931e4-c125-49e0-a797-ebbeafc0e2af no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:44:33.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1986" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":273,"skipped":4428,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:44:33.145: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3611
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Sep 25 03:44:33.287: INFO: Found 0 stateful pods, waiting for 3
Sep 25 03:44:43.292: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:44:43.292: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:44:43.292: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 25 03:44:53.292: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:44:53.292: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:44:53.292: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 25 03:44:53.320: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 25 03:45:03.354: INFO: Updating stateful set ss2
Sep 25 03:45:03.361: INFO: Waiting for Pod statefulset-3611/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep 25 03:45:13.398: INFO: Found 2 stateful pods, waiting for 3
Sep 25 03:45:23.402: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:45:23.402: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 25 03:45:23.402: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 25 03:45:23.425: INFO: Updating stateful set ss2
Sep 25 03:45:23.433: INFO: Waiting for Pod statefulset-3611/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 25 03:45:33.460: INFO: Updating stateful set ss2
Sep 25 03:45:33.469: INFO: Waiting for StatefulSet statefulset-3611/ss2 to complete update
Sep 25 03:45:33.469: INFO: Waiting for Pod statefulset-3611/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Sep 25 03:45:43.477: INFO: Deleting all statefulset in ns statefulset-3611
Sep 25 03:45:43.480: INFO: Scaling statefulset ss2 to 0
Sep 25 03:46:13.494: INFO: Waiting for statefulset status.replicas updated to 0
Sep 25 03:46:13.497: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:46:13.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3611" for this suite.

â€¢ [SLOW TEST:100.373 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":274,"skipped":4428,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:46:13.520: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:46:26.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1407" for this suite.

â€¢ [SLOW TEST:13.205 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":275,"skipped":4469,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:46:26.725: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:46:26.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2e67f56-152f-4df4-bdbe-16c5d51ae47d" in namespace "projected-1476" to be "success or failure"
Sep 25 03:46:26.869: INFO: Pod "downwardapi-volume-b2e67f56-152f-4df4-bdbe-16c5d51ae47d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.304778ms
Sep 25 03:46:28.873: INFO: Pod "downwardapi-volume-b2e67f56-152f-4df4-bdbe-16c5d51ae47d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007306297s
STEP: Saw pod success
Sep 25 03:46:28.873: INFO: Pod "downwardapi-volume-b2e67f56-152f-4df4-bdbe-16c5d51ae47d" satisfied condition "success or failure"
Sep 25 03:46:28.876: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-b2e67f56-152f-4df4-bdbe-16c5d51ae47d container client-container: <nil>
STEP: delete the pod
Sep 25 03:46:28.895: INFO: Waiting for pod downwardapi-volume-b2e67f56-152f-4df4-bdbe-16c5d51ae47d to disappear
Sep 25 03:46:28.898: INFO: Pod downwardapi-volume-b2e67f56-152f-4df4-bdbe-16c5d51ae47d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:46:28.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1476" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":276,"skipped":4472,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:46:28.904: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:46:29.045: INFO: Waiting up to 5m0s for pod "downwardapi-volume-207eb7b0-1bf7-4e56-beac-f8879161b032" in namespace "downward-api-4988" to be "success or failure"
Sep 25 03:46:29.047: INFO: Pod "downwardapi-volume-207eb7b0-1bf7-4e56-beac-f8879161b032": Phase="Pending", Reason="", readiness=false. Elapsed: 1.950821ms
Sep 25 03:46:31.050: INFO: Pod "downwardapi-volume-207eb7b0-1bf7-4e56-beac-f8879161b032": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00592147s
STEP: Saw pod success
Sep 25 03:46:31.051: INFO: Pod "downwardapi-volume-207eb7b0-1bf7-4e56-beac-f8879161b032" satisfied condition "success or failure"
Sep 25 03:46:31.054: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-207eb7b0-1bf7-4e56-beac-f8879161b032 container client-container: <nil>
STEP: delete the pod
Sep 25 03:46:31.069: INFO: Waiting for pod downwardapi-volume-207eb7b0-1bf7-4e56-beac-f8879161b032 to disappear
Sep 25 03:46:31.071: INFO: Pod downwardapi-volume-207eb7b0-1bf7-4e56-beac-f8879161b032 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:46:31.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4988" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":277,"skipped":4486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:46:31.080: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Sep 25 03:46:31.226: INFO: Waiting up to 5m0s for pod "downwardapi-volume-690a12c5-ae8d-4030-99d1-d31772ab5499" in namespace "downward-api-9933" to be "success or failure"
Sep 25 03:46:31.228: INFO: Pod "downwardapi-volume-690a12c5-ae8d-4030-99d1-d31772ab5499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.316201ms
Sep 25 03:46:33.232: INFO: Pod "downwardapi-volume-690a12c5-ae8d-4030-99d1-d31772ab5499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005867405s
STEP: Saw pod success
Sep 25 03:46:33.232: INFO: Pod "downwardapi-volume-690a12c5-ae8d-4030-99d1-d31772ab5499" satisfied condition "success or failure"
Sep 25 03:46:33.235: INFO: Trying to get logs from node biz-k8s-node-3 pod downwardapi-volume-690a12c5-ae8d-4030-99d1-d31772ab5499 container client-container: <nil>
STEP: delete the pod
Sep 25 03:46:33.255: INFO: Waiting for pod downwardapi-volume-690a12c5-ae8d-4030-99d1-d31772ab5499 to disappear
Sep 25 03:46:33.257: INFO: Pod downwardapi-volume-690a12c5-ae8d-4030-99d1-d31772ab5499 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:46:33.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9933" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":278,"skipped":4534,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:46:33.263: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-2413
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2413
STEP: Creating statefulset with conflicting port in namespace statefulset-2413
STEP: Waiting until pod test-pod will start running in namespace statefulset-2413
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2413
Sep 25 03:46:37.421: INFO: Observed stateful pod in namespace: statefulset-2413, name: ss-0, uid: f8230e98-9bd6-4743-a2d8-f3aaf853b41e, status phase: Pending. Waiting for statefulset controller to delete.
Sep 25 03:46:37.486: INFO: Observed stateful pod in namespace: statefulset-2413, name: ss-0, uid: f8230e98-9bd6-4743-a2d8-f3aaf853b41e, status phase: Failed. Waiting for statefulset controller to delete.
Sep 25 03:46:37.492: INFO: Observed stateful pod in namespace: statefulset-2413, name: ss-0, uid: f8230e98-9bd6-4743-a2d8-f3aaf853b41e, status phase: Failed. Waiting for statefulset controller to delete.
Sep 25 03:46:37.496: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2413
STEP: Removing pod with conflicting port in namespace statefulset-2413
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2413 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Sep 25 03:46:41.515: INFO: Deleting all statefulset in ns statefulset-2413
Sep 25 03:46:41.520: INFO: Scaling statefulset ss to 0
Sep 25 03:47:01.534: INFO: Waiting for statefulset status.replicas updated to 0
Sep 25 03:47:01.537: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:47:01.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2413" for this suite.

â€¢ [SLOW TEST:28.292 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":279,"skipped":4538,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Sep 25 03:47:01.556: INFO: >>> kubeConfig: /tmp/kubeconfig-888863303
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4370
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-4370
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4370 to expose endpoints map[]
Sep 25 03:47:01.692: INFO: Get endpoints failed (3.315027ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep 25 03:47:02.696: INFO: successfully validated that service endpoint-test2 in namespace services-4370 exposes endpoints map[] (1.007519226s elapsed)
STEP: Creating pod pod1 in namespace services-4370
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4370 to expose endpoints map[pod1:[80]]
Sep 25 03:47:05.745: INFO: successfully validated that service endpoint-test2 in namespace services-4370 exposes endpoints map[pod1:[80]] (3.035816971s elapsed)
STEP: Creating pod pod2 in namespace services-4370
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4370 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 25 03:47:07.787: INFO: successfully validated that service endpoint-test2 in namespace services-4370 exposes endpoints map[pod1:[80] pod2:[80]] (2.021941731s elapsed)
STEP: Deleting pod pod1 in namespace services-4370
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4370 to expose endpoints map[pod2:[80]]
Sep 25 03:47:07.796: INFO: successfully validated that service endpoint-test2 in namespace services-4370 exposes endpoints map[pod2:[80]] (5.032767ms elapsed)
STEP: Deleting pod pod2 in namespace services-4370
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4370 to expose endpoints map[]
Sep 25 03:47:08.809: INFO: successfully validated that service endpoint-test2 in namespace services-4370 exposes endpoints map[] (1.009188212s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Sep 25 03:47:08.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4370" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:7.277 seconds]
[sig-network] Services
/workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.11-rc.1.3+564c2018c1ea15/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":280,"skipped":4557,"failed":0}
SSSSSSSSep 25 03:47:08.833: INFO: Running AfterSuite actions on all nodes
Sep 25 03:47:08.833: INFO: Running AfterSuite actions on node 1
Sep 25 03:47:08.833: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4564,"failed":0}

Ran 280 of 4844 Specs in 4238.887 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4564 Skipped
PASS

Ginkgo ran 1 suite in 1h10m40.256789939s
Test Suite Passed
